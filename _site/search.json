[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introdução à Econometria Aplicada com R",
    "section": "",
    "text": "Bem-vindos ao livro “Introdução à Econometria Aplicada com R”. Este livro foi concebido para estudantes e investigadores que desejam aprender econometria utilizando a linguagem de programação R.\n\n\nEste livro tem como objectivos:\n\nProporcionar uma introdução sólida aos conceitos fundamentais da econometria\nEnsinar a aplicação prática de técnicas econométricas utilizando R\nDesenvolver competências de análise de dados económicos\nFornecer exemplos práticos com dados reais\n\n\n\n\nO livro está organizado da seguinte forma:\n\nCapítulo 0: Pré-requisitos para Econometria\nCapítulo 1: Breve Introdução ao R\nCapítulo 2: Estrutura de Dados Económicos\nCapítulo 3: Regressão Linear\nCapítulo 4: Pressupostos da Regressão Linear\nCapítulo 5: Avaliação dos Modelos de Regressão\nCapítulo 6: Extensões dos Modelos de Regressão\nCapítulo 7: Modelos de Variável Dependente Limitada\n\n\n\n\nPara aproveitar ao máximo este livro, recomenda-se:\n\nConhecimentos básicos de estatística\nFamiliaridade com conceitos de matemática ao nível do ensino secundário\nInteresse em análise de dados económicos\n\nNão é necessário experiência prévia com R, pois o livro inclui uma introdução à linguagem.\n\n\n\nTodos os dados utilizados nos exemplos estão disponíveis junto com este livro. Os códigos R apresentados podem ser executados directamente, permitindo uma aprendizagem prática e interactiva."
  },
  {
    "objectID": "index.html#sec-objectivos",
    "href": "index.html#sec-objectivos",
    "title": "Introdução à Econometria Aplicada com R",
    "section": "",
    "text": "Este livro tem como objectivos:\n\nProporcionar uma introdução sólida aos conceitos fundamentais da econometria\nEnsinar a aplicação prática de técnicas econométricas utilizando R\nDesenvolver competências de análise de dados económicos\nFornecer exemplos práticos com dados reais"
  },
  {
    "objectID": "index.html#sec-estrutura",
    "href": "index.html#sec-estrutura",
    "title": "Introdução à Econometria Aplicada com R",
    "section": "",
    "text": "O livro está organizado da seguinte forma:\n\nCapítulo 0: Pré-requisitos para Econometria\nCapítulo 1: Breve Introdução ao R\nCapítulo 2: Estrutura de Dados Económicos\nCapítulo 3: Regressão Linear\nCapítulo 4: Pressupostos da Regressão Linear\nCapítulo 5: Avaliação dos Modelos de Regressão\nCapítulo 6: Extensões dos Modelos de Regressão\nCapítulo 7: Modelos de Variável Dependente Limitada"
  },
  {
    "objectID": "index.html#sec-prerequisitos",
    "href": "index.html#sec-prerequisitos",
    "title": "Introdução à Econometria Aplicada com R",
    "section": "",
    "text": "Para aproveitar ao máximo este livro, recomenda-se:\n\nConhecimentos básicos de estatística\nFamiliaridade com conceitos de matemática ao nível do ensino secundário\nInteresse em análise de dados económicos\n\nNão é necessário experiência prévia com R, pois o livro inclui uma introdução à linguagem."
  },
  {
    "objectID": "index.html#sec-recursos",
    "href": "index.html#sec-recursos",
    "title": "Introdução à Econometria Aplicada com R",
    "section": "",
    "text": "Todos os dados utilizados nos exemplos estão disponíveis junto com este livro. Os códigos R apresentados podem ser executados directamente, permitindo uma aprendizagem prática e interactiva."
  },
  {
    "objectID": "capitulo-00-prerequisitos.html",
    "href": "capitulo-00-prerequisitos.html",
    "title": "Pré-requisitos para Econometria",
    "section": "",
    "text": "A econometria é uma disciplina que combina economia, estatística e matemática. Por isso, é importante ter alguns conhecimentos de base nestas áreas. Este capítulo apresenta alguns conhecimentos básicoss que serão úteis para compreender a econometria. Naturalmente, que é necessária uma análise mais profunda de cada um destes tópicos.\n\n\n\nNesta secção vamos rever alguns conceitos de estatística e a fórmulação matemática. O que se pretende é estar familiarizado com as definições. As fórmulas poderão ser sempre consultadas quando necessário.\n\n\nA estatística descritiva envolve um conjunto de indicadores que ajudam a resumir algumas características dos dados:\n\n\n\nMédia: A média aritmética é a soma de todos os valores dividida pelo número de observações.\n\n\\[\nMédia = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\tag{1}\\]\nonde: \\[n\\] é o número de observações; \\[x_i\\] é o valor da i-ésima observação da variável \\(x_i\\)\nA média é sensível a valores extremos (outliers), o que pode distorcer a percepção. A média é o 1º momento da distribuição.\n\nMediana: A mediana é o valor que separa a metade superior da metade inferior dos dados. Se o número de observações for ímpar, a mediana é o valor do meio. Se for par, é a média dos dois valores centrais.\n\n\\[\nMediana =\n\\begin{cases}\nx_{\\frac{n+1}{2}} & \\text{se } n \\text{ ímpar} \\\\\n\\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2} & \\text{se } n \\text{ par}\n\\end{cases}\n\\tag{2}\\]\nA mediana é menos sensível a valores extremos do que a média.\n\n\n\n\nDesvio-padrão: O desvio-padrão mede a dispersão dos dados em relação à média.\n\n\\[\nDesvio\\text{-}padrão = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\tag{3}\\]\nonde: \\(\\bar{x}\\) é a média dos valores de \\(x_i\\).\nUm desvio-padrão elevado indica que os dados estão mais dispersos em relação à média.\n\nVariância: A variância é o quadrado do desvio-padrão:\n\n\\[\nVariância = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\tag{4}\\]\nA variância, por estar ao quadrado, dá um peso maior a valores extremos. A variância é o 2º momento da distribuição.\n\n\n\n\nAssimetria (Skewness): A assimetria mede a simetria da distribuição dos dados em torno da média. Uma assimetria positiva indica que a cauda direita é mais longa ou gorda do que a cauda esquerda, enquanto uma assimetria negativa indica o contrário. A assimetria é calculada pela fórmula:\n\n\\[\nSkewness = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^{n} \\left(\\frac{x_i - \\bar{x}}{s}\\right)^3\n\\tag{5}\\] onde: \\(s\\) é o desvio-padrão dos valores de \\(x_i\\).\nA skewness é o 3º momento da distribuição.\nExemplo de assimetria positiva e negativa:\n\n\nWarning in geom_histogram(binwidth = 0.5, fill = \"#2E86AB\", alpha = 0.8, :\nIgnoring unknown parameters: `size`\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_histogram(binwidth = 0.5, fill = \"#F18F01\", alpha = 0.8, :\nIgnoring unknown parameters: `size`\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nCurtose (Kurtosis): A curtose mede a “altura” da distribuição dos dados. Uma curtose alta indica que os dados têm caudas mais longas e uma concentração maior em torno da média, enquanto uma curtose baixa indica caudas mais curtas e uma distribuição mais uniforme. A curtose é calculada pela fórmula:\n\n\\[\nKurtosis = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^{n} \\left(\\frac{x_i - \\bar{x}}{s}\\right)^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\\tag{6}\\] onde: \\(s\\) é o desvio-padrão dos valores de \\(x_i\\).\nA curtose é o 4º momento da distribuição.\n\n# Exemplo de curtose alta e baixa\nset.seed(123)\n# Criação de dados com diferentes níveis de curtose\ndata_high_kurtosis &lt;- data.frame(value = rt(1100, df = 3))  # Distribuição t com baixos graus de liberdade (caudas longas)\ndata_low_kurtosis &lt;- data.frame(value = runif(1100, min = -2, max = 2))  # Distribuição uniforme (mais achatada)\n\n# Criar dados para demonstrar curtose\np3 &lt;- ggplot(data_high_kurtosis, aes(x = value)) +\n     geom_histogram(binwidth = 0.3, fill = \"#6A0572\", alpha = 0.8, color = \"white\", size = 0.2) +\n     geom_vline(aes(xintercept = mean(value)), color = \"#FF6F61\", linetype = \"dashed\", size = 1) +\n     labs(title = \"Distribuição com Curtose Alta\",\n           subtitle = \"Caudas longas e pico acentuado\",\n           x = \"Valores\", y = \"Frequência\") +\n     theme_minimal() +\n     theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray50\"),\n            axis.title = element_text(size = 11),\n            panel.grid.minor = element_blank(),\n            panel.grid.major.x = element_blank())\n\nWarning in geom_histogram(binwidth = 0.3, fill = \"#6A0572\", alpha = 0.8, :\nIgnoring unknown parameters: `size`\n\np4 &lt;- ggplot(data_low_kurtosis, aes(x = value)) +\n     geom_histogram(binwidth = 0.2, fill = \"#FF6F61\", alpha = 0.8, color = \"white\", size = 0.2) +\n     geom_vline(aes(xintercept = mean(value)), color = \"#6A0572\", linetype = \"dashed\", size = 1) +\n     labs(title = \"Distribuição com Curtose Baixa\",\n           subtitle = \"Distribuição mais achatada e uniforme\",\n           x = \"Valores\", y = \"Frequência\") +\n     theme_minimal() +\n     theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray50\"),\n            axis.title = element_text(size = 11),\n            panel.grid.minor = element_blank(),\n            panel.grid.major.x = element_blank())\n\nWarning in geom_histogram(binwidth = 0.2, fill = \"#FF6F61\", alpha = 0.8, :\nIgnoring unknown parameters: `size`\n\n# Combinar os gráficos\ngrid.arrange(p3, p4, ncol = 2)\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\nDefinição de probabilidade - A probabilidade é uma medida numérica da possibilidade de um acontecimento ocorrer, varia entre 0 (impossivel) e 1 (certo).\nEventos mutuamente exclusivos - Dois eventos são mutuamente exclusivos se não houver a possibilidade de ocorrerem ao mesmo tempo. Por exemplo, ao lançar um dado, os eventos “sair um 3” e “sair um 5” são mutuamente exclusivos.\nEventos independentes - Dois eventos são independentes se a ocorrência de um não afeta a probabilidade do outro ocorrer. Por exemplo, ao lançar dois dados, o resultado do primeiro dado não afeta o resultado do segundo dado.\nDistribuições de probabilidade - Uma distribuição de probabilidade descreve como a probabilidade é distribuída entre os possíveis resultados de uma experiência aleatória. As distribuições podem ser discretas (por exemplo, distribuição binomial) ou contínuas (distribuição normal, distribuição t-Student, distribuição F e distribuição qui-quadrado são das mais importantes).\nTeorema do limite central - O teorema do limite central diz-nos que: à medida que o tamanho da amostra aumenta, a distribuição da média da amostra aproxima-se de uma distribuição normal, independentemente da distribuição da população original.\nLei dos grandes números - A lei dos grandes números diz-nos que, à medida que o tamanho da amostra aumenta, a média da amostra aproxima-se da média da população. Por exemplo, no lançamento de uma moeda ao ar, à medida que o número de lançamentos aumenta, a proporção de caras aproxima-se de 0.5 e de coroas também.\n\n\n\n\n\nTestes de hipóteses: Envolve a formulação de uma hipótese (H0) nula e uma hipótese alternativa (H1). H0 pode ser rejeitada ou não com base nos dados da amostra.\nNível de significância estatística: O nível de significância (\\(\\alpha\\)) é o limite máximo para rejeitar a hipótese nula. Os níveis mais comuns são 0.05, 0.01 e 0.10.\nValor de probabilidade (P-Value): O p-value é a probabilidade de observar a hipótese nula. Um P-Value de 0.03 indica que há uma probabilidade de 3% de a hipótese nula ser verdadeira. Se o p-value for menor que o nível de significância estatística (\\(\\alpha\\)), rejeitamos a hipótese nula.\nErro tipo I e tipo II: O erro tipo I ocorre quando rejeitamos a hipótese nula verdadeira, enquanto o erro tipo II ocorre quando não rejeitamos a hipótese nula falsa. Uma hipótese nula verdadeira é quando o valor real é igual ao valor que foi formulado na hipótese nula enquanto que uma hipótese nula falsa é quando o valor real é diferente do valor que foi formulado na hipótese nula.\nP-hacking: O p-hacking refere-se à manipulação dos dados ou da análise para obter resultados estatisticamente significativos. Pode incluir o enviesamento da seleção de variáveis ou da amostra e ainda a escolha de métodos que favorecem resultados desejados. O p-hacking pode levar a conclusões enganosas e comprometer a integridade da pesquisa.\n\n\n\n\n\n\n\nEmbora não seja necessário um conhecimento profundo, alguns conceitos de álgebra linear são úteis para entender os cálculos das funções do R nos modelos econométricos.\n\nMatrizes - Estruturas denúmeros organizados em linhas e colunas. As matrizes são usadas para representar e manipular dados em econometria.\nOperações matriciais básicas - multiplicação, transposição e inversão de uma matriz."
  },
  {
    "objectID": "capitulo-00-prerequisitos.html#sec-prerequisitos-intro",
    "href": "capitulo-00-prerequisitos.html#sec-prerequisitos-intro",
    "title": "Pré-requisitos para Econometria",
    "section": "",
    "text": "A econometria é uma disciplina que combina economia, estatística e matemática. Por isso, é importante ter alguns conhecimentos de base nestas áreas. Este capítulo apresenta alguns conhecimentos básicoss que serão úteis para compreender a econometria. Naturalmente, que é necessária uma análise mais profunda de cada um destes tópicos."
  },
  {
    "objectID": "capitulo-00-prerequisitos.html#sec-conceitos-estatisticos",
    "href": "capitulo-00-prerequisitos.html#sec-conceitos-estatisticos",
    "title": "Pré-requisitos para Econometria",
    "section": "",
    "text": "Nesta secção vamos rever alguns conceitos de estatística e a fórmulação matemática. O que se pretende é estar familiarizado com as definições. As fórmulas poderão ser sempre consultadas quando necessário.\n\n\nA estatística descritiva envolve um conjunto de indicadores que ajudam a resumir algumas características dos dados:\n\n\n\nMédia: A média aritmética é a soma de todos os valores dividida pelo número de observações.\n\n\\[\nMédia = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\tag{1}\\]\nonde: \\[n\\] é o número de observações; \\[x_i\\] é o valor da i-ésima observação da variável \\(x_i\\)\nA média é sensível a valores extremos (outliers), o que pode distorcer a percepção. A média é o 1º momento da distribuição.\n\nMediana: A mediana é o valor que separa a metade superior da metade inferior dos dados. Se o número de observações for ímpar, a mediana é o valor do meio. Se for par, é a média dos dois valores centrais.\n\n\\[\nMediana =\n\\begin{cases}\nx_{\\frac{n+1}{2}} & \\text{se } n \\text{ ímpar} \\\\\n\\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}}{2} & \\text{se } n \\text{ par}\n\\end{cases}\n\\tag{2}\\]\nA mediana é menos sensível a valores extremos do que a média.\n\n\n\n\nDesvio-padrão: O desvio-padrão mede a dispersão dos dados em relação à média.\n\n\\[\nDesvio\\text{-}padrão = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\tag{3}\\]\nonde: \\(\\bar{x}\\) é a média dos valores de \\(x_i\\).\nUm desvio-padrão elevado indica que os dados estão mais dispersos em relação à média.\n\nVariância: A variância é o quadrado do desvio-padrão:\n\n\\[\nVariância = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\tag{4}\\]\nA variância, por estar ao quadrado, dá um peso maior a valores extremos. A variância é o 2º momento da distribuição.\n\n\n\n\nAssimetria (Skewness): A assimetria mede a simetria da distribuição dos dados em torno da média. Uma assimetria positiva indica que a cauda direita é mais longa ou gorda do que a cauda esquerda, enquanto uma assimetria negativa indica o contrário. A assimetria é calculada pela fórmula:\n\n\\[\nSkewness = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^{n} \\left(\\frac{x_i - \\bar{x}}{s}\\right)^3\n\\tag{5}\\] onde: \\(s\\) é o desvio-padrão dos valores de \\(x_i\\).\nA skewness é o 3º momento da distribuição.\nExemplo de assimetria positiva e negativa:\n\n\nWarning in geom_histogram(binwidth = 0.5, fill = \"#2E86AB\", alpha = 0.8, :\nIgnoring unknown parameters: `size`\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_histogram(binwidth = 0.5, fill = \"#F18F01\", alpha = 0.8, :\nIgnoring unknown parameters: `size`\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\nCurtose (Kurtosis): A curtose mede a “altura” da distribuição dos dados. Uma curtose alta indica que os dados têm caudas mais longas e uma concentração maior em torno da média, enquanto uma curtose baixa indica caudas mais curtas e uma distribuição mais uniforme. A curtose é calculada pela fórmula:\n\n\\[\nKurtosis = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^{n} \\left(\\frac{x_i - \\bar{x}}{s}\\right)^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\\tag{6}\\] onde: \\(s\\) é o desvio-padrão dos valores de \\(x_i\\).\nA curtose é o 4º momento da distribuição.\n\n# Exemplo de curtose alta e baixa\nset.seed(123)\n# Criação de dados com diferentes níveis de curtose\ndata_high_kurtosis &lt;- data.frame(value = rt(1100, df = 3))  # Distribuição t com baixos graus de liberdade (caudas longas)\ndata_low_kurtosis &lt;- data.frame(value = runif(1100, min = -2, max = 2))  # Distribuição uniforme (mais achatada)\n\n# Criar dados para demonstrar curtose\np3 &lt;- ggplot(data_high_kurtosis, aes(x = value)) +\n     geom_histogram(binwidth = 0.3, fill = \"#6A0572\", alpha = 0.8, color = \"white\", size = 0.2) +\n     geom_vline(aes(xintercept = mean(value)), color = \"#FF6F61\", linetype = \"dashed\", size = 1) +\n     labs(title = \"Distribuição com Curtose Alta\",\n           subtitle = \"Caudas longas e pico acentuado\",\n           x = \"Valores\", y = \"Frequência\") +\n     theme_minimal() +\n     theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray50\"),\n            axis.title = element_text(size = 11),\n            panel.grid.minor = element_blank(),\n            panel.grid.major.x = element_blank())\n\nWarning in geom_histogram(binwidth = 0.3, fill = \"#6A0572\", alpha = 0.8, :\nIgnoring unknown parameters: `size`\n\np4 &lt;- ggplot(data_low_kurtosis, aes(x = value)) +\n     geom_histogram(binwidth = 0.2, fill = \"#FF6F61\", alpha = 0.8, color = \"white\", size = 0.2) +\n     geom_vline(aes(xintercept = mean(value)), color = \"#6A0572\", linetype = \"dashed\", size = 1) +\n     labs(title = \"Distribuição com Curtose Baixa\",\n           subtitle = \"Distribuição mais achatada e uniforme\",\n           x = \"Valores\", y = \"Frequência\") +\n     theme_minimal() +\n     theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n            plot.subtitle = element_text(hjust = 0.5, size = 11, color = \"gray50\"),\n            axis.title = element_text(size = 11),\n            panel.grid.minor = element_blank(),\n            panel.grid.major.x = element_blank())\n\nWarning in geom_histogram(binwidth = 0.2, fill = \"#FF6F61\", alpha = 0.8, :\nIgnoring unknown parameters: `size`\n\n# Combinar os gráficos\ngrid.arrange(p3, p4, ncol = 2)\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\n\n\n\nDefinição de probabilidade - A probabilidade é uma medida numérica da possibilidade de um acontecimento ocorrer, varia entre 0 (impossivel) e 1 (certo).\nEventos mutuamente exclusivos - Dois eventos são mutuamente exclusivos se não houver a possibilidade de ocorrerem ao mesmo tempo. Por exemplo, ao lançar um dado, os eventos “sair um 3” e “sair um 5” são mutuamente exclusivos.\nEventos independentes - Dois eventos são independentes se a ocorrência de um não afeta a probabilidade do outro ocorrer. Por exemplo, ao lançar dois dados, o resultado do primeiro dado não afeta o resultado do segundo dado.\nDistribuições de probabilidade - Uma distribuição de probabilidade descreve como a probabilidade é distribuída entre os possíveis resultados de uma experiência aleatória. As distribuições podem ser discretas (por exemplo, distribuição binomial) ou contínuas (distribuição normal, distribuição t-Student, distribuição F e distribuição qui-quadrado são das mais importantes).\nTeorema do limite central - O teorema do limite central diz-nos que: à medida que o tamanho da amostra aumenta, a distribuição da média da amostra aproxima-se de uma distribuição normal, independentemente da distribuição da população original.\nLei dos grandes números - A lei dos grandes números diz-nos que, à medida que o tamanho da amostra aumenta, a média da amostra aproxima-se da média da população. Por exemplo, no lançamento de uma moeda ao ar, à medida que o número de lançamentos aumenta, a proporção de caras aproxima-se de 0.5 e de coroas também.\n\n\n\n\n\nTestes de hipóteses: Envolve a formulação de uma hipótese (H0) nula e uma hipótese alternativa (H1). H0 pode ser rejeitada ou não com base nos dados da amostra.\nNível de significância estatística: O nível de significância (\\(\\alpha\\)) é o limite máximo para rejeitar a hipótese nula. Os níveis mais comuns são 0.05, 0.01 e 0.10.\nValor de probabilidade (P-Value): O p-value é a probabilidade de observar a hipótese nula. Um P-Value de 0.03 indica que há uma probabilidade de 3% de a hipótese nula ser verdadeira. Se o p-value for menor que o nível de significância estatística (\\(\\alpha\\)), rejeitamos a hipótese nula.\nErro tipo I e tipo II: O erro tipo I ocorre quando rejeitamos a hipótese nula verdadeira, enquanto o erro tipo II ocorre quando não rejeitamos a hipótese nula falsa. Uma hipótese nula verdadeira é quando o valor real é igual ao valor que foi formulado na hipótese nula enquanto que uma hipótese nula falsa é quando o valor real é diferente do valor que foi formulado na hipótese nula.\nP-hacking: O p-hacking refere-se à manipulação dos dados ou da análise para obter resultados estatisticamente significativos. Pode incluir o enviesamento da seleção de variáveis ou da amostra e ainda a escolha de métodos que favorecem resultados desejados. O p-hacking pode levar a conclusões enganosas e comprometer a integridade da pesquisa."
  },
  {
    "objectID": "capitulo-00-prerequisitos.html#sec-conceitos-matematicos",
    "href": "capitulo-00-prerequisitos.html#sec-conceitos-matematicos",
    "title": "Pré-requisitos para Econometria",
    "section": "",
    "text": "Embora não seja necessário um conhecimento profundo, alguns conceitos de álgebra linear são úteis para entender os cálculos das funções do R nos modelos econométricos.\n\nMatrizes - Estruturas denúmeros organizados em linhas e colunas. As matrizes são usadas para representar e manipular dados em econometria.\nOperações matriciais básicas - multiplicação, transposição e inversão de uma matriz."
  },
  {
    "objectID": "capitulo-05-avaliacao-modelos.html",
    "href": "capitulo-05-avaliacao-modelos.html",
    "title": "Avaliação dos Modelos de Regressão",
    "section": "",
    "text": "Para da significância estatística dos coeficientes e do modelo existem outros testes para assegurar a validade dos resultados. Existem testes de diagnóstico ao erros (heterocedasticidade, autocorrelação, normalidade), ao modelo (especificação), às variáveis (multicolinearidade), à estabilidade (estabilidade dos coeficientes e do modelo). Além disso, existem critérios para comparar modelos alternativos (R², AIC, BIC), métodos de selecção de modelos (stepwise, validação cruzada) e técnicas para identificar observações influentes.\n\n\nExistem várias formas de detectar a multicolinearidade entre as variáveis independentes num modelo de regressão. Vamos utilizar os dados do ficheiro m_reg.xlsx. O ficheiro pode ser descarregado em https://github.com/tiagolafonso/Files_Intro_Applied_Econometrics. Os dados foram recolhidos no World Development Indicators e contêm as seguintes variáveis:\n\n\n\n\n\n\n\nCódigo\nDescrição\n\n\n\n\ngdp\nPIB (preços constantes, moeda local)\n\n\nair\nTransporte aéreo, passageiros transportados\n\n\nrail\nTransporte ferroviário, passageiros transportados (milhões de passageiros)\n\n\nagri\nAgricultura, silvicultura e pesca, valor acrescentado (preços constantes, moeda local)\n\n\nind\nIndústria (incluindo construção), valor acrescentado (preços constantes, moeda local)\n\n\nman\nIndústria transformadora, valor acrescentado (preços constantes, moeda local)\n\n\nser\nServiços, valor acrescentado (preços constantes, moeda local)\n\n\nx\nExportações de bens e serviços (preços constantes, moeda local)\n\n\nm\nImportações de bens e serviços (preços constantes, moeda local)\n\n\ngfcf\nFormação bruta de capital fixo (preços constantes, moeda local)\n\n\n\nImportar os dados para o R:\n\n#limpar ambiente\nrm(list = ls())\n\n# carregar bibliotecas necessárias\nlibrary(readxl)\nlibrary(tidyverse)\n\n#importar dados\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\nNeste exemplo, vamos analisar a multicolinearidade para as variáveis independentes para o modelo:\n\\[\ngdp_t=\\beta_0 + \\beta_1 ind_t + \\beta_2 ser_t + \\beta_3 agri_t + \\beta_4 man_t + \\mu_t\n\\tag{1}\\]\n\n\nPara a análise gráfico podemos fazer de duas formas: através de um gráfico de dispersão entre duas variáveis, ou através de dois gráficos de linhas comparando as séries temporais das variáveis.\nO grafíco de dispersão entre duas variáveis pode ser feito com a função ggplot(). Para este exemplo camos comparar a variável man com a ind:\n\nggplot(m_reg, aes(x = man, y = ind)) +\n  geom_point(alpha = 0.7, color = \"steelblue\", size = 2.5) +\n  labs(title = \"Gráfico de Dispersão: man vs ind\",\n       x = \"Indústria Transformadora\",\n       y = \"Indústria\") +\n       theme_minimal() + #tema para o gráfico\n       theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray50\"),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nAtravés do gráfico de dispersão, podemos observar a relação entre as variáveis man e ind, o que pode indicar a presença de multicolinearidade imperfeita. Pois parece existe uma relação linear entre as duas variáveis (é possível fazer uma regressão linear bem ajustada para representar as observações). Se não fosse possível fazer uma regressão linear bem ajustada, poderíamos considerar que não há multicolinearidade entre as duas variáveis.\nPara o gráfico de linhas:\n\n# Criar dados para o gráfico\ndados_graf &lt;- data.frame(\n     tempo = 1:nrow(m_reg),\n     man = m_reg$man,\n     ind = m_reg$ind\n)\n\n# Gráfico combinado\nggplot(dados_graf, aes(x = tempo)) +\n     geom_line(aes(y = man, color = \"Indústria Transformadora\"), size = 1.2) +\n     geom_line(aes(y = ind, color = \"Indústria\"), size = 1.2) +\n     scale_color_manual(values = c(\"Indústria Transformadora\" = \"steelblue\", \n                                   \"Indústria\" = \"red\")) +\n     labs(title = \"Comparação das Séries Temporais: man vs ind\",\n                x = \"Tempo\",\n                y = \"Valor\",\n                color = \"Variáveis\") +\n     theme_minimal() +\n     theme(legend.position = \"bottom\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nO comportamento das séries ao longo do tempo é muito semelhantes, o que pode indicar a presença de multicolinearidade imperfeita.\n\n\n\nOutra forma de detetar a multicolinearidade é através da matriz de correlação. A matriz de correlação mostra o grau de associação linear entre as variáveis. Cada valor na matriz varia entre -1 e 1, onde valores próximos de 1 ou -1 indicam uma forte correlação, o que pode indicar a presença de multicolinearidade.\nPara a matriz de correlação, podemos utilizar a função cor() do R. Vamos calcular a matriz de correlação para as variáveis de interesse do conjunto de dados m_reg.\n\n# selecionar variáveis de interesse\nmreg_sel &lt;- m_reg %&gt;%\n    select(ind, ser, agri, man)\n# Matriz de correlação\ncor_matrix &lt;- cor(mreg_sel)\ncor_matrix\n\n           ind       ser      agri       man\nind  1.0000000 0.7093591 0.5217541 0.9155984\nser  0.7093591 1.0000000 0.7109302 0.9103241\nagri 0.5217541 0.7109302 1.0000000 0.6737959\nman  0.9155984 0.9103241 0.6737959 1.0000000\n\n\nOnde podemos ver que as variáveis ind e man têm uma correlação alta, o que pode indicar a presença de multicolinearidade. Nas restantes a correlação não é assim tão elevada.\nTambém podemos criar uma matriz gráfica com o package corrplot. Informações detalhadas sobre esta biblioteca podem ser encontradas em An Introduction to corrplot Package.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\n#com números\ncor_1 &lt;- corrplot(cor_matrix, method = \"number\")\n\n\n\n\n\n\n\n#com círculos\ncor_2 &lt;- corrplot(cor_matrix, method = \"circle\")\n\n\n\n\n\n\n\n#com números e circulos\ncor_3 &lt;- corrplot.mixed(cor_matrix)\n\n\n\n\n\n\n\n\n\n\n\nO primeiro passo é estimar o modelo de regressão com as variáveis independentes para Equation 1:\n\nmodelo_multi &lt;- lm(gdp ~ ind + ser + agri + man, data = m_reg)\nsummary(modelo_multi)\n\n\nCall:\nlm(formula = gdp ~ ind + ser + agri + man, data = m_reg)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.222e+10 -3.659e+09  3.956e+08  3.477e+09  1.778e+10 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.293e+10  2.724e+10   2.678   0.0137 *  \nind          1.619e+00  1.868e-01   8.665 1.53e-08 ***\nser          1.396e+00  2.647e-02  52.754  &lt; 2e-16 ***\nagri        -5.464e-01  8.305e-01  -0.658   0.5174    \nman         -3.940e-01  4.227e-01  -0.932   0.3615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.563e+09 on 22 degrees of freedom\nMultiple R-squared:  0.9995,    Adjusted R-squared:  0.9994 \nF-statistic: 1.112e+04 on 4 and 22 DF,  p-value: &lt; 2.2e-16\n\n\nNo resultado anterior temos 2 de 4 variáveis que não são estatisticamente diferentes de 0 (agri,man`), contudo o modelo tem um R² ajustado muito elevado (0.99), o que pode indicar a presença de multicolinearidade. Neste caso não conseguimos determinar quais as variáveis que estão a causar este problema.\n\n\n\nEste método consiste em estimar a regressão entre duas das variáveis independentes, ou seja, estimar a regressão do método de deteção de multicolinearidade pelo gráfico de dispersão. Quais as variáveis que devemos escolher neste caso? Com já temos uma anáilise de correlações e gráfica, já sabemos quais as que devemos testar. Caso contrário teríamos que fazer várias regressões com duas das variáveis independentes de cada vez. Para a regressão:\n\\[\nman_t = \\beta_0 + \\beta_1 ind_t + \\epsilon_t\n\\]\nNo R:\n\nmodelo_ind &lt;- lm(man ~ ind - 1, data = m_reg)\nsummary(modelo_ind)\n\n\nCall:\nlm(formula = man ~ ind - 1, data = m_reg)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.420e+10 -6.354e+09 -4.233e+09  7.569e+09  1.409e+10 \n\nCoefficients:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nind 0.565867   0.004297   131.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.529e+09 on 26 degrees of freedom\nMultiple R-squared:  0.9985,    Adjusted R-squared:  0.9984 \nF-statistic: 1.734e+04 on 1 and 26 DF,  p-value: &lt; 2.2e-16\n\n\nO R² é muito elevado (0.999), o que indica a presença de multicolinearidade entre as variáveis man e ind. O que indique que a variação de man pode ser explicada em grande parte pela variação de ind.\nem que graficamente:\n\nggplot(m_reg, aes(x = ind, y = man)) +\n  geom_point(alpha = 0.7, color = \"steelblue\", size = 5.5) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x - 1, \n              color = \"red\", se = FALSE, \n              alpha = 0.2) +\n  labs(title = \"Regressão Linear: Indústria vs Indústria Transformadora\",\n       x = \"Indústria (ind)\",\n       y = \"Indústria Transformadora (man)\") +\n  theme_minimal() + #tema para o gráfico\n  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nA conclusão permance, suspeitamos que existe multicolinearidade imperfeita entre estas duas variáveis. També poderíamos inverter a variável dependente pela independente:\n\nmodelo_ind_inv &lt;- lm(ind ~ man - 1, data = m_reg)\nsummary(modelo_ind_inv)\n\n\nCall:\nlm(formula = ind ~ man - 1, data = m_reg)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.425e+10 -1.278e+10  8.000e+09  1.181e+10  2.554e+10 \n\nCoefficients:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nman   1.7646     0.0134   131.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.506e+10 on 26 degrees of freedom\nMultiple R-squared:  0.9985,    Adjusted R-squared:  0.9984 \nF-statistic: 1.734e+04 on 1 and 26 DF,  p-value: &lt; 2.2e-16\n\n\nEm que podemos observar a mesma conclusão.\n\n\n\nPor último o teste VIF que é o mais utilizado em econometria. O teste VIF consiste em calcular para cada uma das variáveis independentes quanto a variância de um coeficiente da regressão é inflacionada devido à presença de multicolinearidade. O VIF é dado por:\n\\[\nVIF_i = \\frac{1}{1 - R^2_i}\n\\tag{2}\\]\nem que \\(R^2_i\\) é o coeficiente de determinação da regressão da variável \\(X_i\\) em função das restantes variáveis independentes. Para a equação Equation 1 (objeto modelo_multi) vamos estimar a regressão de cada variável independente em função das restantes:\n\nmodelo_multi_ind &lt;- lm(ind ~ ser + agri + man, \n                      data = m_reg)\nmodelo_multi_ser &lt;- lm(ser ~ ind + agri + man, \n                       data = m_reg)\nmodelo_multi_agri &lt;- lm(agri ~ ind + ser + man, \n                        data = m_reg)\nmodelo_multi_man &lt;- lm(man ~ ind + ser + agri, \n                       data = m_reg)\n\nAgora vamos extrair o R^2 de cada uma das regressões ($r.squared) e calcular o VIF com a Equation 2:\n\n# Extrair R²\nr2_ind &lt;- summary(modelo_multi_ind)$r.squared\nr2_ser &lt;- summary(modelo_multi_ser)$r.squared\nr2_agri &lt;- summary(modelo_multi_agri)$r.squared\nr2_man &lt;- summary(modelo_multi_man)$r.squared\n\n# Calcular VIF\nvif_ind &lt;- 1 / (1 - r2_ind)\nvif_ser &lt;- 1 / (1 - r2_ser)\nvif_agri &lt;- 1 / (1 - r2_agri)\nvif_man &lt;- 1 / (1 - r2_man)\n\n#mostrar o VIF\nvif_values &lt;- data.frame(\n  Variable = c(\"Indústria\",\n                \"Serviços\",\n                \"Agricultura\",\n                \"Indústria Transformadora\"),\n  VIF = c(vif_ind, vif_ser, vif_agri, vif_man)\n)\nprint(vif_values)\n\n                  Variable       VIF\n1                Indústria 14.182238\n2                 Serviços 13.372233\n3              Agricultura  2.074316\n4 Indústria Transformadora 41.424816\n\n\nEm alternativa, e esta bem mais prática, podemos usar a função vif() da biblioteca car:\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nvif(modelo_multi)\n\n      ind       ser      agri       man \n14.182238 13.372233  2.074316 41.424816 \n\n\nA biblioteca performance pode ser utilizada para calcular o VIF e outras métricas para uma análise de colinearidade mais informativa. Para isso recorremos à função check_collinearity().\n\nlibrary(performance)\ncheck_collinearity(modelo_multi)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Term  VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n agri 2.07 [ 1.47,  3.43]     1.44      0.48     [0.29, 0.68]\n\nHigh Correlation\n\n Term   VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n  ind 14.18 [ 8.39, 24.50]     3.77      0.07     [0.04, 0.12]\n  ser 13.37 [ 7.93, 23.09]     3.66      0.07     [0.04, 0.13]\n  man 41.42 [24.00, 72.05]     6.44      0.02     [0.01, 0.04]\n\n\nEsta função (mais info ?check_collinearity) fornece uma visão geral da colinearidade entre as variáveis independentes. Separa as variáveis em grupos com base na sua colinearidade (baixa e alta). apresenta também o Intervalo de confiança para o VIF (VIF 95% CI). Apresenta mais duas métricas como a Increased SE que indica o aumento percentual do erro padrão do coeficiente devido à colinearidade, e a Tolerance que é o inverso do VIF (1/VIF). Análise para a variável man:\n\nIncreased SE:6.44 - 6.44 vezes maior que o desvio padrão esperado para o coeficiente\nTolerance:0.02 - Indica que apenas 2% da variância é independente\n\nPortanto, podemos concluir que existe uma forte multicolinearidade.\n\n\n\n\nPara esta aplicação vamos utilizar o conjunto de dados hprice1 da biblioteca wooldridge.\n\n# Carregar bibliotecas necessárias\nlibrary(wooldridge)\nlibrary(tidyverse)\n\n# Carregar dados\ndata(\"hprice1\")\n\nA heterocedasticidade é medida nos erros. Para isso, vamos utilizar o modelo base (modelo_0) e obter os resíduos u_i:\n\n#estimar modelo\nmodelo_0 &lt;- lm(price ~ lotsize + sqrft + bdrms,\n                     data = hprice1)\n\n# armazenar erro no data frame\nhprice1$u_i &lt;- residuals(modelo_0)\nsummary(hprice1$u_i) #eststistica descritiva do erro\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-120.026  -38.530   -6.555    0.000   32.323  209.376 \n\n\n\n\nPara testar a heterocedasticidade graficamente, podemos utilizar um gráfico de dispersão dos resíduos em relação aos valores ajustados ou com outra variável independente. Se os resíduos apresentarem um padrão específico (como um funil ou uma curva), pode indicar a presença de heterocedasticidade dos erros.\n\n# Gráfico de dispersão dos resíduos\nggplot(hprice1, aes(x = fitted(modelo_0), y = u_i)) +\n  geom_point(alpha = 0.7, color = \"steelblue\", size = 2.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Gráfico de Resíduos vs Valores Ajustados\",\n       x = \"Valores Ajustados\",\n       y = \"Resíduos\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\nUm dos testes mais utilizado para testar a heterocedasticidade é o teste de Breusch-Pagan [@breusch_simple_1979]. Os testes têm um processo semelhante como referido na secção de pressupostos de homocedasticidade do capítulo anterior. A regressão auxiliar do teste é dada por:\n\\[\n{\\mu_i}^2 = \\delta_0 + \\delta_1 lotsize_i + \\delta_2 sqrft_i + \\delta_3 bdrms_i + \\sigma_i\n\\]\nDepois de estimar vamos obter o \\(LM_{stat}=n*R^2\\) e calcular o valor da probabilidade do \\(\\chi^2\\).\nNo R:\n\n#calcular u_i^2\nhprice1$u_i2 &lt;- hprice1$u_i^2\n\n#Estimar reg-bp\nreg_bp &lt;- lm(u_i2 ~ lotsize + sqrft + bdrms, data = hprice1)\n\n#Obter R2\nr2_bp &lt;- summary(reg_bp)$r.squared\n\n# Obter n\nn &lt;- nrow(hprice1)\n\n# Número de variáveis independentes - k\nk &lt;- reg_bp$rank - 1\n\n# LM stat\nLM_stat_bp &lt;- n * r2_bp\n\n#Obter valor P\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\np_value_bp &lt;- 1 - pchisq(LM_stat_bp, df = k)\np_value_bp\n\n[1] 0.00278206\n\n\nO $r.squared extrai o valor de R² da regressão auxiliar, que é utilizado para calcular o estatístico LM do teste de Breusch-Pagan. O $rank devolve o número de coeficientes (-1 para excluis a constante). A função pchisq() é utilizada para calcular o valor p associado ao estatístico LM, com base na distribuição qui-quadrado, onde df é o número de variáveis independentes na regressão auxiliar (k). Este processo pode ser mais trabalhoso, mas é possível executar qualquer test de heterocedasticidade.\nA biblioteca skedastic tem várias funções para testar a heterocedasticidade. Neste caso, o teste de Breusch-Pagan pode ser obtido com a função breusch_pagan():\n\nlibrary(skedastic)\nbreusch_pagan(modelo_0)\n\n# A tibble: 1 × 5\n  statistic p.value parameter method                alternative\n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;      \n1      14.1 0.00278         3 Koenker (studentised) greater    \n\n\nA conclusão (o valor de P é o mesmo) é que a hipótese nula é rejeitada para qualquer nível de significância estatística. Em que a hipótese nula do teste de heterocedasticidade é: os erros são homocedásticos.\n\n\n\nO teste de Glesjer [@glejser_new_1969] tem como a regressão auxiliar:\n\\[\n|\\mu_i| = \\delta_0 + \\delta_1 \\cdot lotsize_i + \\delta_2 \\cdot sqrft_i + \\delta_3 \\cdot bdrms_i + \\sigma_i\n\\tag{3}\\]\nO processo é semelhante ao teste de Breusch-Pagan. No R:\n\n# Calcular |u_i|\nhprice1$u_i_abs &lt;- abs(hprice1$u_i)\n\n# Estimar reg-glesjer\nreg_glesjer &lt;- lm(u_i_abs ~ lotsize + sqrft + bdrms, \n                  data = hprice1)\n\n# Obter R2\nr2_glesjer &lt;- summary(reg_glesjer)$r.squared\n\n# Obter n\nn &lt;- nrow(hprice1)\n\n# Número de variáveis independentes - k\nk &lt;- reg_glesjer$rank - 1\n\n# LM stat\nLM_stat_glesjer &lt;- n * r2_glesjer\n\n# Obter valor P\nlibrary(lmtest)\np_value_glesjer &lt;- 1 - pchisq(LM_stat_glesjer, \n                        df = k)\np_value_glesjer\n\n[1] 0.0004458974\n\n\nO mesmo resultado pode ser obtido utilizando a função gl() da biblioteca skedastic. A hipótese nula é rejeitada para qualquer nível de significância estatística.\n\n\nO teste Harvey-Godfrey [@harvey_estimating_1976, @godfrey_testing_1978] tem como regressão auxiliar:\n\\[\nln(\\mu_i^2) = \\delta_0 + \\delta_1 lotsize_i + \\delta_2 sqrft_i + \\delta_3 bdrms_i + \\sigma_i\n\\tag{4}\\]\nO processo é semelhante a qualquer LM teste para heterocedasticidade. Tabém pode ser obtido utilizando a função harvey_godfrey() da biblioteca skedastic:\n\nharvey(modelo_0)\n\n# A tibble: 1 × 4\n  statistic p.value parameter alternative\n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1      8.49  0.0370         3 greater    \n\n\nNeste caso a hipótese nula é rejeitada para 5% e 10% de significância estatística.\n\n\n\n\nO teste de Park [@park_estimation_1966] tem como regressão auxiliar:\n\\[\nln(\\mu_i^2) = \\delta_0 + \\delta_1 ln(lotsize_i) + \\delta_2 ln(sqrft_i) + \\delta_3 ln(bdrms_i) + \\sigma_i\n\\tag{5}\\]\nA bibliotca skedastic não tem o teste de Park. Este é um dos exemplos em que temos que fazer o processo passo a passo:\n\n# Calcular ln(u_i^2)\nhprice1$ln_u_i2 &lt;- log(hprice1$u_i^2)\n\n# Estimar regressão auxiliar\nreg_park &lt;- lm(ln_u_i2 ~ log(lotsize) + log(sqrft) + log(bdrms),\n                data = hprice1)\n\n# Obter R2\nr2_park &lt;- summary(reg_park)$r.squared\n\n# Obter n\nn &lt;- nrow(hprice1)\n\n# Número de variáveis independentes - k\nk &lt;- reg_park$rank - 1\n\n# LM stat\nLM_stat_park &lt;- n * r2_park\n\n# Obter valor P\nlibrary(lmtest)\np_value_park &lt;- 1 - pchisq(LM_stat_park, df = k)\np_value_park\n\n[1] 0.1142573\n\n\nA hipótese nula de homocedasticidade não é rejeitada para nenhum nível de significância estatística.\n\n\n\nO teste de White [@white_heteroskedasticity-consistent_1980] é um teste geral para heterocedasticidade que não assume uma forma específica de heterocedasticidade. A regressão auxiliar do teste de White inclui as variáveis independentes e os termos quadráticos:\n\\[\n\\begin{split}\n{\\mu_i}^2 = \\delta_0 & + \\delta_1 lotsize_i + \\delta_2 sqrft_i + \\delta_3 bdrms_i \\\\\n&+ \\delta_4 lotsize_i^2 + \\delta_5 sqrft_i^2 + \\delta_6 bdrms_i^2 + \\sigma_i\n\\end{split}\n\\tag{6}\\]\ne pode incluir também interações entre as variáveis independentes :\n\\[\n\\begin{split}\n{\\mu_i}^2 = \\delta_0 &+ \\delta_1 lotsize_i + \\delta_2 sqrft_i + \\delta_3 bdrms_i \\\\\n&+ \\delta_4 lotsize_i^2 + \\delta_5 sqrft_i^2 + \\delta_6 bdrms_i^2 \\\\\n&+ \\delta_7 (lotsize_i \\cdot sqrft_i) + \\delta_8 (lotsize_i \\cdot bdrms_i) \\\\\n&+ \\delta_9 (sqrft_i \\cdot bdrms_i) + \\sigma_i\n\\end{split}\n\\tag{7}\\]\nPara a Equation 6 podemos utilizar a função white() da biblioteca skedastic:\n\nwhite(modelo_0)\n\n# A tibble: 1 × 5\n  statistic p.value parameter method       alternative\n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n1      20.6 0.00216         6 White's Test greater    \n\n\ne para a Equation 7 podemos utilizar a função white() com o argumento interactions = TRUE:\n\nwhite(modelo_0, interactions = TRUE)\n\n# A tibble: 1 × 5\n  statistic   p.value parameter method       alternative\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n1      33.7 0.0000995         9 White's Test greater    \n\n\nPara qualquer uma das variações do teste, a hipótese nula é rejeitada para todos os níveis de significância estatística.\n\n\n\nO teste de ARCH [@engle_autoregressive_1982] pode ser utilizado para detectar a presença de heterocedasticidade condicional, especialmente em séries temporais. A regressão auxiliar do teste de ARCH é dada por:\nPara este exemplo vamo utilizar os dados do ficheiro m_reg.xlsx.\n\nlibrary(readxl)\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\nEstimar o modelo de regressão:\n\nmodelo_0 &lt;- lm(gdp ~ ind + ser + agri + man, data = m_reg)\n\nPara executar o teste de ARCH utilizado a função ArchTest() da biblioteca FinTS:\n\nlibrary(FinTS)\nteste_arch &lt;- ArchTest(residuals(modelo_0), lags = 1)\nteste_arch\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  residuals(modelo_0)\nChi-squared = 2.6047, df = 1, p-value = 0.1065\n\n\nO argumento lags especifica o número de defasamentos do teste, ou seja, a ordem do modelo ARCH a ser testada. Para a ordem 1, existe heterocedasticidade para qualquer nível de significância estatística (valor P &lt; 0.01). Em que a hipótese nula do teste é: não existem efeitos ARCH para a ordem 1.\n\n\n\nA biblioteca skedastic oferece uma variedade de outros testes para heterocedasticidade, para mais testes:\n\nlibrary(skedastic)\nls(\"package:skedastic\")\n\n [1] \"alvm.fit\"          \"anlvm.fit\"         \"anscombe\"         \n [4] \"avm.ci\"            \"avm.fwls\"          \"avm.vcov\"         \n [7] \"bamset\"            \"bickel\"            \"blus\"             \n[10] \"bootlm\"            \"breusch_pagan\"     \"carapeto_holt\"    \n[13] \"cook_weisberg\"     \"countpeaks\"        \"dDtrend\"          \n[16] \"diblasi_bowman\"    \"dpeak\"             \"dpeakdat\"         \n[19] \"dufour_etal\"       \"evans_king\"        \"glejser\"          \n[22] \"godfrey_orme\"      \"goldfeld_quandt\"   \"GSS\"              \n[25] \"harrison_mccabe\"   \"harvey\"            \"hccme\"            \n[28] \"hetplot\"           \"honda\"             \"horn\"             \n[31] \"li_yao\"            \"pDtrend\"           \"ppeak\"            \n[34] \"pRQF\"              \"rackauskas_zuokas\" \"simonoff_tsai\"    \n[37] \"szroeter\"          \"T_alpha\"           \"twosidedpval\"     \n[40] \"verbyla\"           \"white\"             \"wilcox_keselman\"  \n[43] \"yuce\"              \"zhou_etal\"        \n\n\nA biblioteca performance oferece funções para avaliar o desempenho de modelos de regressão, incluindo testes de heterocedasticidade, como a check_heteroscedasticity():\n\nlibrary(performance)\ncheck_heteroscedasticity(modelo_0)\n\nOK: Error variance appears to be homoscedastic (p = 0.145).\n\n\nEsta função realiza o teste de Breusch-Pagan LM e fornece até uma resposta visual.\n\n\n\n\n\nNesta aplicação vamos utilizar os dados do exemplos anterior (ficheiro m_reg.xlsx) e estimar o modelo:\n\n#carregar bibliotecas\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(performance)\n\n#carregar dados\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\n#estimar modelo\nmodelo &lt;- lm(gdp ~ ind + ser + agri + man, data = m_reg)\n\n\n\nA forma informal de detetar a heterocedaticide é através de um gráfico de dispersão dos resíduos e dos resíduos desfasados do modelo. Para isso é necessário o obter os resíduos (u_t) do modelo e os resíduos desfasados (u_t1). No R:\n\nm_reg &lt;- m_reg |&gt; \n          mutate(\n            u_t = residuals(modelo), \n            u_t1 = lag(u_t))\n\nGráfico de dispersão dos resíduos com ggplot():\n\nlibrary(ggplot2)\n\n#| label: grafico_residuos_autocorr\nggplot(m_reg, aes(x = u_t1, y = u_t)) +\n  geom_point(alpha = 0.7, color = \"steelblue\", size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Gráfico de Dispersão dos Resíduos vs Resíduos Desfasados\",\n       x = \"Resíduos Desfasados (u_t-1)\",\n       y = \"Resíduos (u_t)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank())\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAtravés do gráfico podemos que existe autocorrelação nos resíduos.\n\n\n\nO teste de Durbin-Watson (DW) [@durbin_testing_1950; @durbin_testing_1951] pode ser utilizado para detectar a presença de autocorrelação nos resíduos de um modelo de regressão. O teste tem os seguintes pressupostos:\n\nO modelo tem constante\nAutocorrelação de 1ª ordem\nNão existe variável dependente desfasada com independente\n\nO passo a passo para executar o teste de Durbin-Watson é o seguinte:\n\nEstimar o modelo e obter os resíduos.\nCalcular a estatística de DW\n\n\\[\nDW_{stat} = \\frac{\\sum_{t=2}^{n} (u_t - u_{t-1})^2}{\\sum_{t=1}^{n} u_t^2}\n\\tag{8}\\]\nNo R:\n\n# Calcular numerador: soma de (u_t - u_{t-1})^2\nnumerador &lt;- sum((m_reg$u_t[-1] - m_reg$u_t[-nrow(m_reg)])^2, na.rm = TRUE)\n\n# Calcular denominador: soma de u_t^2\ndenominador &lt;- sum(m_reg$u_t^2, na.rm = TRUE)\n\n# Calcular estatística DW\nDW_stat &lt;- numerador / denominador\nDW_stat\n\n[1] 1.262287\n\n\nO valor de \\(DW_{stat}\\) pode também ser obtido através da função dwtest() da biblioteca lmtest:\n\n#|label: dw_test\nlibrary(lmtest)\ndwtest(modelo)\n\n\n    Durbin-Watson test\n\ndata:  modelo\nDW = 1.2623, p-value = 0.004077\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\nConstruir a tabela com o \\(DW_{stat}\\):\n\n\n\n\n\n\n\n\n\n\nOnde:\n\n\\(d_I\\) é a distância de Durbin-Watson inferior\n\\(d_S\\) é a distância de Durbin-Watson superior.\n\nOs limites para cada nível de significância podem ser obtidas em: Durbin-Watson Significance Tables\nEm alternativa, é possivel calcular o \\(DW_stat\\) e obter diretamente o valor da probabilidade com a função dwtest() da biblioteca lmtest:\n\n#|label: dw_test\nlibrary(lmtest)\ndwtest(modelo)\n\n\n    Durbin-Watson test\n\ndata:  modelo\nDW = 1.2623, p-value = 0.004077\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\nConcluir (a hipótese nula é que não existe autocorrelação)\n\nA hipótese nula do teste de DW é que não existe autocorrelação. Considerando o valor da probabilidade, a Hipótese nula é rejeitada para qualquer nível de significância estatística.\nA biblioteca performance também tem a função check_autocorrelation() que pode ser utilizada para testar a autocorrelação dos resíduos:\n\n#|label: performance_check_autocorrelation\nlibrary(performance)\ncheck_autocorrelation(modelo)\n\nWarning: Autocorrelated residuals detected (p = 0.010).\n\n\nO teste DW tem algumas limitações, já mencionadas nos pressupostos. Tem ainda também a limitação de o \\(DW_stat\\) ficar nas zonas de indecisão e com isto não ser conclusivo. Existe também o teste Durbin h-H (DH) que é uma extensão do teste DW e que pode ser utilizado quando a variável dependente é defasada. O teste DH não é tão utilizado como o DW.\n\n\n\nO teste de Breusch-Godfrey [@breusch_auto_1978; @godfrey_testing_1978] tem vantagem de poder ser utilizado para detetar autocorrelação de ordens superiores a 1. O teste tem como base a equação:\n\\[\n\\begin{split}\ngdp_t = \\beta_0 & + \\beta_1 inf_t + \\beta_2 ser_t+ \\beta_3 agri_t + \\beta_4 man_t + \\mu_t\n\\end{split}\n\\tag{9}\\]\nonde:\n\\[\n\\mu_t = \\rho_1 \\mu_{t-1} + \\rho_2 \\mu_{t-2} + ... + \\rho_p \\mu_{t-p} + \\epsilon_t\n\\tag{10}\\]\nonde \\(p\\) é a ordem de autocorrelação a ser testada.\nPortanto, a H0: \\(\\rho_1 = rho_2 = ... = rho_p = 0\\) (não existe autocorrelação) Enquanto que a H1: pelo menos um \\(\\rho_i \\neq 0\\) (existe autocorrelação). O teste é feito com o modelo LM (ou seja Equation 9 e a Equation 10) em que o \\(LM_{stat} = (n-\\rho)R^2\\) e o valor de P é obtido através da distribuição \\(\\chi^2\\). O teste pode ser calculado de forma conveniente com a função bgtest():\n\n#|label: bg_test\nlibrary(lmtest)\n\n#odem 1\nbgtest(modelo)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  modelo\nLM test = 1.0179, df = 1, p-value = 0.313\n\n#ordem 2\nbgtest(modelo, order = 2, type = \"Chisq\")\n\n\n    Breusch-Godfrey test for serial correlation of order up to 2\n\ndata:  modelo\nLM test = 1.0368, df = 2, p-value = 0.5955\n\n\nO primeiro argumento da função é objeto do modelo. Existem mais alguns argumentos que são necessários order (a ordem da autocorrelação a ser testada) e type (o tipo de teste a ser realizado, como Chisq ou F). Por defeito, o argumento order é 1 e o argumento type é Chisq. A hipótese nula é rejeitada para qualquer nível de significância estatística.\nSegundo o resultado do teste, a hipótese nula não foi rejeitada, o que indique que não existe autocorrelação de ordem 1 nem ordem 2.\n\n\n\n\nTal como os diagnósticos anteriores, este também pode ser feito através de gráficos e de testes estatísticos. Para esta aplicação vamos utilizar o modelo anterior.\n\n#carregar bibliotecas\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(performance)\n\n#carregar dados\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\n#estimar modelo\nmodelo &lt;- lm(gdp ~ ind + ser + agri + man, data = m_reg)\n\n#obter erro\nm_reg$residuos &lt;- residuals(modelo)\n\n\n\nPara o gráfico de histograma dos resíduos e comparar com a linha de normalidade:\n\nlibrary(ggplot2)\n\nggplot(m_reg, aes(x = residuos)) +\n  geom_histogram(aes(y = after_stat(density)),\n                bins = 30, fill = \"steelblue\", \n                color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(m_reg$residuos), \n                           sd = sd(m_reg$residuos)), \n                color = \"red\", size = 1.2) +\n  labs(title = \"Histograma dos Resíduos com Curva Normal\", \n       x = \"Resíduos\", \n       y = \"Densidade\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUtilizando exclusivamente a biblioteca o histograma dos resíduos podemos concluir que aparentemente os resíduos seguem uma distribuição normal. É perfeitamente normal em amostras pequenas haver desvios (barras) em relação à distribuição normal (linha vermelha)\n\n\n\nO teste de @shapiro_analysis_1965 (SW) é um dos testes mais utilizados para testar a normalidade dos resíduos. A hipótese nula é que os resíduos seguem uma distribuição normal. O teste pode ser realizado com a função shapiro.test():\n\nshapiro.test(m_reg$residuos)\n\n\n    Shapiro-Wilk normality test\n\ndata:  m_reg$residuos\nW = 0.96487, p-value = 0.4735\n\n\nA H0 não é rejeitada para qualquer nível de significância estatística. O W é a estatística do teste. O teste SW tem um grande poder estatístico em detetar desvios em relação à normalidade, pois é muit sensível a desvios tanto no centro como nas caudas da distribuição.\nA função check_normality() da biblioteca performance tabém realizado o teste SW:\n\nlibrary(performance)\n\ncheck_normality(modelo)\n\nOK: residuals appear as normally distributed (p = 0.258).\n\n\n\n\n\nO teste de @jarque_test_1987 (JB) é outro teste muito utilizado para verificar a normalidade dos resíduos. A hipótese nula é a mesma do teste SW. O teste pode ser realizado com a função jarque.test() da biblioteca moments.\n\n#carregar biblioteca\nlibrary(moments)\n\n#teste de normalidade\njarque.test(m_reg$residuos)\n\n\n    Jarque-Bera Normality Test\n\ndata:  m_reg$residuos\nJB = 3.3251, p-value = 0.1897\nalternative hypothesis: greater\n\n\nComo H0 não é rejeitada, os erros seguem uma distribuição normal. Se H0 for rejeitada podemo obter os valores de curtose e assimetria para perceberem a razão dos resíduos não seguirem uma distribuição normal:\n\n#Kurtosis (achatamento)\nkurtosis(m_reg$residuos)\n\n[1] 4.288396\n\n#Skewnedd (assimetria)\nskewness(m_reg$residuos)\n\n[1] 0.5691352\n\n\n\n\n\n\n\n\nA especificação de um modelo de regressão está relacionada com a inclusão de variáveis (Overfitting) que não são relevantes e a exclusão de variáveis relevantes (Underfitting).\nA não inclusão de variáveis relevantes pode levar a um modelo mal especificado, em que o valor esperado do resíduos \\(E[\\mu_i|X_i] \\neq 0\\), o que faz com que essa variável (omitida) seja confundida com o erro. Já a inclusão de variáveis irrelevantes pode levar a um modelo com excesso de ajustamento, em que o valor esperado do resíduos \\(E[\\mu_i|X_i] = 0\\), mas a inclusão de variáveis desnecessárias (com informação redundante) pode aumentar a variância dos estimadores.\nPara este exemplo vamos utilizar os dados wage2 da biblioteca wooldridge:\n\nlibrary(wooldridge)\ndata(\"wage2\")\n\nPara ver a definição das variáveis executar ?wage2.\nVamos estimar o seguinte modelo: \\[\nwage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 tenure_i + \\mu_i\n\\tag{11}\\]\ne o modelo sem a variável tenure:\n\\[\nwage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\mu_i\n\\tag{12}\\]\nNo R:\n\nmodelo_completo &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\nmodelo_restrito &lt;- lm(wage ~ educ + exper, data = wage2)\n\n#comparar os modelos\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nstargazer::stargazer(modelo_completo, modelo_restrito, type = \"text\")\n\n\n===================================================================\n                                  Dependent variable:              \n                    -----------------------------------------------\n                                         wage                      \n                              (1)                     (2)          \n-------------------------------------------------------------------\neduc                       74.415***               76.216***       \n                            (6.287)                 (6.297)        \n                                                                   \nexper                      14.892***               17.638***       \n                            (3.253)                 (3.162)        \n                                                                   \ntenure                     8.257***                                \n                            (2.498)                                \n                                                                   \nConstant                  -276.240***             -272.528**       \n                           (106.702)               (107.263)       \n                                                                   \n-------------------------------------------------------------------\nObservations                  935                     935          \nR2                           0.146                   0.136         \nAdjusted R2                  0.143                   0.134         \nResidual Std. Error   374.306 (df = 931)      376.295 (df = 932)   \nF Statistic         53.003*** (df = 3; 931) 73.260*** (df = 2; 932)\n===================================================================\nNote:                                   *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nAo comparar os dois modelos podemos ver que sinais dos coeficinets são consistentes e os valores muito semelhantes. A variável exper representa os anos de experiência profissional e a variável tenure representa os anos na empresa. Será que estas variáveis contém a mesma informação? Pos os anos de exper podem estar contabilizados nos anos de tenure. Para testar vamos utilizar a função waldtest da biblioteca lmtest:\n\nlibrary(lmtest)\nwaldtest(modelo_restrito, modelo_completo)\n\nWald test\n\nModel 1: wage ~ educ + exper\nModel 2: wage ~ educ + exper + tenure\n  Res.Df Df      F   Pr(&gt;F)    \n1    932                       \n2    931  1 10.929 0.000983 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPara este caso, estamos a testar se a variável tenure pode ser omitida do modelo. E concluímos que a variável tenure é necessária, ou seja, a informação da variável vai para além da informação do nº de anos de experiência. Está acapturar efeitos como a lealdade do funcionário à empresa, aumento da produtividade ao longo do tempo, entre outros fatores. A variável exper representa apenas a experiência profissional (pode ser numa profissão totalmente diferente). O que nos pode levar a pensar que o modelo está mal especificado se omitirmos a variável tenure. Este resultado també nos pode fazer pensar se a variável exper é mesmo necessário, para despistar vamos fazer um modelo omitindo a variável exper e realziar o teste:\n\nmodelo_completo2 &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\nmodelo_restrito2 &lt;- lm(wage ~ educ + tenure, data = wage2)\n\n#comparar os modelos\nlibrary(stargazer)\nstargazer::stargazer(modelo_completo2, modelo_restrito2, type = \"text\")\n\n\n===================================================================\n                                  Dependent variable:              \n                    -----------------------------------------------\n                                         wage                      \n                              (1)                     (2)          \n-------------------------------------------------------------------\neduc                       74.415***               61.148***       \n                            (6.287)                 (5.639)        \n                                                                   \nexper                      14.892***                               \n                            (3.253)                                \n                                                                   \ntenure                     8.257***                11.177***       \n                            (2.498)                 (2.441)        \n                                                                   \nConstant                  -276.240***               53.519         \n                           (106.702)               (79.557)        \n                                                                   \n-------------------------------------------------------------------\nObservations                  935                     935          \nR2                           0.146                   0.127         \nAdjusted R2                  0.143                   0.125         \nResidual Std. Error   374.306 (df = 931)      378.293 (df = 932)   \nF Statistic         53.003*** (df = 3; 931) 67.579*** (df = 2; 932)\n===================================================================\nNote:                                   *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\nwaldtest(modelo_restrito2, modelo_completo2)\n\nWald test\n\nModel 1: wage ~ educ + tenure\nModel 2: wage ~ educ + exper + tenure\n  Res.Df Df      F    Pr(&gt;F)    \n1    932                        \n2    931  1 20.957 5.333e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConcluímos que a variável exper também é necessária no modelo.\nE está também relacionada com a forma funcional do modelo. Algumas formas funcionais são:\n\nLinear\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0+\\beta_1x_{i}+\\mu_i\n\\tag{13}\\]\n\\(\\beta_0\\): representa o valor esperado de \\(y_i\\) quando \\(x_i=0\\)\n\\(\\beta_1\\): um aumento de uma unidade em \\(x_i\\) está associado a um aumento de \\(\\beta_1\\) unidades em \\(y_i\\), ceteris paribus.\n\nLinear - logarítmica\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0+\\beta_1\\ln(x_{i})+\\mu_i\n\\tag{14}\\]\n\\(\\beta_0\\): representa o valor esperado de \\(y_i\\) quando \\(ln(x_i)=0\\) (ou seja, \\(x_i=1\\))\n\\(\\beta_1\\): um aumento de 1% em \\(x_i\\) está associado a um aumento de \\(\\frac{\\beta_1}{100}\\) unidades em \\(y_i\\), ceteris paribus.\n\nLogarítmica - linear\n\n\n\n\n\n\n\n\n\n\n\\[\n\\ln(y_i)=\\beta_0+\\beta_1x_{i}+\\mu_i\n\\tag{15}\\]\n\\(\\beta_0\\): representa o valor esperado de \\(ln(y_i)\\) quando \\(x_i=0\\), para obter em ralação a \\(y = e^{\\beta_0}\\), no R: exp(coef(modelo))[\"(Intercept)\"].\n\\(\\beta_1\\): um aumento de uma unidade em \\(x_i\\) está associado a um aumento de \\(\\beta_1 \\cdot 100\\) % em \\(y_i\\), ceteris paribus.\n\nLogarítmica - logarítmica\n\n\n\n\n\n\n\n\n\n\n\\[\n\\ln(y_i)=\\beta_0+\\beta_1\\ln(x_{i})+\\mu_i\n\\tag{16}\\]\n\\(\\beta_1\\): um aumento de 1% em \\(x_i\\) está associado a um aumento de \\(\\beta_1\\) % em \\(y_i\\), ceteris paribus.\n\nExponencial\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0\\beta_1^{x_i}\\mu_i\n\\]\ncom LN’s podemos linearizar a equação exponencial:\n\\[\n\\ln(y_i)=\\ln(\\beta_0)+\\ln(\\beta_1).x_i+\\ln(\\mu_i)\n\\]\ne estimar:\n\\[\n\\ln(y_i)=\\beta'_0+\\beta'_1x_i+\\mu'_i\n\\]\n\nPotência\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0X_i^{\\beta_1}\\mu_i\n\\tag{17}\\]\nCom LN’s:\n\\[\nln(y_i)=ln(\\beta_0)+\\beta_1.ln(x_i)+ln(\\mu_i)\n\\tag{18}\\]\npara estimar: \\[\nln(y_i)=\\beta'_0+\\beta_1 ln(x_i)+\\mu'_i\n\\tag{19}\\]\n\nPolinomial\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0+\\beta_1x_i+\\beta_2x_i^{2}+\\mu_i\n\\tag{20}\\]\npara estimar:\n\\[\nz_i=x'_i=x_i^2\n\\tag{21}\\]\n\nHiperbólica\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0+\\beta_1\\frac{1}x_i+\\mu_i\n\\tag{22}\\]\npara estimar:\n\\[\nw_i=x'_i=\\frac{1}x_i\n\\tag{23}\\]\nDepois de linearizar, a interpretação da forma funcional é semelhante às lineares.\n\n\n\nO teste de Ramsey RESET [@ramsey_tests_1969] é utilizado para testar a especificação do modelo, nomeadamente se a forma funcional está correta. Para evitar que:\n\n\n\n\n\n\n\n\n\nO teste consiste em:\n\nEstimar o modelo e obter os valores previstos (wage_hat).\nIncluir termos polinomiais das variáveis independentes e reestimar o modelo com os termos (wage_hat^2, wage_hat^3, …).\nRealizar o teste F para verificar se os coeficientes dos termos polinomiais são conjuntamente significativos.\n\nNo R:\n\n#estimar modelo\nmodelo &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\n\n#obter valores previstos e y^ e y^3\n wage2 &lt;- wage2 |&gt; \n            mutate(\n              wage_hat = predict(modelo),\n              wage_hat2 = wage_hat^2,\n              wage_hat3 = wage_hat^3\n            )\n\nmodelo_reset &lt;- lm(wage ~ educ + exper + tenure +\n              wage_hat2 + wage_hat3, data = wage2)\n\n#teste\nwaldtest(modelo, modelo_reset)\n\nWald test\n\nModel 1: wage ~ educ + exper + tenure\nModel 2: wage ~ educ + exper + tenure + wage_hat2 + wage_hat3\n  Res.Df Df      F Pr(&gt;F)\n1    931                 \n2    929  2 0.5526 0.5756\n\n#ou diretamente\nresettest(modelo, power = 2:3)\n\n\n    RESET test\n\ndata:  modelo\nRESET = 0.55259, df1 = 2, df2 = 929, p-value = 0.5756\n\n\nO argumento power do teste de Ramsey RESET especifica os graus de liberdade dos termos polinomiais a serem incluídos no modelo. Neste caso, estamos a incluir os termos quadráticos e cúbicos.\nA hipótese nula do teste de Ramsey RESET é que o modelo original está corretamente especificado. Como a H0 não é rejeitada para nenhum nível de significância estatística, não há evidências suficientes para concluir que o modelo esteja mal especificado.\n\n\n\nA estabilidade dos coeficientes pode ser realizado com o teste CUSUM [@ploberger_cusum_1992]. Este teste verifica se os coeficientes do modelo permanecem constantes ao longo da amostra.\nNo R:\n\n#estimar modelo\nmodelo &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\n\n# Teste CUSUM\nlibrary(strucchange)\n\nLoading required package: sandwich\n\n\n\nAttaching package: 'strucchange'\n\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\n# Teste CUSUMSQ\nsctest(modelo, type = \"CUSUM\")\n\n\n    M-fluctuation test\n\ndata:  modelo\nf(efp) = 1.6798, p-value = 0.02803\n\n#graficamente\nplot(efp(modelo, data = wage2, type = \"Rec-CUSUM\"))\n\n\n\n\n\n\n\n\nA hipótese nula do teste CUSUM é que os coeficientes são estáveis ao longo do tempo. H0 é rejeitada para 5% e 10%. A forma de apresentação mais comum é o gráfico onde é possível visualizar a estabilidade dos coeficientes ao longo da amostra, as linhas vermelhas representam os limites de significância de 5% e a preta representa a estatística do teste. Se a linha preta ultrapassar as linhas vermelhas, rejeitamos a hipótese nula de estabilidade.\n\n\n\n\nA biblioteca performance tem a função check_model() que realiza uma série de testes visuais para diagnosticar um modelo de regressão. Vamos utilizar o exemplo anterior, para dados seccionais, e executar a função:\n\n#carregar bibliotecas\nlibrary(wooldridge)\nlibrary(performance)\n\n#carregar dados\ndata(\"wage2\")\n\n#estimar modelo\nmodelo &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\n\n#\ncheck_model(modelo, check = \"all\")\n\n\n\n\n\n\n\n\nO resultado da função são 6 gráficos:\n\n“Posterior Predictive Check”: Verifica a distribuição dos valores previstos em relação aos valores observados.\n“Linearity”: Verifica a linearidade da relação entre as variáveis independentes e a variável dependente.\n“Homogeneity of Variance”: Verifica se a variância dos resíduos é constante (homocedasticidade).\n“Influential Observations”: Identifica observações que têm um impacto desproporcional na estimativa dos coeficientes do modelo.\n“Colinearity”: Verifica se há colinearidade entre as variáveis independentes.\n“Normality of Residuals”: Verifica se os resíduos seguem uma distribuição normal.\n\nA descrição completa de cada um deles pode ser verificada com ?performance::check_model.\nO título de cada gráfico é autoexplicativo. No subtítulo de cada um deles está a informação adicional sobre o que deveríamos verificar para que o modelo seja adequada. Esta é uma ferramenta muito útil para ter uma visão geral do modelo e diagnosticar potenciais problemas.\nPara este modelo podemos concluir que:\n\nO modelo captura bem a distribuição dos dados\nA relação entre as variáveis independentes e a variável dependente aparenta ser linear, mas pode ser melhorada.\nPoderá existir heterocedasticidade (é necessário fazer os testes)\nNão existem observações influentes (o modelo está dentro das linhas de distância de Cook’s)\nNão há indícios de colinearidade entre as variáveis independentes.\nOs resíduos não parecem seguir uma distribuição normal ( as observação deveriam estar em cima da linha verde)\n\nAgora um exemplo para séries temporais:\n\n#carregar bibliotecas\nlibrary(readxl) \nlibrary(performance)\n\n#carregar dados\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\n#estimar modelo\nmodelo_st &lt;- lm(gdp ~ ind + agri + ser + man, data = m_reg)\n\n#\ncheck_model(modelo_st)\n\n\n\n\n\n\n\n\n\nO modelo captura bem a distribuição dos dados.\nA relação entre as variáveis independentes e a variável dependente não aparenta ser linear, e por isso deve ser melhorada (exemplo: introduzir termos quadráticos ou interações).\nAparentemente existe heterocedasticidade.\nNão existem observações influentes (o modelo está dentro das linhas de distância de Cook’s).\nExistem indícios de colinearidade entre as variáveis ind, agri e ser.\nOs resíduos estão muito próximos de seguir uma distribuição normal.\n\n\n\n\nOs modelos devem ser escolhidos com base na capacidade de previsão e na parcimónia (simplicidade). O primeiro passo é definir um conjunto de modelos candidatos, quer seja pela número de variáveis independentes, quer seja pela sua estrutura funcional.\nPara este exemplo vamos utilizar a base de dados hprice3.\n\n#carregar bibliotecas\nlibrary(wooldridge)\n\n#carregar dados\ndata(\"hprice3\")\n\nA comparação de modelos é essencial para garantir que escolhemos o modelo mais adequado para os nossos dados. Quando a variável dependente é a mesma, devemos ter em consideração, entre modelos, as trocas de sinais dos coeficientes, a significância estatística e o coeficiente de determinação ajustado.\nVamos começar por estimar alguns modelos para explicar o preço das casas. E vamos compará-los com a função stargazer e com a função tbl_regression.\n\n# Estimar modelos\nmodelo1 &lt;- lm(price ~ area + rooms + age, \n              data = hprice3)\nmodelo2 &lt;- lm(price ~ area + inst + rooms + age, \n              data = hprice3)\nmodelo3 &lt;- lm(price ~ area + inst + rooms + age + I(inst^2), \n              data = hprice3)\nmodelo4 &lt;- lm(price ~ area + inst + rooms + age + \n              I(rooms^2) + I(age^2), \n              data = hprice3)\n\n#comparar com stargazer\nstargazer(modelo1, modelo2, modelo3, modelo4, \n          type = \"text\")\n\n\n====================================================================================================================\n                                                          Dependent variable:                                       \n                    ------------------------------------------------------------------------------------------------\n                                                                 price                                              \n                              (1)                      (2)                     (3)                     (4)          \n--------------------------------------------------------------------------------------------------------------------\narea                       35.091***                35.187***               33.078***               32.757***       \n                            (2.865)                  (2.863)                 (2.891)                 (2.895)        \n                                                                                                                    \ninst                                                 -0.273                 2.545***                -0.586***       \n                                                     (0.212)                 (0.882)                 (0.224)        \n                                                                                                                    \nrooms                     6,062.454***            6,795.345***             5,565.108**              2,246.530       \n                          (2,209.610)              (2,279.613)             (2,276.052)            (16,337.200)      \n                                                                                                                    \nage                       -397.964***              -426.322***             -387.241***            -1,053.441***     \n                            (51.747)                (56.195)                (56.606)                (168.996)       \n                                                                                                                    \nI(inst2)                                                                   -0.0001***                               \n                                                                            (0.00002)                               \n                                                                                                                    \nI(rooms2)                                                                                            114.788        \n                                                                                                   (1,204.167)      \n                                                                                                                    \nI(age2)                                                                                             4.329***        \n                                                                                                     (1.106)        \n                                                                                                                    \nConstant                  -10,585.510              -10,612.260             -16,647.810             29,842.910       \n                          (12,623.150)            (12,610.130)            (12,554.040)            (54,548.440)      \n                                                                                                                    \n--------------------------------------------------------------------------------------------------------------------\nObservations                  321                      321                     321                     321          \nR2                           0.520                    0.522                   0.538                   0.545         \nAdjusted R2                  0.515                    0.516                   0.531                   0.536         \nResidual Std. Error  30,098.970 (df = 317)    30,067.870 (df = 316)   29,612.130 (df = 315)   29,448.930 (df = 314) \nF Statistic         114.307*** (df = 3; 317) 86.321*** (df = 4; 316) 73.359*** (df = 5; 315) 62.562*** (df = 6; 314)\n====================================================================================================================\nNote:                                                                                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n#comparar com gtsummary\nlibrary(gtsummary)\n\ntbl1 &lt;- tbl_regression(modelo1) |&gt; #converter\n        add_glance_source_note() #adiciona metricas\ntbl2 &lt;- tbl_regression(modelo2) |&gt; \n        add_glance_source_note()\ntbl3 &lt;- tbl_regression(modelo3) |&gt; \n        add_glance_source_note()\ntbl4 &lt;- tbl_regression(modelo4) |&gt; \n        add_glance_source_note()\n\ntbl_merge(tbls=list(tbl1, tbl2, tbl3, tbl4),  #mostar modelos\n          tab_spanner = c(\"Modelo 1\", \"Modelo 2\", \n                        \"Modelo 3\", \"Modelo 4\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModelo 1\n\n\nModelo 2\n\n\nModelo 3\n\n\nModelo 4\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\narea\n35\n29, 41\n&lt;0.001\n35\n30, 41\n&lt;0.001\n33\n27, 39\n&lt;0.001\n33\n27, 38\n&lt;0.001\n\n\nrooms\n6,062\n1,715, 10,410\n0.006\n6,795\n2,310, 11,280\n0.003\n5,565\n1,087, 10,043\n0.015\n2,247\n-29,898, 34,391\n0.9\n\n\nage\n-398\n-500, -296\n&lt;0.001\n-426\n-537, -316\n&lt;0.001\n-387\n-499, -276\n&lt;0.001\n-1,053\n-1,386, -721\n&lt;0.001\n\n\ninst\n\n\n\n\n\n\n-0.27\n-0.69, 0.14\n0.2\n2.5\n0.81, 4.3\n0.004\n-0.59\n-1.0, -0.14\n0.009\n\n\nI(inst^2)\n\n\n\n\n\n\n\n\n\n\n\n\n0.00\n0.00, 0.00\n0.001\n\n\n\n\n\n\n\n\nI(rooms^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n115\n-2,254, 2,484\n&gt;0.9\n\n\nI(age^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3\n2.2, 6.5\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\nR² = 0.545; Adjusted R² = 0.536; Sigma = 29,449; Statistic = 62.6; p-value = &lt;0.001; df = 6; Log-likelihood = -3,755; AIC = 7,526; BIC = 7,556; Deviance = 272,313,154,349; Residual df = 314; No. Obs. = 321\n\n\nR² = 0.538; Adjusted R² = 0.531; Sigma = 29,612; Statistic = 73.4; p-value = &lt;0.001; df = 5; Log-likelihood = -3,757; AIC = 7,529; BIC = 7,555; Deviance = 276,216,655,491; Residual df = 315; No. Obs. = 321\n\n\nR² = 0.522; Adjusted R² = 0.516; Sigma = 30,068; Statistic = 86.3; p-value = &lt;0.001; df = 4; Log-likelihood = -3,763; AIC = 7,538; BIC = 7,560; Deviance = 285,688,284,750; Residual df = 316; No. Obs. = 321\n\n\nR² = 0.520; Adjusted R² = 0.515; Sigma = 30,099; Statistic = 114; p-value = &lt;0.001; df = 3; Log-likelihood = -3,764; AIC = 7,537; BIC = 7,556; Deviance = 287,185,487,877; Residual df = 317; No. Obs. = 321\n\n\n\n\n\n\n\n\nNuma primeira análise, podemos observar que que o coeficinete da area é consistente entre os modelos. Os coeficientes das variáveis rooms e age também sao bastante consistentes en termos de sinais. A introdução do termo quadrático de inst no modelo 3 faz com que o coeficiente de inst passe a ser estatisticamento diferente de zero. Este deve ser o primeiro passo para perceber a robustez dos coeficientes. Neste exemplo não houve uma razão específica para a escolha dos modelos, mas em situações reais, a escolha deve ser feita com base em teoria económica e conhecimento do domínio/tópico. Existem alguns critérios que podem ser utilizados para comparar modelos:\n\n\nNeste método, todas as variáveis independentes disponíveis são incluídas no modelo. Este método é simples, mas pode levar a problemas de multicolinearidade e sobreajuste (overfitting). Este método exige prior knowledge. Um exemplo deste método é prever o risco de incumprimento de um empréstimo, onde todas as variáveis disponíveis sobre o cliente são incluídas no modelo, até para comprar o risco entre os vários clientes.\nPara exte exemplo vamos utilizar mais uma vez os dados hprice3 e estimar um modelo com todas as variáveis disponíveis.\n\nlibrary(wooldridge)\ninvisible(library(tidyverse))\ndata(\"hprice3\")\n\n#excluir variáveis transformadas\nhprice3 &lt;- hprice3 |&gt; \n            select(-c(year,agesq, linst, ldist, \n            lprice, larea, lland, linstsq))\n# Estimar modelo \"all in\"\nmodelo_all_in &lt;- lm(price ~ ., data = hprice3)\nsummary(modelo_all_in)\n\n\nCall:\nlm(formula = price ~ ., data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-94658 -13947  -1089  11687 135173 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.339e+04  1.184e+04  -1.976  0.04900 *  \nage         -2.308e+02  4.940e+01  -4.672 4.46e-06 ***\nnbh         -2.024e+03  6.532e+02  -3.099  0.00212 ** \ncbd         -7.693e-01  1.821e+00  -0.423  0.67290    \ninst        -2.684e-02  1.281e+00  -0.021  0.98329    \nrooms        4.201e+03  1.942e+03   2.164  0.03127 *  \narea         2.174e+01  2.769e+00   7.851 6.80e-14 ***\nland         1.224e-01  3.770e-02   3.247  0.00129 ** \nbaths        1.297e+04  2.858e+03   4.540 8.07e-06 ***\ndist         7.814e-01  7.628e-01   1.024  0.30645    \ny81          3.589e+04  2.771e+03  12.953  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23430 on 310 degrees of freedom\nMultiple R-squared:  0.7152,    Adjusted R-squared:  0.7061 \nF-statistic: 77.87 on 10 and 310 DF,  p-value: &lt; 2.2e-16\n\n\nO modelo “all in” pode ser útil como um ponto de partida, mas é importante considerar a parcimónia e a interpretabilidade do modelo. Nem sempre o modelo com mais variáveis será o melhor. Este método é o ponto de partida para o próximo método de seleção.\n\n\n\nA eliminação é um método de seleção de modelos que começa com um modelo completo e remove iterativamente as variáveis com menos informação. Para isso vamos recorrer à função step() da biblioteca MASS. Primeiro vamos estimar o modelo completo com todas as variáveis e depois aplicar o método de eliminação backward. Para a função stepAIC() vamos utilizar o argumento object que é o modelo completo, e o argumento direction que indica a direção da seleção, neste caso backward.\n\n# Estimar modelo completo\nmodelo_completo &lt;- lm(price ~ ., data = hprice3)\n# Aplicar backward elimination\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:gtsummary':\n\n    select\n\n\nThe following object is masked from 'package:wooldridge':\n\n    cement\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nmodelo_backward &lt;- stepAIC(modelo_completo, \n                        direction = \"backward\")\n\nStart:  AIC=6470.58\nprice ~ age + nbh + cbd + inst + rooms + area + land + baths + \n    dist + y81\n\n        Df  Sum of Sq        RSS    AIC\n- inst   1 2.4123e+05 1.7024e+11 6468.6\n- cbd    1 9.8066e+07 1.7034e+11 6468.8\n- dist   1 5.7629e+08 1.7082e+11 6469.7\n&lt;none&gt;                1.7024e+11 6470.6\n- rooms  1 2.5705e+09 1.7281e+11 6473.4\n- nbh    1 5.2734e+09 1.7552e+11 6478.4\n- land   1 5.7894e+09 1.7603e+11 6479.3\n- baths  1 1.1318e+10 1.8156e+11 6489.2\n- age    1 1.1985e+10 1.8223e+11 6490.4\n- area   1 3.3847e+10 2.0409e+11 6526.8\n- y81    1 9.2133e+10 2.6237e+11 6607.4\n\nStep:  AIC=6468.58\nprice ~ age + nbh + cbd + rooms + area + land + baths + dist + \n    y81\n\n        Df  Sum of Sq        RSS    AIC\n- dist   1 1.0629e+09 1.7130e+11 6468.6\n&lt;none&gt;                1.7024e+11 6468.6\n- cbd    1 1.0684e+09 1.7131e+11 6468.6\n- rooms  1 2.5824e+09 1.7282e+11 6471.4\n- nbh    1 5.4832e+09 1.7573e+11 6476.8\n- land   1 5.8105e+09 1.7605e+11 6477.4\n- baths  1 1.1320e+10 1.8156e+11 6487.2\n- age    1 1.1989e+10 1.8223e+11 6488.4\n- area   1 3.3887e+10 2.0413e+11 6524.9\n- y81    1 9.2372e+10 2.6261e+11 6605.7\n\nStep:  AIC=6468.58\nprice ~ age + nbh + cbd + rooms + area + land + baths + y81\n\n        Df  Sum of Sq        RSS    AIC\n- cbd    1 3.2900e+07 1.7134e+11 6466.6\n&lt;none&gt;                1.7130e+11 6468.6\n- rooms  1 2.4764e+09 1.7378e+11 6471.2\n- land   1 5.0440e+09 1.7635e+11 6475.9\n- nbh    1 5.8906e+09 1.7720e+11 6477.4\n- age    1 1.1211e+10 1.8252e+11 6486.9\n- baths  1 1.1829e+10 1.8313e+11 6488.0\n- area   1 3.2899e+10 2.0420e+11 6523.0\n- y81    1 9.3967e+10 2.6527e+11 6607.0\n\nStep:  AIC=6466.64\nprice ~ age + nbh + rooms + area + land + baths + y81\n\n        Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                1.7134e+11 6466.6\n- rooms  1 2.4575e+09 1.7380e+11 6469.2\n- land   1 5.4354e+09 1.7677e+11 6474.7\n- nbh    1 6.6906e+09 1.7803e+11 6476.9\n- baths  1 1.1859e+10 1.8320e+11 6486.1\n- age    1 1.2175e+10 1.8351e+11 6486.7\n- area   1 3.3214e+10 2.0455e+11 6521.5\n- y81    1 9.4601e+10 2.6594e+11 6605.8\n\nsummary(modelo_backward)\n\n\nCall:\nlm(formula = price ~ age + nbh + rooms + area + land + baths + \n    y81, data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-98481 -13086  -1139  11638 139184 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.766e+04  1.019e+04  -1.733  0.08405 .  \nage         -2.168e+02  4.597e+01  -4.716 3.63e-06 ***\nnbh         -2.139e+03  6.120e+02  -3.496  0.00054 ***\nrooms        4.025e+03  1.899e+03   2.119  0.03489 *  \narea         2.128e+01  2.732e+00   7.789 9.98e-14 ***\nland         1.084e-01  3.439e-02   3.151  0.00178 ** \nbaths        1.314e+04  2.824e+03   4.654 4.80e-06 ***\ny81          3.618e+04  2.752e+03  13.146  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23400 on 313 degrees of freedom\nMultiple R-squared:  0.7134,    Adjusted R-squared:  0.707 \nF-statistic: 111.3 on 7 and 313 DF,  p-value: &lt; 2.2e-16\n\n\nO modelo resultante d é mais simples e contém apenas as variáveis informação necessária. Este método é útil para reduzir a complexidade do modelo. Para este método, o critério de seleção padrão é o AIC (Critério de Informação de Akaike). A cada iteração, a variável que, quando removida, resulta na maior redução do AIC é eliminada do modelo. O processo continua até que a remoção de qualquer variável adicional não melhore o AIC.\n\n\n\nA seleção forward é o oposto da eliminação backward. Começa com um modelo só com constante e é adiconada a variável com o valor P mais baixo do all in. É adicioanda uma variável de cada vez, em que variável só permanece no modelo se tiver informação suficiente. As variáveis são adicionadas até que nenhuma variável adicional melhore significativamente o modelo. Para este método vamos utilizar o argumento object que é o modelo nulo, o argumento direction que indica a direção da seleção, neste caso forward, e o argumento scope que é o modelo completo. O scope define o limite superior do modelo que pode ser construído.\n\n# Estimar modelo nulo\nmodelo_nulo &lt;- lm(price ~ 1, data = hprice3)\n# Aplicar forward selection\nlibrary(MASS)\nmodelo_forward &lt;- stepAIC(modelo_nulo, \n                      direction = \"forward\", \n                      scope = formula(modelo_completo))\n\nStart:  AIC=6853.8\nprice ~ 1\n\n        Df  Sum of Sq        RSS    AIC\n+ area   1 2.4898e+11 3.4887e+11 6682.9\n+ baths  1 2.3424e+11 3.6362e+11 6696.2\n+ y81    1 1.5343e+11 4.4442e+11 6760.6\n+ rooms  1 1.1739e+11 4.8046e+11 6785.6\n+ age    1 6.5871e+10 5.3198e+11 6818.3\n+ cbd    1 2.9096e+10 5.6876e+11 6839.8\n+ inst   1 2.8000e+10 5.6985e+11 6840.4\n+ nbh    1 2.7872e+10 5.6998e+11 6840.5\n+ land   1 2.5512e+10 5.7234e+11 6841.8\n+ dist   1 2.3611e+10 5.7424e+11 6842.9\n&lt;none&gt;                5.9785e+11 6853.8\n\nStep:  AIC=6682.89\nprice ~ area\n\n        Df  Sum of Sq        RSS    AIC\n+ y81    1 9.6055e+10 2.5281e+11 6581.5\n+ age    1 5.4864e+10 2.9401e+11 6630.0\n+ baths  1 4.1886e+10 3.0698e+11 6643.8\n+ nbh    1 1.8465e+10 3.3040e+11 6667.4\n+ dist   1 8.1622e+09 3.4071e+11 6677.3\n+ rooms  1 8.1015e+09 3.4077e+11 6677.4\n+ cbd    1 7.2749e+09 3.4159e+11 6678.1\n+ land   1 5.8607e+09 3.4301e+11 6679.5\n+ inst   1 5.6794e+09 3.4319e+11 6679.6\n&lt;none&gt;                3.4887e+11 6682.9\n\nStep:  AIC=6581.52\nprice ~ area + y81\n\n        Df  Sum of Sq        RSS    AIC\n+ baths  1 5.4806e+10 1.9801e+11 6505.1\n+ age    1 4.1224e+10 2.1159e+11 6526.4\n+ rooms  1 1.5120e+10 2.3769e+11 6563.7\n+ cbd    1 1.4338e+10 2.3848e+11 6564.8\n+ dist   1 1.3921e+10 2.3889e+11 6565.3\n+ land   1 1.3706e+10 2.3911e+11 6565.6\n+ inst   1 1.2985e+10 2.3983e+11 6566.6\n+ nbh    1 1.1678e+10 2.4114e+11 6568.3\n&lt;none&gt;                2.5281e+11 6581.5\n\nStep:  AIC=6505.08\nprice ~ area + y81 + baths\n\n        Df  Sum of Sq        RSS    AIC\n+ age    1 1.1929e+10 1.8608e+11 6487.1\n+ nbh    1 6.8226e+09 1.9119e+11 6495.8\n+ land   1 6.4476e+09 1.9156e+11 6496.5\n+ dist   1 1.5748e+09 1.9643e+11 6504.5\n+ cbd    1 1.5257e+09 1.9648e+11 6504.6\n+ inst   1 1.2522e+09 1.9676e+11 6505.0\n&lt;none&gt;                1.9801e+11 6505.1\n+ rooms  1 1.2041e+09 1.9680e+11 6505.1\n\nStep:  AIC=6487.14\nprice ~ area + y81 + baths + age\n\n        Df  Sum of Sq        RSS    AIC\n+ nbh    1 6558044922 1.7952e+11 6477.6\n+ land   1 5536264112 1.8054e+11 6479.4\n+ rooms  1 2800423408 1.8328e+11 6484.3\n&lt;none&gt;                1.8608e+11 6487.1\n+ dist   1  180122584 1.8590e+11 6488.8\n+ cbd    1   54725421 1.8602e+11 6489.0\n+ inst   1    8256988 1.8607e+11 6489.1\n\nStep:  AIC=6477.62\nprice ~ area + y81 + baths + age + nbh\n\n        Df  Sum of Sq        RSS    AIC\n+ land   1 5724921758 1.7380e+11 6469.2\n+ rooms  1 2747028352 1.7677e+11 6474.7\n+ dist   1 1135961225 1.7838e+11 6477.6\n&lt;none&gt;                1.7952e+11 6477.6\n+ cbd    1  911989874 1.7861e+11 6478.0\n+ inst   1  778563582 1.7874e+11 6478.2\n\nStep:  AIC=6469.21\nprice ~ area + y81 + baths + age + nbh + land\n\n        Df  Sum of Sq        RSS    AIC\n+ rooms  1 2457508573 1.7134e+11 6466.6\n&lt;none&gt;                1.7380e+11 6469.2\n+ dist   1  182635168 1.7361e+11 6470.9\n+ cbd    1   14054756 1.7378e+11 6471.2\n+ inst   1     759751 1.7379e+11 6471.2\n\nStep:  AIC=6466.64\nprice ~ area + y81 + baths + age + nbh + land + rooms\n\n       Df Sum of Sq        RSS    AIC\n&lt;none&gt;              1.7134e+11 6466.6\n+ inst  1 100552078 1.7124e+11 6468.5\n+ cbd   1  32900269 1.7130e+11 6468.6\n+ dist  1  27334059 1.7131e+11 6468.6\n\nsummary(modelo_forward)\n\n\nCall:\nlm(formula = price ~ area + y81 + baths + age + nbh + land + \n    rooms, data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-98481 -13086  -1139  11638 139184 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.766e+04  1.019e+04  -1.733  0.08405 .  \narea         2.128e+01  2.732e+00   7.789 9.98e-14 ***\ny81          3.618e+04  2.752e+03  13.146  &lt; 2e-16 ***\nbaths        1.314e+04  2.824e+03   4.654 4.80e-06 ***\nage         -2.168e+02  4.597e+01  -4.716 3.63e-06 ***\nnbh         -2.139e+03  6.120e+02  -3.496  0.00054 ***\nland         1.084e-01  3.439e-02   3.151  0.00178 ** \nrooms        4.025e+03  1.899e+03   2.119  0.03489 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23400 on 313 degrees of freedom\nMultiple R-squared:  0.7134,    Adjusted R-squared:  0.707 \nF-statistic: 111.3 on 7 and 313 DF,  p-value: &lt; 2.2e-16\n\n\nNeste modelo, existem duas variáveis que não são estatisticamente significativas, mas que foram incluídas no modelo porque melhoram o AIC. O facto de uma variável não ser estatisticamente significativa não significa que ela não tenha informação para explicar a dependente. A decisão de incluir ou excluir uma variável deve ser baseada em uma combinação de critérios estatísticos e conhecimento tórico.\n\n\n\nA seleção stepwise combina os métodos forward e backward. Começa com um modelo nulo e adiciona variáveis como no método forward, mas também verifica se alguma variável já incluída no modelo pode ser removida, como no método backward. Este processo continua até que nenhuma variável adicional possa ser adicionada ou removida. Or argumento object é o modelo nulo, o argumento direction que indica a direção da seleção, neste caso both, e o argumento scope que é o modelo completo.\n\n# Estimar modelo nulo\nmodelo_nulo &lt;- lm(price ~ 1, data = hprice3)\n# Aplicar stepwise selection\nlibrary(MASS)\nmodelo_stepwise &lt;- stepAIC(modelo_nulo, direction = \"both\", scope = formula(modelo_completo))\n\nStart:  AIC=6853.8\nprice ~ 1\n\n        Df  Sum of Sq        RSS    AIC\n+ area   1 2.4898e+11 3.4887e+11 6682.9\n+ baths  1 2.3424e+11 3.6362e+11 6696.2\n+ y81    1 1.5343e+11 4.4442e+11 6760.6\n+ rooms  1 1.1739e+11 4.8046e+11 6785.6\n+ age    1 6.5871e+10 5.3198e+11 6818.3\n+ cbd    1 2.9096e+10 5.6876e+11 6839.8\n+ inst   1 2.8000e+10 5.6985e+11 6840.4\n+ nbh    1 2.7872e+10 5.6998e+11 6840.5\n+ land   1 2.5512e+10 5.7234e+11 6841.8\n+ dist   1 2.3611e+10 5.7424e+11 6842.9\n&lt;none&gt;                5.9785e+11 6853.8\n\nStep:  AIC=6682.89\nprice ~ area\n\n        Df  Sum of Sq        RSS    AIC\n+ y81    1 9.6055e+10 2.5281e+11 6581.5\n+ age    1 5.4864e+10 2.9401e+11 6630.0\n+ baths  1 4.1886e+10 3.0698e+11 6643.8\n+ nbh    1 1.8465e+10 3.3040e+11 6667.4\n+ dist   1 8.1622e+09 3.4071e+11 6677.3\n+ rooms  1 8.1015e+09 3.4077e+11 6677.4\n+ cbd    1 7.2749e+09 3.4159e+11 6678.1\n+ land   1 5.8607e+09 3.4301e+11 6679.5\n+ inst   1 5.6794e+09 3.4319e+11 6679.6\n&lt;none&gt;                3.4887e+11 6682.9\n- area   1 2.4898e+11 5.9785e+11 6853.8\n\nStep:  AIC=6581.52\nprice ~ area + y81\n\n        Df  Sum of Sq        RSS    AIC\n+ baths  1 5.4806e+10 1.9801e+11 6505.1\n+ age    1 4.1224e+10 2.1159e+11 6526.4\n+ rooms  1 1.5120e+10 2.3769e+11 6563.7\n+ cbd    1 1.4338e+10 2.3848e+11 6564.8\n+ dist   1 1.3921e+10 2.3889e+11 6565.3\n+ land   1 1.3706e+10 2.3911e+11 6565.6\n+ inst   1 1.2985e+10 2.3983e+11 6566.6\n+ nbh    1 1.1678e+10 2.4114e+11 6568.3\n&lt;none&gt;                2.5281e+11 6581.5\n- y81    1 9.6055e+10 3.4887e+11 6682.9\n- area   1 1.9161e+11 4.4442e+11 6760.6\n\nStep:  AIC=6505.08\nprice ~ area + y81 + baths\n\n        Df  Sum of Sq        RSS    AIC\n+ age    1 1.1929e+10 1.8608e+11 6487.1\n+ nbh    1 6.8226e+09 1.9119e+11 6495.8\n+ land   1 6.4476e+09 1.9156e+11 6496.5\n+ dist   1 1.5748e+09 1.9643e+11 6504.5\n+ cbd    1 1.5257e+09 1.9648e+11 6504.6\n+ inst   1 1.2522e+09 1.9676e+11 6505.0\n&lt;none&gt;                1.9801e+11 6505.1\n+ rooms  1 1.2041e+09 1.9680e+11 6505.1\n- area   1 2.9226e+10 2.2723e+11 6547.3\n- baths  1 5.4806e+10 2.5281e+11 6581.5\n- y81    1 1.0898e+11 3.0698e+11 6643.8\n\nStep:  AIC=6487.14\nprice ~ area + y81 + baths + age\n\n        Df  Sum of Sq        RSS    AIC\n+ nbh    1 6.5580e+09 1.7952e+11 6477.6\n+ land   1 5.5363e+09 1.8054e+11 6479.4\n+ rooms  1 2.8004e+09 1.8328e+11 6484.3\n&lt;none&gt;                1.8608e+11 6487.1\n+ dist   1 1.8012e+08 1.8590e+11 6488.8\n+ cbd    1 5.4725e+07 1.8602e+11 6489.0\n+ inst   1 8.2570e+06 1.8607e+11 6489.1\n- age    1 1.1929e+10 1.9801e+11 6505.1\n- baths  1 2.5512e+10 2.1159e+11 6526.4\n- area   1 3.8341e+10 2.2442e+11 6545.3\n- y81    1 9.4988e+10 2.8107e+11 6617.5\n\nStep:  AIC=6477.62\nprice ~ area + y81 + baths + age + nbh\n\n        Df  Sum of Sq        RSS    AIC\n+ land   1 5.7249e+09 1.7380e+11 6469.2\n+ rooms  1 2.7470e+09 1.7677e+11 6474.7\n+ dist   1 1.1360e+09 1.7838e+11 6477.6\n&lt;none&gt;                1.7952e+11 6477.6\n+ cbd    1 9.1199e+08 1.7861e+11 6478.0\n+ inst   1 7.7856e+08 1.7874e+11 6478.2\n- nbh    1 6.5580e+09 1.8608e+11 6487.1\n- age    1 1.1665e+10 1.9119e+11 6495.8\n- baths  1 2.2965e+10 2.0249e+11 6514.3\n- area   1 3.9358e+10 2.1888e+11 6539.2\n- y81    1 8.9281e+10 2.6880e+11 6605.2\n\nStep:  AIC=6469.21\nprice ~ area + y81 + baths + age + nbh + land\n\n        Df  Sum of Sq        RSS    AIC\n+ rooms  1 2.4575e+09 1.7134e+11 6466.6\n&lt;none&gt;                1.7380e+11 6469.2\n+ dist   1 1.8264e+08 1.7361e+11 6470.9\n+ cbd    1 1.4055e+07 1.7378e+11 6471.2\n+ inst   1 7.5975e+05 1.7379e+11 6471.2\n- land   1 5.7249e+09 1.7952e+11 6477.6\n- nbh    1 6.7467e+09 1.8054e+11 6479.4\n- age    1 1.0746e+10 1.8454e+11 6486.5\n- baths  1 1.9943e+10 1.9374e+11 6502.1\n- area   1 3.7746e+10 2.1154e+11 6530.3\n- y81    1 9.3450e+10 2.6725e+11 6605.3\n\nStep:  AIC=6466.64\nprice ~ area + y81 + baths + age + nbh + land + rooms\n\n        Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                1.7134e+11 6466.6\n+ inst   1 1.0055e+08 1.7124e+11 6468.5\n+ cbd    1 3.2900e+07 1.7130e+11 6468.6\n+ dist   1 2.7334e+07 1.7131e+11 6468.6\n- rooms  1 2.4575e+09 1.7380e+11 6469.2\n- land   1 5.4354e+09 1.7677e+11 6474.7\n- nbh    1 6.6906e+09 1.7803e+11 6476.9\n- baths  1 1.1859e+10 1.8320e+11 6486.1\n- age    1 1.2175e+10 1.8351e+11 6486.7\n- area   1 3.3214e+10 2.0455e+11 6521.5\n- y81    1 9.4601e+10 2.6594e+11 6605.8\n\nsummary(modelo_stepwise)\n\n\nCall:\nlm(formula = price ~ area + y81 + baths + age + nbh + land + \n    rooms, data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-98481 -13086  -1139  11638 139184 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.766e+04  1.019e+04  -1.733  0.08405 .  \narea         2.128e+01  2.732e+00   7.789 9.98e-14 ***\ny81          3.618e+04  2.752e+03  13.146  &lt; 2e-16 ***\nbaths        1.314e+04  2.824e+03   4.654 4.80e-06 ***\nage         -2.168e+02  4.597e+01  -4.716 3.63e-06 ***\nnbh         -2.139e+03  6.120e+02  -3.496  0.00054 ***\nland         1.084e-01  3.439e-02   3.151  0.00178 ** \nrooms        4.025e+03  1.899e+03   2.119  0.03489 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23400 on 313 degrees of freedom\nMultiple R-squared:  0.7134,    Adjusted R-squared:  0.707 \nF-statistic: 111.3 on 7 and 313 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nOs métodos de seleção de modelos podem ser comparados utilizando critérios de informação, como o AIC (Critério de Informação de Akaike) e o BIC (Critério de Informação Bayesiano). Estes critérios penalizam a complexidade do modelo, são muito úteis quando existem muitas variáveis disponíveis.\nO AIC é mais flexível e tende a selecionar modelos mais complexos, enquanto o BIC é mais conservador e favorece modelos mais simples. O AIC é calculado:\n\\[\nAIC = \\ln\\left(\\frac{SSR}{n}\\right) + \\frac{2k}{n}\n\\tag{24}\\]\nonde: - \\(SSR\\) é a soma dos quadrados dos resíduos do modelo, - \\(n\\) é o número de observações, - \\(k\\) é o número de parâmetros do modelo.\nOu seja, o AIC penaliza o número de parâmetros no modelo (quanto maior o número de parâmetros, maior a penalização). O objetivo é minimizar o AIC, ou seja, escolher o modelo com o menor AIC.\nO BIC, também conhecido como Critério de Informação de Schwarz (SIC), é calculado como:\n\\[\nBIC = \\ln\\left(\\frac{SSR}{n}\\right) + \\frac{k \\ln(n)}{n}\n\\tag{25}\\]\nem que: - \\(SSR\\) é a soma dos quadrados dos resíduos do modelo, - \\(n\\) é o número de observações, - \\(k\\) é o número de parâmetros do modelo.\nOu seja, o BIC penaliza o número de parâmetros no modelo, mas a penalização é mais acentuada do que no AIC, sobretudo em grandes amostras. O objetivo é minimizar o BIC, ou seja, escolher o modelo com o menor BIC.\nPara obter o AIC e o BIC de um modelo no R vamos utilizar a função compare_performance() da biblioteca performance. Para um modelo utilizamos a função model_performance(). Vamos estimar 2 modelos para efeitos de comparação:\n\nlibrary(performance)\n\nmodelo1 &lt;- lm(price ~ area + rooms + age, \n              data = hprice3)\nmodelo2 &lt;- lm(price ~ area + inst + rooms + age + I(inst^2), \n              data = hprice3)\n\ncompare_performance(modelo1, modelo2)\n\n# Comparison of Model Performance Indices\n\nName    | Model |  AIC (weights) | AICc (weights) |  BIC (weights) |    R2\n--------------------------------------------------------------------------\nmodelo1 |    lm | 7537.4 (0.014) | 7537.6 (0.015) | 7556.3 (0.383) | 0.520\nmodelo2 |    lm | 7528.9 (0.986) | 7529.3 (0.985) | 7555.3 (0.617) | 0.538\n\nName    | R2 (adj.) |      RMSE |     Sigma\n-------------------------------------------\nmodelo1 |     0.515 | 29910.848 | 30098.969\nmodelo2 |     0.531 | 29334.076 | 29612.130\n\n\nObtemos bárias métricas:\n\nAIC: Critério de Informação de Akaike\nAICc: AIC corrigido para amostras pequenas\nBIC: Critério de Informação Bayesiano\nR2: Coeficiente de Determinação - mede a proporção da variabilidade na variável dependente que é explicada pelo modelo.\nR2 adj: Coeficiente de Determinação Ajustado - ajusta o R² para o número de variáveis independentes no modelo.\nRMSE: Raiz do Erro Quadrático Médio (\\(RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\)) - mede a média dos erros de previsão do modelo.\nSigma: Estimativa do desvio padrão dos resíduos - mede a variabilidade dos resíduos do modelo.\n\nO objetivo é minimizar o AIC, BIC, RMSE e o Sigma. Enquanto que o R² e o R² ajustado devem ser maximizados. Neste exemplo, o modelo 2 é melhor em todas as métricas. A escolha entre os modelos deve considerar o trade-off entre a complexidade do modelo e a sua capacidade de prever a variável dependente.\n\n\n\nA validação cruzada é uma técnica utilizada para avaliar a capacidade de previsão de um modelo. O objetivo é dividir os dados em subamostras de treino e de teste, onde o modelo é “treinado” no conjunto de treino e avaliado no conjunto de teste. Existem várias formas de realizar a validação cruzada, mas as mais comuns são a validação cruzada simples (hold-out) e a validação cruzada k-fold.\nA Validação Cruzada Simples (Hold-out) consiste em dividir os dados em duas partes: uma para treino e outra para teste. O modelo é “treinado”/estimado com a parte de treino e avaliado com a parte de teste. Esta abordagem é simples, mas muito sensível à forma de como os dados são divididos. Para dividir os dados recorresmos à biblioteca caTools com as funções sample.split() para definir a proporção, e subset() para criar os conjuntos de treino e teste. Para os dados wage3 vamos utilizar 70% dos dados para treino e 30% para teste.\n\n#dados\nlibrary(wooldridge)\ndata(\"wage2\")\n\n#separar dados treino e teste\nlibrary(caTools)\n\nsplit &lt;- sample.split(wage2$wage, SplitRatio = 0.7)\ndados_treino &lt;- subset(wage2, split == TRUE)\ndados_teste &lt;- subset(wage2, split == FALSE)\n\nEstimar um modelo para com dados de treino e fazemos uma previão com os dados de teste, mas recorremos ao modelo de treino. A métrica mais comum para avaliar a capacidade de previsão é o RMSE (Root Mean Squared Error).\n\n# Estimar modelo nos dados de treino\nmodelo_treino &lt;- lm(wage ~ educ + exper + tenure, data = dados_treino)\n\n# Prever valores nos dados de teste com o modelo de treino\nprevisoes &lt;- predict(modelo_treino, newdata = dados_teste)\n# Calcular RMSE\nrmse &lt;- sqrt(mean((dados_teste$wage - previsoes)^2))\nrmse\n\n[1] 365.1301\n\n\nObtivemos um RMSE de 384.199, o que significa que, em média, as previsões do modelo estão a 384.199 unidades do valor real da variável dependente wage. Este valor pode ser comparado com a média da variável dependente para ter uma ideia da precisão do modelo.\n\nmean_wage&lt;-mean(wage2$wage)\n\nrmse / mean_wage\n\n[1] 0.3811596\n\n\nComo a média de wage é 957.9455, o RMSE representa aproximadamente 39% da média, o que indica que o modelo tem uma capacidade de previsão moderada.\nTambém podemos comparar graficamente o modelo nas duas subamostras:\n\nlibrary(ggplot2)\n\n# Criar data frame combinando dados de treino e teste\ndados_plot &lt;- data.frame(\n  Valores_Reais = c(dados_treino$wage, dados_teste$wage),\n  Previsão = c(predict(modelo_treino, newdata = dados_treino), previsoes),\n  Amostras = c(rep(\"Treino\", nrow(dados_treino)), rep(\"Teste\", nrow(dados_teste)))\n)\n\nggplot(dados_plot, aes(x = Valores_Reais, y = Previsão, color = Amostras)) +\n  geom_point(alpha = 0.7, size = 5.5) +\n  scale_color_manual(values = c(\"Treino\" = \"steelblue\", \"Teste\" = \"coral\")) +\n  geom_abline(intercept = 0, slope = 1, colour = \"black\", linetype = \"dashed\", size = 2) +\n  labs(title = \"Comparação da Previsão: Treino vs Teste\",\n       x = \"Valores Reais (wage)\",\n       y = \"Previsões do Modelo\",\n       color = \"Amostras\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank(),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nA validação cruzada k-fold é uma técnica mais robusta do que a validação cruzada simples. Consiste em dividir os dados em k subamostras (folds). O modelo é “treinado” em k-1 folds e testado no fold restante. Este processo é repetido k vezes, cada vez com um fold diferente como conjunto de teste. A métrica de avaliação (por exemplo, RMSE) é então calculada como a média das métricas obtidas em cada iteração.\nPara este exemplo vamos utilizar a biblioteca caret que tem uma função específica para realizar a validação cruzada k-fold. Vamos definir o número de folds (k) e utilizar a função trainControl() para configurar a validação cruzada. Depois, utilizamos a função train() para estimar o modelo com validação cruzada.\n\n#carregar bibliotecas\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n# Definir controle de treino com validação cruzada k-fold\ncontrole_treino &lt;- trainControl(method = \"cv\", number = 10) # 10-fold CV\n# Estimar modelo com validação cruzada k-fold\nmodelo_kfold &lt;- train(wage ~ educ + exper + tenure, \n                      data = wage2, \n                      method = \"lm\", \n                      trControl = controle_treino)\n# Resultados do modelo\nprint(modelo_kfold)\n\nLinear Regression \n\n935 samples\n  3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 842, 842, 841, 843, 843, 840, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  373.7798  0.1549275  281.1262\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# RMSE médio\nmodelo_kfold$results$RMSE\n\n[1] 373.7798\n\n# R² médio\nmodelo_kfold$results$Rsquared\n\n[1] 0.1549275\n\n\nOs valores de RMSE e R² apresentados são a média dos valores obtidos em cada uma das 10 iterações da validação cruzada. O RMSE médio é 372.959, o que indica que, em média, as previsões do modelo estão a 372.959 unidades do valor real da variável dependente wage. O R² médio é 0.1487, o que significa que, em média, o modelo explica aproximadamente 14.87% da variabilidade na variável dependente wage. Provavelmente o modelo pode ser melhorado, seja com a introdução de novas variáveis, seja com a alteração da forma funcional. No próximo capítulo vamos abordar algumas extensões do modeli de regressão linear que podem ajudar a melhorar a capacidade de previsão dos modelos."
  },
  {
    "objectID": "capitulo-05-avaliacao-modelos.html#sec-multicolinearidade",
    "href": "capitulo-05-avaliacao-modelos.html#sec-multicolinearidade",
    "title": "Avaliação dos Modelos de Regressão",
    "section": "",
    "text": "Existem várias formas de detectar a multicolinearidade entre as variáveis independentes num modelo de regressão. Vamos utilizar os dados do ficheiro m_reg.xlsx. O ficheiro pode ser descarregado em https://github.com/tiagolafonso/Files_Intro_Applied_Econometrics. Os dados foram recolhidos no World Development Indicators e contêm as seguintes variáveis:\n\n\n\n\n\n\n\nCódigo\nDescrição\n\n\n\n\ngdp\nPIB (preços constantes, moeda local)\n\n\nair\nTransporte aéreo, passageiros transportados\n\n\nrail\nTransporte ferroviário, passageiros transportados (milhões de passageiros)\n\n\nagri\nAgricultura, silvicultura e pesca, valor acrescentado (preços constantes, moeda local)\n\n\nind\nIndústria (incluindo construção), valor acrescentado (preços constantes, moeda local)\n\n\nman\nIndústria transformadora, valor acrescentado (preços constantes, moeda local)\n\n\nser\nServiços, valor acrescentado (preços constantes, moeda local)\n\n\nx\nExportações de bens e serviços (preços constantes, moeda local)\n\n\nm\nImportações de bens e serviços (preços constantes, moeda local)\n\n\ngfcf\nFormação bruta de capital fixo (preços constantes, moeda local)\n\n\n\nImportar os dados para o R:\n\n#limpar ambiente\nrm(list = ls())\n\n# carregar bibliotecas necessárias\nlibrary(readxl)\nlibrary(tidyverse)\n\n#importar dados\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\nNeste exemplo, vamos analisar a multicolinearidade para as variáveis independentes para o modelo:\n\\[\ngdp_t=\\beta_0 + \\beta_1 ind_t + \\beta_2 ser_t + \\beta_3 agri_t + \\beta_4 man_t + \\mu_t\n\\tag{1}\\]\n\n\nPara a análise gráfico podemos fazer de duas formas: através de um gráfico de dispersão entre duas variáveis, ou através de dois gráficos de linhas comparando as séries temporais das variáveis.\nO grafíco de dispersão entre duas variáveis pode ser feito com a função ggplot(). Para este exemplo camos comparar a variável man com a ind:\n\nggplot(m_reg, aes(x = man, y = ind)) +\n  geom_point(alpha = 0.7, color = \"steelblue\", size = 2.5) +\n  labs(title = \"Gráfico de Dispersão: man vs ind\",\n       x = \"Indústria Transformadora\",\n       y = \"Indústria\") +\n       theme_minimal() + #tema para o gráfico\n       theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"gray50\"),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nAtravés do gráfico de dispersão, podemos observar a relação entre as variáveis man e ind, o que pode indicar a presença de multicolinearidade imperfeita. Pois parece existe uma relação linear entre as duas variáveis (é possível fazer uma regressão linear bem ajustada para representar as observações). Se não fosse possível fazer uma regressão linear bem ajustada, poderíamos considerar que não há multicolinearidade entre as duas variáveis.\nPara o gráfico de linhas:\n\n# Criar dados para o gráfico\ndados_graf &lt;- data.frame(\n     tempo = 1:nrow(m_reg),\n     man = m_reg$man,\n     ind = m_reg$ind\n)\n\n# Gráfico combinado\nggplot(dados_graf, aes(x = tempo)) +\n     geom_line(aes(y = man, color = \"Indústria Transformadora\"), size = 1.2) +\n     geom_line(aes(y = ind, color = \"Indústria\"), size = 1.2) +\n     scale_color_manual(values = c(\"Indústria Transformadora\" = \"steelblue\", \n                                   \"Indústria\" = \"red\")) +\n     labs(title = \"Comparação das Séries Temporais: man vs ind\",\n                x = \"Tempo\",\n                y = \"Valor\",\n                color = \"Variáveis\") +\n     theme_minimal() +\n     theme(legend.position = \"bottom\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nO comportamento das séries ao longo do tempo é muito semelhantes, o que pode indicar a presença de multicolinearidade imperfeita.\n\n\n\nOutra forma de detetar a multicolinearidade é através da matriz de correlação. A matriz de correlação mostra o grau de associação linear entre as variáveis. Cada valor na matriz varia entre -1 e 1, onde valores próximos de 1 ou -1 indicam uma forte correlação, o que pode indicar a presença de multicolinearidade.\nPara a matriz de correlação, podemos utilizar a função cor() do R. Vamos calcular a matriz de correlação para as variáveis de interesse do conjunto de dados m_reg.\n\n# selecionar variáveis de interesse\nmreg_sel &lt;- m_reg %&gt;%\n    select(ind, ser, agri, man)\n# Matriz de correlação\ncor_matrix &lt;- cor(mreg_sel)\ncor_matrix\n\n           ind       ser      agri       man\nind  1.0000000 0.7093591 0.5217541 0.9155984\nser  0.7093591 1.0000000 0.7109302 0.9103241\nagri 0.5217541 0.7109302 1.0000000 0.6737959\nman  0.9155984 0.9103241 0.6737959 1.0000000\n\n\nOnde podemos ver que as variáveis ind e man têm uma correlação alta, o que pode indicar a presença de multicolinearidade. Nas restantes a correlação não é assim tão elevada.\nTambém podemos criar uma matriz gráfica com o package corrplot. Informações detalhadas sobre esta biblioteca podem ser encontradas em An Introduction to corrplot Package.\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\n#com números\ncor_1 &lt;- corrplot(cor_matrix, method = \"number\")\n\n\n\n\n\n\n\n#com círculos\ncor_2 &lt;- corrplot(cor_matrix, method = \"circle\")\n\n\n\n\n\n\n\n#com números e circulos\ncor_3 &lt;- corrplot.mixed(cor_matrix)\n\n\n\n\n\n\n\n\n\n\n\nO primeiro passo é estimar o modelo de regressão com as variáveis independentes para Equation 1:\n\nmodelo_multi &lt;- lm(gdp ~ ind + ser + agri + man, data = m_reg)\nsummary(modelo_multi)\n\n\nCall:\nlm(formula = gdp ~ ind + ser + agri + man, data = m_reg)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.222e+10 -3.659e+09  3.956e+08  3.477e+09  1.778e+10 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.293e+10  2.724e+10   2.678   0.0137 *  \nind          1.619e+00  1.868e-01   8.665 1.53e-08 ***\nser          1.396e+00  2.647e-02  52.754  &lt; 2e-16 ***\nagri        -5.464e-01  8.305e-01  -0.658   0.5174    \nman         -3.940e-01  4.227e-01  -0.932   0.3615    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.563e+09 on 22 degrees of freedom\nMultiple R-squared:  0.9995,    Adjusted R-squared:  0.9994 \nF-statistic: 1.112e+04 on 4 and 22 DF,  p-value: &lt; 2.2e-16\n\n\nNo resultado anterior temos 2 de 4 variáveis que não são estatisticamente diferentes de 0 (agri,man`), contudo o modelo tem um R² ajustado muito elevado (0.99), o que pode indicar a presença de multicolinearidade. Neste caso não conseguimos determinar quais as variáveis que estão a causar este problema.\n\n\n\nEste método consiste em estimar a regressão entre duas das variáveis independentes, ou seja, estimar a regressão do método de deteção de multicolinearidade pelo gráfico de dispersão. Quais as variáveis que devemos escolher neste caso? Com já temos uma anáilise de correlações e gráfica, já sabemos quais as que devemos testar. Caso contrário teríamos que fazer várias regressões com duas das variáveis independentes de cada vez. Para a regressão:\n\\[\nman_t = \\beta_0 + \\beta_1 ind_t + \\epsilon_t\n\\]\nNo R:\n\nmodelo_ind &lt;- lm(man ~ ind - 1, data = m_reg)\nsummary(modelo_ind)\n\n\nCall:\nlm(formula = man ~ ind - 1, data = m_reg)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.420e+10 -6.354e+09 -4.233e+09  7.569e+09  1.409e+10 \n\nCoefficients:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nind 0.565867   0.004297   131.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.529e+09 on 26 degrees of freedom\nMultiple R-squared:  0.9985,    Adjusted R-squared:  0.9984 \nF-statistic: 1.734e+04 on 1 and 26 DF,  p-value: &lt; 2.2e-16\n\n\nO R² é muito elevado (0.999), o que indica a presença de multicolinearidade entre as variáveis man e ind. O que indique que a variação de man pode ser explicada em grande parte pela variação de ind.\nem que graficamente:\n\nggplot(m_reg, aes(x = ind, y = man)) +\n  geom_point(alpha = 0.7, color = \"steelblue\", size = 5.5) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x - 1, \n              color = \"red\", se = FALSE, \n              alpha = 0.2) +\n  labs(title = \"Regressão Linear: Indústria vs Indústria Transformadora\",\n       x = \"Indústria (ind)\",\n       y = \"Indústria Transformadora (man)\") +\n  theme_minimal() + #tema para o gráfico\n  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nA conclusão permance, suspeitamos que existe multicolinearidade imperfeita entre estas duas variáveis. També poderíamos inverter a variável dependente pela independente:\n\nmodelo_ind_inv &lt;- lm(ind ~ man - 1, data = m_reg)\nsummary(modelo_ind_inv)\n\n\nCall:\nlm(formula = ind ~ man - 1, data = m_reg)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.425e+10 -1.278e+10  8.000e+09  1.181e+10  2.554e+10 \n\nCoefficients:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nman   1.7646     0.0134   131.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.506e+10 on 26 degrees of freedom\nMultiple R-squared:  0.9985,    Adjusted R-squared:  0.9984 \nF-statistic: 1.734e+04 on 1 and 26 DF,  p-value: &lt; 2.2e-16\n\n\nEm que podemos observar a mesma conclusão.\n\n\n\nPor último o teste VIF que é o mais utilizado em econometria. O teste VIF consiste em calcular para cada uma das variáveis independentes quanto a variância de um coeficiente da regressão é inflacionada devido à presença de multicolinearidade. O VIF é dado por:\n\\[\nVIF_i = \\frac{1}{1 - R^2_i}\n\\tag{2}\\]\nem que \\(R^2_i\\) é o coeficiente de determinação da regressão da variável \\(X_i\\) em função das restantes variáveis independentes. Para a equação Equation 1 (objeto modelo_multi) vamos estimar a regressão de cada variável independente em função das restantes:\n\nmodelo_multi_ind &lt;- lm(ind ~ ser + agri + man, \n                      data = m_reg)\nmodelo_multi_ser &lt;- lm(ser ~ ind + agri + man, \n                       data = m_reg)\nmodelo_multi_agri &lt;- lm(agri ~ ind + ser + man, \n                        data = m_reg)\nmodelo_multi_man &lt;- lm(man ~ ind + ser + agri, \n                       data = m_reg)\n\nAgora vamos extrair o R^2 de cada uma das regressões ($r.squared) e calcular o VIF com a Equation 2:\n\n# Extrair R²\nr2_ind &lt;- summary(modelo_multi_ind)$r.squared\nr2_ser &lt;- summary(modelo_multi_ser)$r.squared\nr2_agri &lt;- summary(modelo_multi_agri)$r.squared\nr2_man &lt;- summary(modelo_multi_man)$r.squared\n\n# Calcular VIF\nvif_ind &lt;- 1 / (1 - r2_ind)\nvif_ser &lt;- 1 / (1 - r2_ser)\nvif_agri &lt;- 1 / (1 - r2_agri)\nvif_man &lt;- 1 / (1 - r2_man)\n\n#mostrar o VIF\nvif_values &lt;- data.frame(\n  Variable = c(\"Indústria\",\n                \"Serviços\",\n                \"Agricultura\",\n                \"Indústria Transformadora\"),\n  VIF = c(vif_ind, vif_ser, vif_agri, vif_man)\n)\nprint(vif_values)\n\n                  Variable       VIF\n1                Indústria 14.182238\n2                 Serviços 13.372233\n3              Agricultura  2.074316\n4 Indústria Transformadora 41.424816\n\n\nEm alternativa, e esta bem mais prática, podemos usar a função vif() da biblioteca car:\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nvif(modelo_multi)\n\n      ind       ser      agri       man \n14.182238 13.372233  2.074316 41.424816 \n\n\nA biblioteca performance pode ser utilizada para calcular o VIF e outras métricas para uma análise de colinearidade mais informativa. Para isso recorremos à função check_collinearity().\n\nlibrary(performance)\ncheck_collinearity(modelo_multi)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Term  VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n agri 2.07 [ 1.47,  3.43]     1.44      0.48     [0.29, 0.68]\n\nHigh Correlation\n\n Term   VIF     VIF 95% CI adj. VIF Tolerance Tolerance 95% CI\n  ind 14.18 [ 8.39, 24.50]     3.77      0.07     [0.04, 0.12]\n  ser 13.37 [ 7.93, 23.09]     3.66      0.07     [0.04, 0.13]\n  man 41.42 [24.00, 72.05]     6.44      0.02     [0.01, 0.04]\n\n\nEsta função (mais info ?check_collinearity) fornece uma visão geral da colinearidade entre as variáveis independentes. Separa as variáveis em grupos com base na sua colinearidade (baixa e alta). apresenta também o Intervalo de confiança para o VIF (VIF 95% CI). Apresenta mais duas métricas como a Increased SE que indica o aumento percentual do erro padrão do coeficiente devido à colinearidade, e a Tolerance que é o inverso do VIF (1/VIF). Análise para a variável man:\n\nIncreased SE:6.44 - 6.44 vezes maior que o desvio padrão esperado para o coeficiente\nTolerance:0.02 - Indica que apenas 2% da variância é independente\n\nPortanto, podemos concluir que existe uma forte multicolinearidade."
  },
  {
    "objectID": "capitulo-05-avaliacao-modelos.html#sec-heterocedasticidade",
    "href": "capitulo-05-avaliacao-modelos.html#sec-heterocedasticidade",
    "title": "Avaliação dos Modelos de Regressão",
    "section": "",
    "text": "Para esta aplicação vamos utilizar o conjunto de dados hprice1 da biblioteca wooldridge.\n\n# Carregar bibliotecas necessárias\nlibrary(wooldridge)\nlibrary(tidyverse)\n\n# Carregar dados\ndata(\"hprice1\")\n\nA heterocedasticidade é medida nos erros. Para isso, vamos utilizar o modelo base (modelo_0) e obter os resíduos u_i:\n\n#estimar modelo\nmodelo_0 &lt;- lm(price ~ lotsize + sqrft + bdrms,\n                     data = hprice1)\n\n# armazenar erro no data frame\nhprice1$u_i &lt;- residuals(modelo_0)\nsummary(hprice1$u_i) #eststistica descritiva do erro\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-120.026  -38.530   -6.555    0.000   32.323  209.376 \n\n\n\n\nPara testar a heterocedasticidade graficamente, podemos utilizar um gráfico de dispersão dos resíduos em relação aos valores ajustados ou com outra variável independente. Se os resíduos apresentarem um padrão específico (como um funil ou uma curva), pode indicar a presença de heterocedasticidade dos erros.\n\n# Gráfico de dispersão dos resíduos\nggplot(hprice1, aes(x = fitted(modelo_0), y = u_i)) +\n  geom_point(alpha = 0.7, color = \"steelblue\", size = 2.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Gráfico de Resíduos vs Valores Ajustados\",\n       x = \"Valores Ajustados\",\n       y = \"Resíduos\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\nUm dos testes mais utilizado para testar a heterocedasticidade é o teste de Breusch-Pagan [@breusch_simple_1979]. Os testes têm um processo semelhante como referido na secção de pressupostos de homocedasticidade do capítulo anterior. A regressão auxiliar do teste é dada por:\n\\[\n{\\mu_i}^2 = \\delta_0 + \\delta_1 lotsize_i + \\delta_2 sqrft_i + \\delta_3 bdrms_i + \\sigma_i\n\\]\nDepois de estimar vamos obter o \\(LM_{stat}=n*R^2\\) e calcular o valor da probabilidade do \\(\\chi^2\\).\nNo R:\n\n#calcular u_i^2\nhprice1$u_i2 &lt;- hprice1$u_i^2\n\n#Estimar reg-bp\nreg_bp &lt;- lm(u_i2 ~ lotsize + sqrft + bdrms, data = hprice1)\n\n#Obter R2\nr2_bp &lt;- summary(reg_bp)$r.squared\n\n# Obter n\nn &lt;- nrow(hprice1)\n\n# Número de variáveis independentes - k\nk &lt;- reg_bp$rank - 1\n\n# LM stat\nLM_stat_bp &lt;- n * r2_bp\n\n#Obter valor P\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\np_value_bp &lt;- 1 - pchisq(LM_stat_bp, df = k)\np_value_bp\n\n[1] 0.00278206\n\n\nO $r.squared extrai o valor de R² da regressão auxiliar, que é utilizado para calcular o estatístico LM do teste de Breusch-Pagan. O $rank devolve o número de coeficientes (-1 para excluis a constante). A função pchisq() é utilizada para calcular o valor p associado ao estatístico LM, com base na distribuição qui-quadrado, onde df é o número de variáveis independentes na regressão auxiliar (k). Este processo pode ser mais trabalhoso, mas é possível executar qualquer test de heterocedasticidade.\nA biblioteca skedastic tem várias funções para testar a heterocedasticidade. Neste caso, o teste de Breusch-Pagan pode ser obtido com a função breusch_pagan():\n\nlibrary(skedastic)\nbreusch_pagan(modelo_0)\n\n# A tibble: 1 × 5\n  statistic p.value parameter method                alternative\n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;      \n1      14.1 0.00278         3 Koenker (studentised) greater    \n\n\nA conclusão (o valor de P é o mesmo) é que a hipótese nula é rejeitada para qualquer nível de significância estatística. Em que a hipótese nula do teste de heterocedasticidade é: os erros são homocedásticos.\n\n\n\nO teste de Glesjer [@glejser_new_1969] tem como a regressão auxiliar:\n\\[\n|\\mu_i| = \\delta_0 + \\delta_1 \\cdot lotsize_i + \\delta_2 \\cdot sqrft_i + \\delta_3 \\cdot bdrms_i + \\sigma_i\n\\tag{3}\\]\nO processo é semelhante ao teste de Breusch-Pagan. No R:\n\n# Calcular |u_i|\nhprice1$u_i_abs &lt;- abs(hprice1$u_i)\n\n# Estimar reg-glesjer\nreg_glesjer &lt;- lm(u_i_abs ~ lotsize + sqrft + bdrms, \n                  data = hprice1)\n\n# Obter R2\nr2_glesjer &lt;- summary(reg_glesjer)$r.squared\n\n# Obter n\nn &lt;- nrow(hprice1)\n\n# Número de variáveis independentes - k\nk &lt;- reg_glesjer$rank - 1\n\n# LM stat\nLM_stat_glesjer &lt;- n * r2_glesjer\n\n# Obter valor P\nlibrary(lmtest)\np_value_glesjer &lt;- 1 - pchisq(LM_stat_glesjer, \n                        df = k)\np_value_glesjer\n\n[1] 0.0004458974\n\n\nO mesmo resultado pode ser obtido utilizando a função gl() da biblioteca skedastic. A hipótese nula é rejeitada para qualquer nível de significância estatística.\n\n\nO teste Harvey-Godfrey [@harvey_estimating_1976, @godfrey_testing_1978] tem como regressão auxiliar:\n\\[\nln(\\mu_i^2) = \\delta_0 + \\delta_1 lotsize_i + \\delta_2 sqrft_i + \\delta_3 bdrms_i + \\sigma_i\n\\tag{4}\\]\nO processo é semelhante a qualquer LM teste para heterocedasticidade. Tabém pode ser obtido utilizando a função harvey_godfrey() da biblioteca skedastic:\n\nharvey(modelo_0)\n\n# A tibble: 1 × 4\n  statistic p.value parameter alternative\n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      \n1      8.49  0.0370         3 greater    \n\n\nNeste caso a hipótese nula é rejeitada para 5% e 10% de significância estatística.\n\n\n\n\nO teste de Park [@park_estimation_1966] tem como regressão auxiliar:\n\\[\nln(\\mu_i^2) = \\delta_0 + \\delta_1 ln(lotsize_i) + \\delta_2 ln(sqrft_i) + \\delta_3 ln(bdrms_i) + \\sigma_i\n\\tag{5}\\]\nA bibliotca skedastic não tem o teste de Park. Este é um dos exemplos em que temos que fazer o processo passo a passo:\n\n# Calcular ln(u_i^2)\nhprice1$ln_u_i2 &lt;- log(hprice1$u_i^2)\n\n# Estimar regressão auxiliar\nreg_park &lt;- lm(ln_u_i2 ~ log(lotsize) + log(sqrft) + log(bdrms),\n                data = hprice1)\n\n# Obter R2\nr2_park &lt;- summary(reg_park)$r.squared\n\n# Obter n\nn &lt;- nrow(hprice1)\n\n# Número de variáveis independentes - k\nk &lt;- reg_park$rank - 1\n\n# LM stat\nLM_stat_park &lt;- n * r2_park\n\n# Obter valor P\nlibrary(lmtest)\np_value_park &lt;- 1 - pchisq(LM_stat_park, df = k)\np_value_park\n\n[1] 0.1142573\n\n\nA hipótese nula de homocedasticidade não é rejeitada para nenhum nível de significância estatística.\n\n\n\nO teste de White [@white_heteroskedasticity-consistent_1980] é um teste geral para heterocedasticidade que não assume uma forma específica de heterocedasticidade. A regressão auxiliar do teste de White inclui as variáveis independentes e os termos quadráticos:\n\\[\n\\begin{split}\n{\\mu_i}^2 = \\delta_0 & + \\delta_1 lotsize_i + \\delta_2 sqrft_i + \\delta_3 bdrms_i \\\\\n&+ \\delta_4 lotsize_i^2 + \\delta_5 sqrft_i^2 + \\delta_6 bdrms_i^2 + \\sigma_i\n\\end{split}\n\\tag{6}\\]\ne pode incluir também interações entre as variáveis independentes :\n\\[\n\\begin{split}\n{\\mu_i}^2 = \\delta_0 &+ \\delta_1 lotsize_i + \\delta_2 sqrft_i + \\delta_3 bdrms_i \\\\\n&+ \\delta_4 lotsize_i^2 + \\delta_5 sqrft_i^2 + \\delta_6 bdrms_i^2 \\\\\n&+ \\delta_7 (lotsize_i \\cdot sqrft_i) + \\delta_8 (lotsize_i \\cdot bdrms_i) \\\\\n&+ \\delta_9 (sqrft_i \\cdot bdrms_i) + \\sigma_i\n\\end{split}\n\\tag{7}\\]\nPara a Equation 6 podemos utilizar a função white() da biblioteca skedastic:\n\nwhite(modelo_0)\n\n# A tibble: 1 × 5\n  statistic p.value parameter method       alternative\n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n1      20.6 0.00216         6 White's Test greater    \n\n\ne para a Equation 7 podemos utilizar a função white() com o argumento interactions = TRUE:\n\nwhite(modelo_0, interactions = TRUE)\n\n# A tibble: 1 × 5\n  statistic   p.value parameter method       alternative\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;      \n1      33.7 0.0000995         9 White's Test greater    \n\n\nPara qualquer uma das variações do teste, a hipótese nula é rejeitada para todos os níveis de significância estatística.\n\n\n\nO teste de ARCH [@engle_autoregressive_1982] pode ser utilizado para detectar a presença de heterocedasticidade condicional, especialmente em séries temporais. A regressão auxiliar do teste de ARCH é dada por:\nPara este exemplo vamo utilizar os dados do ficheiro m_reg.xlsx.\n\nlibrary(readxl)\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\nEstimar o modelo de regressão:\n\nmodelo_0 &lt;- lm(gdp ~ ind + ser + agri + man, data = m_reg)\n\nPara executar o teste de ARCH utilizado a função ArchTest() da biblioteca FinTS:\n\nlibrary(FinTS)\nteste_arch &lt;- ArchTest(residuals(modelo_0), lags = 1)\nteste_arch\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  residuals(modelo_0)\nChi-squared = 2.6047, df = 1, p-value = 0.1065\n\n\nO argumento lags especifica o número de defasamentos do teste, ou seja, a ordem do modelo ARCH a ser testada. Para a ordem 1, existe heterocedasticidade para qualquer nível de significância estatística (valor P &lt; 0.01). Em que a hipótese nula do teste é: não existem efeitos ARCH para a ordem 1.\n\n\n\nA biblioteca skedastic oferece uma variedade de outros testes para heterocedasticidade, para mais testes:\n\nlibrary(skedastic)\nls(\"package:skedastic\")\n\n [1] \"alvm.fit\"          \"anlvm.fit\"         \"anscombe\"         \n [4] \"avm.ci\"            \"avm.fwls\"          \"avm.vcov\"         \n [7] \"bamset\"            \"bickel\"            \"blus\"             \n[10] \"bootlm\"            \"breusch_pagan\"     \"carapeto_holt\"    \n[13] \"cook_weisberg\"     \"countpeaks\"        \"dDtrend\"          \n[16] \"diblasi_bowman\"    \"dpeak\"             \"dpeakdat\"         \n[19] \"dufour_etal\"       \"evans_king\"        \"glejser\"          \n[22] \"godfrey_orme\"      \"goldfeld_quandt\"   \"GSS\"              \n[25] \"harrison_mccabe\"   \"harvey\"            \"hccme\"            \n[28] \"hetplot\"           \"honda\"             \"horn\"             \n[31] \"li_yao\"            \"pDtrend\"           \"ppeak\"            \n[34] \"pRQF\"              \"rackauskas_zuokas\" \"simonoff_tsai\"    \n[37] \"szroeter\"          \"T_alpha\"           \"twosidedpval\"     \n[40] \"verbyla\"           \"white\"             \"wilcox_keselman\"  \n[43] \"yuce\"              \"zhou_etal\"        \n\n\nA biblioteca performance oferece funções para avaliar o desempenho de modelos de regressão, incluindo testes de heterocedasticidade, como a check_heteroscedasticity():\n\nlibrary(performance)\ncheck_heteroscedasticity(modelo_0)\n\nOK: Error variance appears to be homoscedastic (p = 0.145).\n\n\nEsta função realiza o teste de Breusch-Pagan LM e fornece até uma resposta visual."
  },
  {
    "objectID": "capitulo-05-avaliacao-modelos.html#diagnóstico-da-autocorrelação-dos-erros",
    "href": "capitulo-05-avaliacao-modelos.html#diagnóstico-da-autocorrelação-dos-erros",
    "title": "Avaliação dos Modelos de Regressão",
    "section": "",
    "text": "Nesta aplicação vamos utilizar os dados do exemplos anterior (ficheiro m_reg.xlsx) e estimar o modelo:\n\n#carregar bibliotecas\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(performance)\n\n#carregar dados\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\n#estimar modelo\nmodelo &lt;- lm(gdp ~ ind + ser + agri + man, data = m_reg)\n\n\n\nA forma informal de detetar a heterocedaticide é através de um gráfico de dispersão dos resíduos e dos resíduos desfasados do modelo. Para isso é necessário o obter os resíduos (u_t) do modelo e os resíduos desfasados (u_t1). No R:\n\nm_reg &lt;- m_reg |&gt; \n          mutate(\n            u_t = residuals(modelo), \n            u_t1 = lag(u_t))\n\nGráfico de dispersão dos resíduos com ggplot():\n\nlibrary(ggplot2)\n\n#| label: grafico_residuos_autocorr\nggplot(m_reg, aes(x = u_t1, y = u_t)) +\n  geom_point(alpha = 0.7, color = \"steelblue\", size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Gráfico de Dispersão dos Resíduos vs Resíduos Desfasados\",\n       x = \"Resíduos Desfasados (u_t-1)\",\n       y = \"Resíduos (u_t)\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank())\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAtravés do gráfico podemos que existe autocorrelação nos resíduos.\n\n\n\nO teste de Durbin-Watson (DW) [@durbin_testing_1950; @durbin_testing_1951] pode ser utilizado para detectar a presença de autocorrelação nos resíduos de um modelo de regressão. O teste tem os seguintes pressupostos:\n\nO modelo tem constante\nAutocorrelação de 1ª ordem\nNão existe variável dependente desfasada com independente\n\nO passo a passo para executar o teste de Durbin-Watson é o seguinte:\n\nEstimar o modelo e obter os resíduos.\nCalcular a estatística de DW\n\n\\[\nDW_{stat} = \\frac{\\sum_{t=2}^{n} (u_t - u_{t-1})^2}{\\sum_{t=1}^{n} u_t^2}\n\\tag{8}\\]\nNo R:\n\n# Calcular numerador: soma de (u_t - u_{t-1})^2\nnumerador &lt;- sum((m_reg$u_t[-1] - m_reg$u_t[-nrow(m_reg)])^2, na.rm = TRUE)\n\n# Calcular denominador: soma de u_t^2\ndenominador &lt;- sum(m_reg$u_t^2, na.rm = TRUE)\n\n# Calcular estatística DW\nDW_stat &lt;- numerador / denominador\nDW_stat\n\n[1] 1.262287\n\n\nO valor de \\(DW_{stat}\\) pode também ser obtido através da função dwtest() da biblioteca lmtest:\n\n#|label: dw_test\nlibrary(lmtest)\ndwtest(modelo)\n\n\n    Durbin-Watson test\n\ndata:  modelo\nDW = 1.2623, p-value = 0.004077\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\nConstruir a tabela com o \\(DW_{stat}\\):\n\n\n\n\n\n\n\n\n\n\nOnde:\n\n\\(d_I\\) é a distância de Durbin-Watson inferior\n\\(d_S\\) é a distância de Durbin-Watson superior.\n\nOs limites para cada nível de significância podem ser obtidas em: Durbin-Watson Significance Tables\nEm alternativa, é possivel calcular o \\(DW_stat\\) e obter diretamente o valor da probabilidade com a função dwtest() da biblioteca lmtest:\n\n#|label: dw_test\nlibrary(lmtest)\ndwtest(modelo)\n\n\n    Durbin-Watson test\n\ndata:  modelo\nDW = 1.2623, p-value = 0.004077\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\nConcluir (a hipótese nula é que não existe autocorrelação)\n\nA hipótese nula do teste de DW é que não existe autocorrelação. Considerando o valor da probabilidade, a Hipótese nula é rejeitada para qualquer nível de significância estatística.\nA biblioteca performance também tem a função check_autocorrelation() que pode ser utilizada para testar a autocorrelação dos resíduos:\n\n#|label: performance_check_autocorrelation\nlibrary(performance)\ncheck_autocorrelation(modelo)\n\nWarning: Autocorrelated residuals detected (p = 0.010).\n\n\nO teste DW tem algumas limitações, já mencionadas nos pressupostos. Tem ainda também a limitação de o \\(DW_stat\\) ficar nas zonas de indecisão e com isto não ser conclusivo. Existe também o teste Durbin h-H (DH) que é uma extensão do teste DW e que pode ser utilizado quando a variável dependente é defasada. O teste DH não é tão utilizado como o DW.\n\n\n\nO teste de Breusch-Godfrey [@breusch_auto_1978; @godfrey_testing_1978] tem vantagem de poder ser utilizado para detetar autocorrelação de ordens superiores a 1. O teste tem como base a equação:\n\\[\n\\begin{split}\ngdp_t = \\beta_0 & + \\beta_1 inf_t + \\beta_2 ser_t+ \\beta_3 agri_t + \\beta_4 man_t + \\mu_t\n\\end{split}\n\\tag{9}\\]\nonde:\n\\[\n\\mu_t = \\rho_1 \\mu_{t-1} + \\rho_2 \\mu_{t-2} + ... + \\rho_p \\mu_{t-p} + \\epsilon_t\n\\tag{10}\\]\nonde \\(p\\) é a ordem de autocorrelação a ser testada.\nPortanto, a H0: \\(\\rho_1 = rho_2 = ... = rho_p = 0\\) (não existe autocorrelação) Enquanto que a H1: pelo menos um \\(\\rho_i \\neq 0\\) (existe autocorrelação). O teste é feito com o modelo LM (ou seja Equation 9 e a Equation 10) em que o \\(LM_{stat} = (n-\\rho)R^2\\) e o valor de P é obtido através da distribuição \\(\\chi^2\\). O teste pode ser calculado de forma conveniente com a função bgtest():\n\n#|label: bg_test\nlibrary(lmtest)\n\n#odem 1\nbgtest(modelo)\n\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  modelo\nLM test = 1.0179, df = 1, p-value = 0.313\n\n#ordem 2\nbgtest(modelo, order = 2, type = \"Chisq\")\n\n\n    Breusch-Godfrey test for serial correlation of order up to 2\n\ndata:  modelo\nLM test = 1.0368, df = 2, p-value = 0.5955\n\n\nO primeiro argumento da função é objeto do modelo. Existem mais alguns argumentos que são necessários order (a ordem da autocorrelação a ser testada) e type (o tipo de teste a ser realizado, como Chisq ou F). Por defeito, o argumento order é 1 e o argumento type é Chisq. A hipótese nula é rejeitada para qualquer nível de significância estatística.\nSegundo o resultado do teste, a hipótese nula não foi rejeitada, o que indique que não existe autocorrelação de ordem 1 nem ordem 2."
  },
  {
    "objectID": "capitulo-05-avaliacao-modelos.html#sec-normalidade",
    "href": "capitulo-05-avaliacao-modelos.html#sec-normalidade",
    "title": "Avaliação dos Modelos de Regressão",
    "section": "",
    "text": "Tal como os diagnósticos anteriores, este também pode ser feito através de gráficos e de testes estatísticos. Para esta aplicação vamos utilizar o modelo anterior.\n\n#carregar bibliotecas\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(performance)\n\n#carregar dados\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\n#estimar modelo\nmodelo &lt;- lm(gdp ~ ind + ser + agri + man, data = m_reg)\n\n#obter erro\nm_reg$residuos &lt;- residuals(modelo)\n\n\n\nPara o gráfico de histograma dos resíduos e comparar com a linha de normalidade:\n\nlibrary(ggplot2)\n\nggplot(m_reg, aes(x = residuos)) +\n  geom_histogram(aes(y = after_stat(density)),\n                bins = 30, fill = \"steelblue\", \n                color = \"black\", alpha = 0.7) +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(m_reg$residuos), \n                           sd = sd(m_reg$residuos)), \n                color = \"red\", size = 1.2) +\n  labs(title = \"Histograma dos Resíduos com Curva Normal\", \n       x = \"Resíduos\", \n       y = \"Densidade\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nUtilizando exclusivamente a biblioteca o histograma dos resíduos podemos concluir que aparentemente os resíduos seguem uma distribuição normal. É perfeitamente normal em amostras pequenas haver desvios (barras) em relação à distribuição normal (linha vermelha)\n\n\n\nO teste de @shapiro_analysis_1965 (SW) é um dos testes mais utilizados para testar a normalidade dos resíduos. A hipótese nula é que os resíduos seguem uma distribuição normal. O teste pode ser realizado com a função shapiro.test():\n\nshapiro.test(m_reg$residuos)\n\n\n    Shapiro-Wilk normality test\n\ndata:  m_reg$residuos\nW = 0.96487, p-value = 0.4735\n\n\nA H0 não é rejeitada para qualquer nível de significância estatística. O W é a estatística do teste. O teste SW tem um grande poder estatístico em detetar desvios em relação à normalidade, pois é muit sensível a desvios tanto no centro como nas caudas da distribuição.\nA função check_normality() da biblioteca performance tabém realizado o teste SW:\n\nlibrary(performance)\n\ncheck_normality(modelo)\n\nOK: residuals appear as normally distributed (p = 0.258).\n\n\n\n\n\nO teste de @jarque_test_1987 (JB) é outro teste muito utilizado para verificar a normalidade dos resíduos. A hipótese nula é a mesma do teste SW. O teste pode ser realizado com a função jarque.test() da biblioteca moments.\n\n#carregar biblioteca\nlibrary(moments)\n\n#teste de normalidade\njarque.test(m_reg$residuos)\n\n\n    Jarque-Bera Normality Test\n\ndata:  m_reg$residuos\nJB = 3.3251, p-value = 0.1897\nalternative hypothesis: greater\n\n\nComo H0 não é rejeitada, os erros seguem uma distribuição normal. Se H0 for rejeitada podemo obter os valores de curtose e assimetria para perceberem a razão dos resíduos não seguirem uma distribuição normal:\n\n#Kurtosis (achatamento)\nkurtosis(m_reg$residuos)\n\n[1] 4.288396\n\n#Skewnedd (assimetria)\nskewness(m_reg$residuos)\n\n[1] 0.5691352"
  },
  {
    "objectID": "capitulo-05-avaliacao-modelos.html#sec-especificacao",
    "href": "capitulo-05-avaliacao-modelos.html#sec-especificacao",
    "title": "Avaliação dos Modelos de Regressão",
    "section": "",
    "text": "A especificação de um modelo de regressão está relacionada com a inclusão de variáveis (Overfitting) que não são relevantes e a exclusão de variáveis relevantes (Underfitting).\nA não inclusão de variáveis relevantes pode levar a um modelo mal especificado, em que o valor esperado do resíduos \\(E[\\mu_i|X_i] \\neq 0\\), o que faz com que essa variável (omitida) seja confundida com o erro. Já a inclusão de variáveis irrelevantes pode levar a um modelo com excesso de ajustamento, em que o valor esperado do resíduos \\(E[\\mu_i|X_i] = 0\\), mas a inclusão de variáveis desnecessárias (com informação redundante) pode aumentar a variância dos estimadores.\nPara este exemplo vamos utilizar os dados wage2 da biblioteca wooldridge:\n\nlibrary(wooldridge)\ndata(\"wage2\")\n\nPara ver a definição das variáveis executar ?wage2.\nVamos estimar o seguinte modelo: \\[\nwage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 tenure_i + \\mu_i\n\\tag{11}\\]\ne o modelo sem a variável tenure:\n\\[\nwage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\mu_i\n\\tag{12}\\]\nNo R:\n\nmodelo_completo &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\nmodelo_restrito &lt;- lm(wage ~ educ + exper, data = wage2)\n\n#comparar os modelos\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nstargazer::stargazer(modelo_completo, modelo_restrito, type = \"text\")\n\n\n===================================================================\n                                  Dependent variable:              \n                    -----------------------------------------------\n                                         wage                      \n                              (1)                     (2)          \n-------------------------------------------------------------------\neduc                       74.415***               76.216***       \n                            (6.287)                 (6.297)        \n                                                                   \nexper                      14.892***               17.638***       \n                            (3.253)                 (3.162)        \n                                                                   \ntenure                     8.257***                                \n                            (2.498)                                \n                                                                   \nConstant                  -276.240***             -272.528**       \n                           (106.702)               (107.263)       \n                                                                   \n-------------------------------------------------------------------\nObservations                  935                     935          \nR2                           0.146                   0.136         \nAdjusted R2                  0.143                   0.134         \nResidual Std. Error   374.306 (df = 931)      376.295 (df = 932)   \nF Statistic         53.003*** (df = 3; 931) 73.260*** (df = 2; 932)\n===================================================================\nNote:                                   *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nAo comparar os dois modelos podemos ver que sinais dos coeficinets são consistentes e os valores muito semelhantes. A variável exper representa os anos de experiência profissional e a variável tenure representa os anos na empresa. Será que estas variáveis contém a mesma informação? Pos os anos de exper podem estar contabilizados nos anos de tenure. Para testar vamos utilizar a função waldtest da biblioteca lmtest:\n\nlibrary(lmtest)\nwaldtest(modelo_restrito, modelo_completo)\n\nWald test\n\nModel 1: wage ~ educ + exper\nModel 2: wage ~ educ + exper + tenure\n  Res.Df Df      F   Pr(&gt;F)    \n1    932                       \n2    931  1 10.929 0.000983 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPara este caso, estamos a testar se a variável tenure pode ser omitida do modelo. E concluímos que a variável tenure é necessária, ou seja, a informação da variável vai para além da informação do nº de anos de experiência. Está acapturar efeitos como a lealdade do funcionário à empresa, aumento da produtividade ao longo do tempo, entre outros fatores. A variável exper representa apenas a experiência profissional (pode ser numa profissão totalmente diferente). O que nos pode levar a pensar que o modelo está mal especificado se omitirmos a variável tenure. Este resultado també nos pode fazer pensar se a variável exper é mesmo necessário, para despistar vamos fazer um modelo omitindo a variável exper e realziar o teste:\n\nmodelo_completo2 &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\nmodelo_restrito2 &lt;- lm(wage ~ educ + tenure, data = wage2)\n\n#comparar os modelos\nlibrary(stargazer)\nstargazer::stargazer(modelo_completo2, modelo_restrito2, type = \"text\")\n\n\n===================================================================\n                                  Dependent variable:              \n                    -----------------------------------------------\n                                         wage                      \n                              (1)                     (2)          \n-------------------------------------------------------------------\neduc                       74.415***               61.148***       \n                            (6.287)                 (5.639)        \n                                                                   \nexper                      14.892***                               \n                            (3.253)                                \n                                                                   \ntenure                     8.257***                11.177***       \n                            (2.498)                 (2.441)        \n                                                                   \nConstant                  -276.240***               53.519         \n                           (106.702)               (79.557)        \n                                                                   \n-------------------------------------------------------------------\nObservations                  935                     935          \nR2                           0.146                   0.127         \nAdjusted R2                  0.143                   0.125         \nResidual Std. Error   374.306 (df = 931)      378.293 (df = 932)   \nF Statistic         53.003*** (df = 3; 931) 67.579*** (df = 2; 932)\n===================================================================\nNote:                                   *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\nwaldtest(modelo_restrito2, modelo_completo2)\n\nWald test\n\nModel 1: wage ~ educ + tenure\nModel 2: wage ~ educ + exper + tenure\n  Res.Df Df      F    Pr(&gt;F)    \n1    932                        \n2    931  1 20.957 5.333e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConcluímos que a variável exper também é necessária no modelo.\nE está também relacionada com a forma funcional do modelo. Algumas formas funcionais são:\n\nLinear\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0+\\beta_1x_{i}+\\mu_i\n\\tag{13}\\]\n\\(\\beta_0\\): representa o valor esperado de \\(y_i\\) quando \\(x_i=0\\)\n\\(\\beta_1\\): um aumento de uma unidade em \\(x_i\\) está associado a um aumento de \\(\\beta_1\\) unidades em \\(y_i\\), ceteris paribus.\n\nLinear - logarítmica\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0+\\beta_1\\ln(x_{i})+\\mu_i\n\\tag{14}\\]\n\\(\\beta_0\\): representa o valor esperado de \\(y_i\\) quando \\(ln(x_i)=0\\) (ou seja, \\(x_i=1\\))\n\\(\\beta_1\\): um aumento de 1% em \\(x_i\\) está associado a um aumento de \\(\\frac{\\beta_1}{100}\\) unidades em \\(y_i\\), ceteris paribus.\n\nLogarítmica - linear\n\n\n\n\n\n\n\n\n\n\n\\[\n\\ln(y_i)=\\beta_0+\\beta_1x_{i}+\\mu_i\n\\tag{15}\\]\n\\(\\beta_0\\): representa o valor esperado de \\(ln(y_i)\\) quando \\(x_i=0\\), para obter em ralação a \\(y = e^{\\beta_0}\\), no R: exp(coef(modelo))[\"(Intercept)\"].\n\\(\\beta_1\\): um aumento de uma unidade em \\(x_i\\) está associado a um aumento de \\(\\beta_1 \\cdot 100\\) % em \\(y_i\\), ceteris paribus.\n\nLogarítmica - logarítmica\n\n\n\n\n\n\n\n\n\n\n\\[\n\\ln(y_i)=\\beta_0+\\beta_1\\ln(x_{i})+\\mu_i\n\\tag{16}\\]\n\\(\\beta_1\\): um aumento de 1% em \\(x_i\\) está associado a um aumento de \\(\\beta_1\\) % em \\(y_i\\), ceteris paribus.\n\nExponencial\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0\\beta_1^{x_i}\\mu_i\n\\]\ncom LN’s podemos linearizar a equação exponencial:\n\\[\n\\ln(y_i)=\\ln(\\beta_0)+\\ln(\\beta_1).x_i+\\ln(\\mu_i)\n\\]\ne estimar:\n\\[\n\\ln(y_i)=\\beta'_0+\\beta'_1x_i+\\mu'_i\n\\]\n\nPotência\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0X_i^{\\beta_1}\\mu_i\n\\tag{17}\\]\nCom LN’s:\n\\[\nln(y_i)=ln(\\beta_0)+\\beta_1.ln(x_i)+ln(\\mu_i)\n\\tag{18}\\]\npara estimar: \\[\nln(y_i)=\\beta'_0+\\beta_1 ln(x_i)+\\mu'_i\n\\tag{19}\\]\n\nPolinomial\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0+\\beta_1x_i+\\beta_2x_i^{2}+\\mu_i\n\\tag{20}\\]\npara estimar:\n\\[\nz_i=x'_i=x_i^2\n\\tag{21}\\]\n\nHiperbólica\n\n\n\n\n\n\n\n\n\n\n\\[\ny_i=\\beta_0+\\beta_1\\frac{1}x_i+\\mu_i\n\\tag{22}\\]\npara estimar:\n\\[\nw_i=x'_i=\\frac{1}x_i\n\\tag{23}\\]\nDepois de linearizar, a interpretação da forma funcional é semelhante às lineares.\n\n\n\nO teste de Ramsey RESET [@ramsey_tests_1969] é utilizado para testar a especificação do modelo, nomeadamente se a forma funcional está correta. Para evitar que:\n\n\n\n\n\n\n\n\n\nO teste consiste em:\n\nEstimar o modelo e obter os valores previstos (wage_hat).\nIncluir termos polinomiais das variáveis independentes e reestimar o modelo com os termos (wage_hat^2, wage_hat^3, …).\nRealizar o teste F para verificar se os coeficientes dos termos polinomiais são conjuntamente significativos.\n\nNo R:\n\n#estimar modelo\nmodelo &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\n\n#obter valores previstos e y^ e y^3\n wage2 &lt;- wage2 |&gt; \n            mutate(\n              wage_hat = predict(modelo),\n              wage_hat2 = wage_hat^2,\n              wage_hat3 = wage_hat^3\n            )\n\nmodelo_reset &lt;- lm(wage ~ educ + exper + tenure +\n              wage_hat2 + wage_hat3, data = wage2)\n\n#teste\nwaldtest(modelo, modelo_reset)\n\nWald test\n\nModel 1: wage ~ educ + exper + tenure\nModel 2: wage ~ educ + exper + tenure + wage_hat2 + wage_hat3\n  Res.Df Df      F Pr(&gt;F)\n1    931                 \n2    929  2 0.5526 0.5756\n\n#ou diretamente\nresettest(modelo, power = 2:3)\n\n\n    RESET test\n\ndata:  modelo\nRESET = 0.55259, df1 = 2, df2 = 929, p-value = 0.5756\n\n\nO argumento power do teste de Ramsey RESET especifica os graus de liberdade dos termos polinomiais a serem incluídos no modelo. Neste caso, estamos a incluir os termos quadráticos e cúbicos.\nA hipótese nula do teste de Ramsey RESET é que o modelo original está corretamente especificado. Como a H0 não é rejeitada para nenhum nível de significância estatística, não há evidências suficientes para concluir que o modelo esteja mal especificado.\n\n\n\nA estabilidade dos coeficientes pode ser realizado com o teste CUSUM [@ploberger_cusum_1992]. Este teste verifica se os coeficientes do modelo permanecem constantes ao longo da amostra.\nNo R:\n\n#estimar modelo\nmodelo &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\n\n# Teste CUSUM\nlibrary(strucchange)\n\nLoading required package: sandwich\n\n\n\nAttaching package: 'strucchange'\n\n\nThe following object is masked from 'package:stringr':\n\n    boundary\n\n# Teste CUSUMSQ\nsctest(modelo, type = \"CUSUM\")\n\n\n    M-fluctuation test\n\ndata:  modelo\nf(efp) = 1.6798, p-value = 0.02803\n\n#graficamente\nplot(efp(modelo, data = wage2, type = \"Rec-CUSUM\"))\n\n\n\n\n\n\n\n\nA hipótese nula do teste CUSUM é que os coeficientes são estáveis ao longo do tempo. H0 é rejeitada para 5% e 10%. A forma de apresentação mais comum é o gráfico onde é possível visualizar a estabilidade dos coeficientes ao longo da amostra, as linhas vermelhas representam os limites de significância de 5% e a preta representa a estatística do teste. Se a linha preta ultrapassar as linhas vermelhas, rejeitamos a hipótese nula de estabilidade."
  },
  {
    "objectID": "capitulo-05-avaliacao-modelos.html#testes-gerais",
    "href": "capitulo-05-avaliacao-modelos.html#testes-gerais",
    "title": "Avaliação dos Modelos de Regressão",
    "section": "",
    "text": "A biblioteca performance tem a função check_model() que realiza uma série de testes visuais para diagnosticar um modelo de regressão. Vamos utilizar o exemplo anterior, para dados seccionais, e executar a função:\n\n#carregar bibliotecas\nlibrary(wooldridge)\nlibrary(performance)\n\n#carregar dados\ndata(\"wage2\")\n\n#estimar modelo\nmodelo &lt;- lm(wage ~ educ + exper + tenure, data = wage2)\n\n#\ncheck_model(modelo, check = \"all\")\n\n\n\n\n\n\n\n\nO resultado da função são 6 gráficos:\n\n“Posterior Predictive Check”: Verifica a distribuição dos valores previstos em relação aos valores observados.\n“Linearity”: Verifica a linearidade da relação entre as variáveis independentes e a variável dependente.\n“Homogeneity of Variance”: Verifica se a variância dos resíduos é constante (homocedasticidade).\n“Influential Observations”: Identifica observações que têm um impacto desproporcional na estimativa dos coeficientes do modelo.\n“Colinearity”: Verifica se há colinearidade entre as variáveis independentes.\n“Normality of Residuals”: Verifica se os resíduos seguem uma distribuição normal.\n\nA descrição completa de cada um deles pode ser verificada com ?performance::check_model.\nO título de cada gráfico é autoexplicativo. No subtítulo de cada um deles está a informação adicional sobre o que deveríamos verificar para que o modelo seja adequada. Esta é uma ferramenta muito útil para ter uma visão geral do modelo e diagnosticar potenciais problemas.\nPara este modelo podemos concluir que:\n\nO modelo captura bem a distribuição dos dados\nA relação entre as variáveis independentes e a variável dependente aparenta ser linear, mas pode ser melhorada.\nPoderá existir heterocedasticidade (é necessário fazer os testes)\nNão existem observações influentes (o modelo está dentro das linhas de distância de Cook’s)\nNão há indícios de colinearidade entre as variáveis independentes.\nOs resíduos não parecem seguir uma distribuição normal ( as observação deveriam estar em cima da linha verde)\n\nAgora um exemplo para séries temporais:\n\n#carregar bibliotecas\nlibrary(readxl) \nlibrary(performance)\n\n#carregar dados\nm_reg &lt;- read_excel(\"m_reg.xlsx\")\n\n#estimar modelo\nmodelo_st &lt;- lm(gdp ~ ind + agri + ser + man, data = m_reg)\n\n#\ncheck_model(modelo_st)\n\n\n\n\n\n\n\n\n\nO modelo captura bem a distribuição dos dados.\nA relação entre as variáveis independentes e a variável dependente não aparenta ser linear, e por isso deve ser melhorada (exemplo: introduzir termos quadráticos ou interações).\nAparentemente existe heterocedasticidade.\nNão existem observações influentes (o modelo está dentro das linhas de distância de Cook’s).\nExistem indícios de colinearidade entre as variáveis ind, agri e ser.\nOs resíduos estão muito próximos de seguir uma distribuição normal."
  },
  {
    "objectID": "capitulo-05-avaliacao-modelos.html#sec-selecao-modelos",
    "href": "capitulo-05-avaliacao-modelos.html#sec-selecao-modelos",
    "title": "Avaliação dos Modelos de Regressão",
    "section": "",
    "text": "Os modelos devem ser escolhidos com base na capacidade de previsão e na parcimónia (simplicidade). O primeiro passo é definir um conjunto de modelos candidatos, quer seja pela número de variáveis independentes, quer seja pela sua estrutura funcional.\nPara este exemplo vamos utilizar a base de dados hprice3.\n\n#carregar bibliotecas\nlibrary(wooldridge)\n\n#carregar dados\ndata(\"hprice3\")\n\nA comparação de modelos é essencial para garantir que escolhemos o modelo mais adequado para os nossos dados. Quando a variável dependente é a mesma, devemos ter em consideração, entre modelos, as trocas de sinais dos coeficientes, a significância estatística e o coeficiente de determinação ajustado.\nVamos começar por estimar alguns modelos para explicar o preço das casas. E vamos compará-los com a função stargazer e com a função tbl_regression.\n\n# Estimar modelos\nmodelo1 &lt;- lm(price ~ area + rooms + age, \n              data = hprice3)\nmodelo2 &lt;- lm(price ~ area + inst + rooms + age, \n              data = hprice3)\nmodelo3 &lt;- lm(price ~ area + inst + rooms + age + I(inst^2), \n              data = hprice3)\nmodelo4 &lt;- lm(price ~ area + inst + rooms + age + \n              I(rooms^2) + I(age^2), \n              data = hprice3)\n\n#comparar com stargazer\nstargazer(modelo1, modelo2, modelo3, modelo4, \n          type = \"text\")\n\n\n====================================================================================================================\n                                                          Dependent variable:                                       \n                    ------------------------------------------------------------------------------------------------\n                                                                 price                                              \n                              (1)                      (2)                     (3)                     (4)          \n--------------------------------------------------------------------------------------------------------------------\narea                       35.091***                35.187***               33.078***               32.757***       \n                            (2.865)                  (2.863)                 (2.891)                 (2.895)        \n                                                                                                                    \ninst                                                 -0.273                 2.545***                -0.586***       \n                                                     (0.212)                 (0.882)                 (0.224)        \n                                                                                                                    \nrooms                     6,062.454***            6,795.345***             5,565.108**              2,246.530       \n                          (2,209.610)              (2,279.613)             (2,276.052)            (16,337.200)      \n                                                                                                                    \nage                       -397.964***              -426.322***             -387.241***            -1,053.441***     \n                            (51.747)                (56.195)                (56.606)                (168.996)       \n                                                                                                                    \nI(inst2)                                                                   -0.0001***                               \n                                                                            (0.00002)                               \n                                                                                                                    \nI(rooms2)                                                                                            114.788        \n                                                                                                   (1,204.167)      \n                                                                                                                    \nI(age2)                                                                                             4.329***        \n                                                                                                     (1.106)        \n                                                                                                                    \nConstant                  -10,585.510              -10,612.260             -16,647.810             29,842.910       \n                          (12,623.150)            (12,610.130)            (12,554.040)            (54,548.440)      \n                                                                                                                    \n--------------------------------------------------------------------------------------------------------------------\nObservations                  321                      321                     321                     321          \nR2                           0.520                    0.522                   0.538                   0.545         \nAdjusted R2                  0.515                    0.516                   0.531                   0.536         \nResidual Std. Error  30,098.970 (df = 317)    30,067.870 (df = 316)   29,612.130 (df = 315)   29,448.930 (df = 314) \nF Statistic         114.307*** (df = 3; 317) 86.321*** (df = 4; 316) 73.359*** (df = 5; 315) 62.562*** (df = 6; 314)\n====================================================================================================================\nNote:                                                                                    *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n#comparar com gtsummary\nlibrary(gtsummary)\n\ntbl1 &lt;- tbl_regression(modelo1) |&gt; #converter\n        add_glance_source_note() #adiciona metricas\ntbl2 &lt;- tbl_regression(modelo2) |&gt; \n        add_glance_source_note()\ntbl3 &lt;- tbl_regression(modelo3) |&gt; \n        add_glance_source_note()\ntbl4 &lt;- tbl_regression(modelo4) |&gt; \n        add_glance_source_note()\n\ntbl_merge(tbls=list(tbl1, tbl2, tbl3, tbl4),  #mostar modelos\n          tab_spanner = c(\"Modelo 1\", \"Modelo 2\", \n                        \"Modelo 3\", \"Modelo 4\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n\nModelo 1\n\n\nModelo 2\n\n\nModelo 3\n\n\nModelo 4\n\n\n\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\nBeta\n95% CI\np-value\n\n\n\n\narea\n35\n29, 41\n&lt;0.001\n35\n30, 41\n&lt;0.001\n33\n27, 39\n&lt;0.001\n33\n27, 38\n&lt;0.001\n\n\nrooms\n6,062\n1,715, 10,410\n0.006\n6,795\n2,310, 11,280\n0.003\n5,565\n1,087, 10,043\n0.015\n2,247\n-29,898, 34,391\n0.9\n\n\nage\n-398\n-500, -296\n&lt;0.001\n-426\n-537, -316\n&lt;0.001\n-387\n-499, -276\n&lt;0.001\n-1,053\n-1,386, -721\n&lt;0.001\n\n\ninst\n\n\n\n\n\n\n-0.27\n-0.69, 0.14\n0.2\n2.5\n0.81, 4.3\n0.004\n-0.59\n-1.0, -0.14\n0.009\n\n\nI(inst^2)\n\n\n\n\n\n\n\n\n\n\n\n\n0.00\n0.00, 0.00\n0.001\n\n\n\n\n\n\n\n\nI(rooms^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n115\n-2,254, 2,484\n&gt;0.9\n\n\nI(age^2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3\n2.2, 6.5\n&lt;0.001\n\n\n\nAbbreviation: CI = Confidence Interval\n\n\nR² = 0.545; Adjusted R² = 0.536; Sigma = 29,449; Statistic = 62.6; p-value = &lt;0.001; df = 6; Log-likelihood = -3,755; AIC = 7,526; BIC = 7,556; Deviance = 272,313,154,349; Residual df = 314; No. Obs. = 321\n\n\nR² = 0.538; Adjusted R² = 0.531; Sigma = 29,612; Statistic = 73.4; p-value = &lt;0.001; df = 5; Log-likelihood = -3,757; AIC = 7,529; BIC = 7,555; Deviance = 276,216,655,491; Residual df = 315; No. Obs. = 321\n\n\nR² = 0.522; Adjusted R² = 0.516; Sigma = 30,068; Statistic = 86.3; p-value = &lt;0.001; df = 4; Log-likelihood = -3,763; AIC = 7,538; BIC = 7,560; Deviance = 285,688,284,750; Residual df = 316; No. Obs. = 321\n\n\nR² = 0.520; Adjusted R² = 0.515; Sigma = 30,099; Statistic = 114; p-value = &lt;0.001; df = 3; Log-likelihood = -3,764; AIC = 7,537; BIC = 7,556; Deviance = 287,185,487,877; Residual df = 317; No. Obs. = 321\n\n\n\n\n\n\n\n\nNuma primeira análise, podemos observar que que o coeficinete da area é consistente entre os modelos. Os coeficientes das variáveis rooms e age também sao bastante consistentes en termos de sinais. A introdução do termo quadrático de inst no modelo 3 faz com que o coeficiente de inst passe a ser estatisticamento diferente de zero. Este deve ser o primeiro passo para perceber a robustez dos coeficientes. Neste exemplo não houve uma razão específica para a escolha dos modelos, mas em situações reais, a escolha deve ser feita com base em teoria económica e conhecimento do domínio/tópico. Existem alguns critérios que podem ser utilizados para comparar modelos:\n\n\nNeste método, todas as variáveis independentes disponíveis são incluídas no modelo. Este método é simples, mas pode levar a problemas de multicolinearidade e sobreajuste (overfitting). Este método exige prior knowledge. Um exemplo deste método é prever o risco de incumprimento de um empréstimo, onde todas as variáveis disponíveis sobre o cliente são incluídas no modelo, até para comprar o risco entre os vários clientes.\nPara exte exemplo vamos utilizar mais uma vez os dados hprice3 e estimar um modelo com todas as variáveis disponíveis.\n\nlibrary(wooldridge)\ninvisible(library(tidyverse))\ndata(\"hprice3\")\n\n#excluir variáveis transformadas\nhprice3 &lt;- hprice3 |&gt; \n            select(-c(year,agesq, linst, ldist, \n            lprice, larea, lland, linstsq))\n# Estimar modelo \"all in\"\nmodelo_all_in &lt;- lm(price ~ ., data = hprice3)\nsummary(modelo_all_in)\n\n\nCall:\nlm(formula = price ~ ., data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-94658 -13947  -1089  11687 135173 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.339e+04  1.184e+04  -1.976  0.04900 *  \nage         -2.308e+02  4.940e+01  -4.672 4.46e-06 ***\nnbh         -2.024e+03  6.532e+02  -3.099  0.00212 ** \ncbd         -7.693e-01  1.821e+00  -0.423  0.67290    \ninst        -2.684e-02  1.281e+00  -0.021  0.98329    \nrooms        4.201e+03  1.942e+03   2.164  0.03127 *  \narea         2.174e+01  2.769e+00   7.851 6.80e-14 ***\nland         1.224e-01  3.770e-02   3.247  0.00129 ** \nbaths        1.297e+04  2.858e+03   4.540 8.07e-06 ***\ndist         7.814e-01  7.628e-01   1.024  0.30645    \ny81          3.589e+04  2.771e+03  12.953  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23430 on 310 degrees of freedom\nMultiple R-squared:  0.7152,    Adjusted R-squared:  0.7061 \nF-statistic: 77.87 on 10 and 310 DF,  p-value: &lt; 2.2e-16\n\n\nO modelo “all in” pode ser útil como um ponto de partida, mas é importante considerar a parcimónia e a interpretabilidade do modelo. Nem sempre o modelo com mais variáveis será o melhor. Este método é o ponto de partida para o próximo método de seleção.\n\n\n\nA eliminação é um método de seleção de modelos que começa com um modelo completo e remove iterativamente as variáveis com menos informação. Para isso vamos recorrer à função step() da biblioteca MASS. Primeiro vamos estimar o modelo completo com todas as variáveis e depois aplicar o método de eliminação backward. Para a função stepAIC() vamos utilizar o argumento object que é o modelo completo, e o argumento direction que indica a direção da seleção, neste caso backward.\n\n# Estimar modelo completo\nmodelo_completo &lt;- lm(price ~ ., data = hprice3)\n# Aplicar backward elimination\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:gtsummary':\n\n    select\n\n\nThe following object is masked from 'package:wooldridge':\n\n    cement\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nmodelo_backward &lt;- stepAIC(modelo_completo, \n                        direction = \"backward\")\n\nStart:  AIC=6470.58\nprice ~ age + nbh + cbd + inst + rooms + area + land + baths + \n    dist + y81\n\n        Df  Sum of Sq        RSS    AIC\n- inst   1 2.4123e+05 1.7024e+11 6468.6\n- cbd    1 9.8066e+07 1.7034e+11 6468.8\n- dist   1 5.7629e+08 1.7082e+11 6469.7\n&lt;none&gt;                1.7024e+11 6470.6\n- rooms  1 2.5705e+09 1.7281e+11 6473.4\n- nbh    1 5.2734e+09 1.7552e+11 6478.4\n- land   1 5.7894e+09 1.7603e+11 6479.3\n- baths  1 1.1318e+10 1.8156e+11 6489.2\n- age    1 1.1985e+10 1.8223e+11 6490.4\n- area   1 3.3847e+10 2.0409e+11 6526.8\n- y81    1 9.2133e+10 2.6237e+11 6607.4\n\nStep:  AIC=6468.58\nprice ~ age + nbh + cbd + rooms + area + land + baths + dist + \n    y81\n\n        Df  Sum of Sq        RSS    AIC\n- dist   1 1.0629e+09 1.7130e+11 6468.6\n&lt;none&gt;                1.7024e+11 6468.6\n- cbd    1 1.0684e+09 1.7131e+11 6468.6\n- rooms  1 2.5824e+09 1.7282e+11 6471.4\n- nbh    1 5.4832e+09 1.7573e+11 6476.8\n- land   1 5.8105e+09 1.7605e+11 6477.4\n- baths  1 1.1320e+10 1.8156e+11 6487.2\n- age    1 1.1989e+10 1.8223e+11 6488.4\n- area   1 3.3887e+10 2.0413e+11 6524.9\n- y81    1 9.2372e+10 2.6261e+11 6605.7\n\nStep:  AIC=6468.58\nprice ~ age + nbh + cbd + rooms + area + land + baths + y81\n\n        Df  Sum of Sq        RSS    AIC\n- cbd    1 3.2900e+07 1.7134e+11 6466.6\n&lt;none&gt;                1.7130e+11 6468.6\n- rooms  1 2.4764e+09 1.7378e+11 6471.2\n- land   1 5.0440e+09 1.7635e+11 6475.9\n- nbh    1 5.8906e+09 1.7720e+11 6477.4\n- age    1 1.1211e+10 1.8252e+11 6486.9\n- baths  1 1.1829e+10 1.8313e+11 6488.0\n- area   1 3.2899e+10 2.0420e+11 6523.0\n- y81    1 9.3967e+10 2.6527e+11 6607.0\n\nStep:  AIC=6466.64\nprice ~ age + nbh + rooms + area + land + baths + y81\n\n        Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                1.7134e+11 6466.6\n- rooms  1 2.4575e+09 1.7380e+11 6469.2\n- land   1 5.4354e+09 1.7677e+11 6474.7\n- nbh    1 6.6906e+09 1.7803e+11 6476.9\n- baths  1 1.1859e+10 1.8320e+11 6486.1\n- age    1 1.2175e+10 1.8351e+11 6486.7\n- area   1 3.3214e+10 2.0455e+11 6521.5\n- y81    1 9.4601e+10 2.6594e+11 6605.8\n\nsummary(modelo_backward)\n\n\nCall:\nlm(formula = price ~ age + nbh + rooms + area + land + baths + \n    y81, data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-98481 -13086  -1139  11638 139184 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.766e+04  1.019e+04  -1.733  0.08405 .  \nage         -2.168e+02  4.597e+01  -4.716 3.63e-06 ***\nnbh         -2.139e+03  6.120e+02  -3.496  0.00054 ***\nrooms        4.025e+03  1.899e+03   2.119  0.03489 *  \narea         2.128e+01  2.732e+00   7.789 9.98e-14 ***\nland         1.084e-01  3.439e-02   3.151  0.00178 ** \nbaths        1.314e+04  2.824e+03   4.654 4.80e-06 ***\ny81          3.618e+04  2.752e+03  13.146  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23400 on 313 degrees of freedom\nMultiple R-squared:  0.7134,    Adjusted R-squared:  0.707 \nF-statistic: 111.3 on 7 and 313 DF,  p-value: &lt; 2.2e-16\n\n\nO modelo resultante d é mais simples e contém apenas as variáveis informação necessária. Este método é útil para reduzir a complexidade do modelo. Para este método, o critério de seleção padrão é o AIC (Critério de Informação de Akaike). A cada iteração, a variável que, quando removida, resulta na maior redução do AIC é eliminada do modelo. O processo continua até que a remoção de qualquer variável adicional não melhore o AIC.\n\n\n\nA seleção forward é o oposto da eliminação backward. Começa com um modelo só com constante e é adiconada a variável com o valor P mais baixo do all in. É adicioanda uma variável de cada vez, em que variável só permanece no modelo se tiver informação suficiente. As variáveis são adicionadas até que nenhuma variável adicional melhore significativamente o modelo. Para este método vamos utilizar o argumento object que é o modelo nulo, o argumento direction que indica a direção da seleção, neste caso forward, e o argumento scope que é o modelo completo. O scope define o limite superior do modelo que pode ser construído.\n\n# Estimar modelo nulo\nmodelo_nulo &lt;- lm(price ~ 1, data = hprice3)\n# Aplicar forward selection\nlibrary(MASS)\nmodelo_forward &lt;- stepAIC(modelo_nulo, \n                      direction = \"forward\", \n                      scope = formula(modelo_completo))\n\nStart:  AIC=6853.8\nprice ~ 1\n\n        Df  Sum of Sq        RSS    AIC\n+ area   1 2.4898e+11 3.4887e+11 6682.9\n+ baths  1 2.3424e+11 3.6362e+11 6696.2\n+ y81    1 1.5343e+11 4.4442e+11 6760.6\n+ rooms  1 1.1739e+11 4.8046e+11 6785.6\n+ age    1 6.5871e+10 5.3198e+11 6818.3\n+ cbd    1 2.9096e+10 5.6876e+11 6839.8\n+ inst   1 2.8000e+10 5.6985e+11 6840.4\n+ nbh    1 2.7872e+10 5.6998e+11 6840.5\n+ land   1 2.5512e+10 5.7234e+11 6841.8\n+ dist   1 2.3611e+10 5.7424e+11 6842.9\n&lt;none&gt;                5.9785e+11 6853.8\n\nStep:  AIC=6682.89\nprice ~ area\n\n        Df  Sum of Sq        RSS    AIC\n+ y81    1 9.6055e+10 2.5281e+11 6581.5\n+ age    1 5.4864e+10 2.9401e+11 6630.0\n+ baths  1 4.1886e+10 3.0698e+11 6643.8\n+ nbh    1 1.8465e+10 3.3040e+11 6667.4\n+ dist   1 8.1622e+09 3.4071e+11 6677.3\n+ rooms  1 8.1015e+09 3.4077e+11 6677.4\n+ cbd    1 7.2749e+09 3.4159e+11 6678.1\n+ land   1 5.8607e+09 3.4301e+11 6679.5\n+ inst   1 5.6794e+09 3.4319e+11 6679.6\n&lt;none&gt;                3.4887e+11 6682.9\n\nStep:  AIC=6581.52\nprice ~ area + y81\n\n        Df  Sum of Sq        RSS    AIC\n+ baths  1 5.4806e+10 1.9801e+11 6505.1\n+ age    1 4.1224e+10 2.1159e+11 6526.4\n+ rooms  1 1.5120e+10 2.3769e+11 6563.7\n+ cbd    1 1.4338e+10 2.3848e+11 6564.8\n+ dist   1 1.3921e+10 2.3889e+11 6565.3\n+ land   1 1.3706e+10 2.3911e+11 6565.6\n+ inst   1 1.2985e+10 2.3983e+11 6566.6\n+ nbh    1 1.1678e+10 2.4114e+11 6568.3\n&lt;none&gt;                2.5281e+11 6581.5\n\nStep:  AIC=6505.08\nprice ~ area + y81 + baths\n\n        Df  Sum of Sq        RSS    AIC\n+ age    1 1.1929e+10 1.8608e+11 6487.1\n+ nbh    1 6.8226e+09 1.9119e+11 6495.8\n+ land   1 6.4476e+09 1.9156e+11 6496.5\n+ dist   1 1.5748e+09 1.9643e+11 6504.5\n+ cbd    1 1.5257e+09 1.9648e+11 6504.6\n+ inst   1 1.2522e+09 1.9676e+11 6505.0\n&lt;none&gt;                1.9801e+11 6505.1\n+ rooms  1 1.2041e+09 1.9680e+11 6505.1\n\nStep:  AIC=6487.14\nprice ~ area + y81 + baths + age\n\n        Df  Sum of Sq        RSS    AIC\n+ nbh    1 6558044922 1.7952e+11 6477.6\n+ land   1 5536264112 1.8054e+11 6479.4\n+ rooms  1 2800423408 1.8328e+11 6484.3\n&lt;none&gt;                1.8608e+11 6487.1\n+ dist   1  180122584 1.8590e+11 6488.8\n+ cbd    1   54725421 1.8602e+11 6489.0\n+ inst   1    8256988 1.8607e+11 6489.1\n\nStep:  AIC=6477.62\nprice ~ area + y81 + baths + age + nbh\n\n        Df  Sum of Sq        RSS    AIC\n+ land   1 5724921758 1.7380e+11 6469.2\n+ rooms  1 2747028352 1.7677e+11 6474.7\n+ dist   1 1135961225 1.7838e+11 6477.6\n&lt;none&gt;                1.7952e+11 6477.6\n+ cbd    1  911989874 1.7861e+11 6478.0\n+ inst   1  778563582 1.7874e+11 6478.2\n\nStep:  AIC=6469.21\nprice ~ area + y81 + baths + age + nbh + land\n\n        Df  Sum of Sq        RSS    AIC\n+ rooms  1 2457508573 1.7134e+11 6466.6\n&lt;none&gt;                1.7380e+11 6469.2\n+ dist   1  182635168 1.7361e+11 6470.9\n+ cbd    1   14054756 1.7378e+11 6471.2\n+ inst   1     759751 1.7379e+11 6471.2\n\nStep:  AIC=6466.64\nprice ~ area + y81 + baths + age + nbh + land + rooms\n\n       Df Sum of Sq        RSS    AIC\n&lt;none&gt;              1.7134e+11 6466.6\n+ inst  1 100552078 1.7124e+11 6468.5\n+ cbd   1  32900269 1.7130e+11 6468.6\n+ dist  1  27334059 1.7131e+11 6468.6\n\nsummary(modelo_forward)\n\n\nCall:\nlm(formula = price ~ area + y81 + baths + age + nbh + land + \n    rooms, data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-98481 -13086  -1139  11638 139184 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.766e+04  1.019e+04  -1.733  0.08405 .  \narea         2.128e+01  2.732e+00   7.789 9.98e-14 ***\ny81          3.618e+04  2.752e+03  13.146  &lt; 2e-16 ***\nbaths        1.314e+04  2.824e+03   4.654 4.80e-06 ***\nage         -2.168e+02  4.597e+01  -4.716 3.63e-06 ***\nnbh         -2.139e+03  6.120e+02  -3.496  0.00054 ***\nland         1.084e-01  3.439e-02   3.151  0.00178 ** \nrooms        4.025e+03  1.899e+03   2.119  0.03489 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23400 on 313 degrees of freedom\nMultiple R-squared:  0.7134,    Adjusted R-squared:  0.707 \nF-statistic: 111.3 on 7 and 313 DF,  p-value: &lt; 2.2e-16\n\n\nNeste modelo, existem duas variáveis que não são estatisticamente significativas, mas que foram incluídas no modelo porque melhoram o AIC. O facto de uma variável não ser estatisticamente significativa não significa que ela não tenha informação para explicar a dependente. A decisão de incluir ou excluir uma variável deve ser baseada em uma combinação de critérios estatísticos e conhecimento tórico.\n\n\n\nA seleção stepwise combina os métodos forward e backward. Começa com um modelo nulo e adiciona variáveis como no método forward, mas também verifica se alguma variável já incluída no modelo pode ser removida, como no método backward. Este processo continua até que nenhuma variável adicional possa ser adicionada ou removida. Or argumento object é o modelo nulo, o argumento direction que indica a direção da seleção, neste caso both, e o argumento scope que é o modelo completo.\n\n# Estimar modelo nulo\nmodelo_nulo &lt;- lm(price ~ 1, data = hprice3)\n# Aplicar stepwise selection\nlibrary(MASS)\nmodelo_stepwise &lt;- stepAIC(modelo_nulo, direction = \"both\", scope = formula(modelo_completo))\n\nStart:  AIC=6853.8\nprice ~ 1\n\n        Df  Sum of Sq        RSS    AIC\n+ area   1 2.4898e+11 3.4887e+11 6682.9\n+ baths  1 2.3424e+11 3.6362e+11 6696.2\n+ y81    1 1.5343e+11 4.4442e+11 6760.6\n+ rooms  1 1.1739e+11 4.8046e+11 6785.6\n+ age    1 6.5871e+10 5.3198e+11 6818.3\n+ cbd    1 2.9096e+10 5.6876e+11 6839.8\n+ inst   1 2.8000e+10 5.6985e+11 6840.4\n+ nbh    1 2.7872e+10 5.6998e+11 6840.5\n+ land   1 2.5512e+10 5.7234e+11 6841.8\n+ dist   1 2.3611e+10 5.7424e+11 6842.9\n&lt;none&gt;                5.9785e+11 6853.8\n\nStep:  AIC=6682.89\nprice ~ area\n\n        Df  Sum of Sq        RSS    AIC\n+ y81    1 9.6055e+10 2.5281e+11 6581.5\n+ age    1 5.4864e+10 2.9401e+11 6630.0\n+ baths  1 4.1886e+10 3.0698e+11 6643.8\n+ nbh    1 1.8465e+10 3.3040e+11 6667.4\n+ dist   1 8.1622e+09 3.4071e+11 6677.3\n+ rooms  1 8.1015e+09 3.4077e+11 6677.4\n+ cbd    1 7.2749e+09 3.4159e+11 6678.1\n+ land   1 5.8607e+09 3.4301e+11 6679.5\n+ inst   1 5.6794e+09 3.4319e+11 6679.6\n&lt;none&gt;                3.4887e+11 6682.9\n- area   1 2.4898e+11 5.9785e+11 6853.8\n\nStep:  AIC=6581.52\nprice ~ area + y81\n\n        Df  Sum of Sq        RSS    AIC\n+ baths  1 5.4806e+10 1.9801e+11 6505.1\n+ age    1 4.1224e+10 2.1159e+11 6526.4\n+ rooms  1 1.5120e+10 2.3769e+11 6563.7\n+ cbd    1 1.4338e+10 2.3848e+11 6564.8\n+ dist   1 1.3921e+10 2.3889e+11 6565.3\n+ land   1 1.3706e+10 2.3911e+11 6565.6\n+ inst   1 1.2985e+10 2.3983e+11 6566.6\n+ nbh    1 1.1678e+10 2.4114e+11 6568.3\n&lt;none&gt;                2.5281e+11 6581.5\n- y81    1 9.6055e+10 3.4887e+11 6682.9\n- area   1 1.9161e+11 4.4442e+11 6760.6\n\nStep:  AIC=6505.08\nprice ~ area + y81 + baths\n\n        Df  Sum of Sq        RSS    AIC\n+ age    1 1.1929e+10 1.8608e+11 6487.1\n+ nbh    1 6.8226e+09 1.9119e+11 6495.8\n+ land   1 6.4476e+09 1.9156e+11 6496.5\n+ dist   1 1.5748e+09 1.9643e+11 6504.5\n+ cbd    1 1.5257e+09 1.9648e+11 6504.6\n+ inst   1 1.2522e+09 1.9676e+11 6505.0\n&lt;none&gt;                1.9801e+11 6505.1\n+ rooms  1 1.2041e+09 1.9680e+11 6505.1\n- area   1 2.9226e+10 2.2723e+11 6547.3\n- baths  1 5.4806e+10 2.5281e+11 6581.5\n- y81    1 1.0898e+11 3.0698e+11 6643.8\n\nStep:  AIC=6487.14\nprice ~ area + y81 + baths + age\n\n        Df  Sum of Sq        RSS    AIC\n+ nbh    1 6.5580e+09 1.7952e+11 6477.6\n+ land   1 5.5363e+09 1.8054e+11 6479.4\n+ rooms  1 2.8004e+09 1.8328e+11 6484.3\n&lt;none&gt;                1.8608e+11 6487.1\n+ dist   1 1.8012e+08 1.8590e+11 6488.8\n+ cbd    1 5.4725e+07 1.8602e+11 6489.0\n+ inst   1 8.2570e+06 1.8607e+11 6489.1\n- age    1 1.1929e+10 1.9801e+11 6505.1\n- baths  1 2.5512e+10 2.1159e+11 6526.4\n- area   1 3.8341e+10 2.2442e+11 6545.3\n- y81    1 9.4988e+10 2.8107e+11 6617.5\n\nStep:  AIC=6477.62\nprice ~ area + y81 + baths + age + nbh\n\n        Df  Sum of Sq        RSS    AIC\n+ land   1 5.7249e+09 1.7380e+11 6469.2\n+ rooms  1 2.7470e+09 1.7677e+11 6474.7\n+ dist   1 1.1360e+09 1.7838e+11 6477.6\n&lt;none&gt;                1.7952e+11 6477.6\n+ cbd    1 9.1199e+08 1.7861e+11 6478.0\n+ inst   1 7.7856e+08 1.7874e+11 6478.2\n- nbh    1 6.5580e+09 1.8608e+11 6487.1\n- age    1 1.1665e+10 1.9119e+11 6495.8\n- baths  1 2.2965e+10 2.0249e+11 6514.3\n- area   1 3.9358e+10 2.1888e+11 6539.2\n- y81    1 8.9281e+10 2.6880e+11 6605.2\n\nStep:  AIC=6469.21\nprice ~ area + y81 + baths + age + nbh + land\n\n        Df  Sum of Sq        RSS    AIC\n+ rooms  1 2.4575e+09 1.7134e+11 6466.6\n&lt;none&gt;                1.7380e+11 6469.2\n+ dist   1 1.8264e+08 1.7361e+11 6470.9\n+ cbd    1 1.4055e+07 1.7378e+11 6471.2\n+ inst   1 7.5975e+05 1.7379e+11 6471.2\n- land   1 5.7249e+09 1.7952e+11 6477.6\n- nbh    1 6.7467e+09 1.8054e+11 6479.4\n- age    1 1.0746e+10 1.8454e+11 6486.5\n- baths  1 1.9943e+10 1.9374e+11 6502.1\n- area   1 3.7746e+10 2.1154e+11 6530.3\n- y81    1 9.3450e+10 2.6725e+11 6605.3\n\nStep:  AIC=6466.64\nprice ~ area + y81 + baths + age + nbh + land + rooms\n\n        Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                1.7134e+11 6466.6\n+ inst   1 1.0055e+08 1.7124e+11 6468.5\n+ cbd    1 3.2900e+07 1.7130e+11 6468.6\n+ dist   1 2.7334e+07 1.7131e+11 6468.6\n- rooms  1 2.4575e+09 1.7380e+11 6469.2\n- land   1 5.4354e+09 1.7677e+11 6474.7\n- nbh    1 6.6906e+09 1.7803e+11 6476.9\n- baths  1 1.1859e+10 1.8320e+11 6486.1\n- age    1 1.2175e+10 1.8351e+11 6486.7\n- area   1 3.3214e+10 2.0455e+11 6521.5\n- y81    1 9.4601e+10 2.6594e+11 6605.8\n\nsummary(modelo_stepwise)\n\n\nCall:\nlm(formula = price ~ area + y81 + baths + age + nbh + land + \n    rooms, data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-98481 -13086  -1139  11638 139184 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.766e+04  1.019e+04  -1.733  0.08405 .  \narea         2.128e+01  2.732e+00   7.789 9.98e-14 ***\ny81          3.618e+04  2.752e+03  13.146  &lt; 2e-16 ***\nbaths        1.314e+04  2.824e+03   4.654 4.80e-06 ***\nage         -2.168e+02  4.597e+01  -4.716 3.63e-06 ***\nnbh         -2.139e+03  6.120e+02  -3.496  0.00054 ***\nland         1.084e-01  3.439e-02   3.151  0.00178 ** \nrooms        4.025e+03  1.899e+03   2.119  0.03489 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23400 on 313 degrees of freedom\nMultiple R-squared:  0.7134,    Adjusted R-squared:  0.707 \nF-statistic: 111.3 on 7 and 313 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nOs métodos de seleção de modelos podem ser comparados utilizando critérios de informação, como o AIC (Critério de Informação de Akaike) e o BIC (Critério de Informação Bayesiano). Estes critérios penalizam a complexidade do modelo, são muito úteis quando existem muitas variáveis disponíveis.\nO AIC é mais flexível e tende a selecionar modelos mais complexos, enquanto o BIC é mais conservador e favorece modelos mais simples. O AIC é calculado:\n\\[\nAIC = \\ln\\left(\\frac{SSR}{n}\\right) + \\frac{2k}{n}\n\\tag{24}\\]\nonde: - \\(SSR\\) é a soma dos quadrados dos resíduos do modelo, - \\(n\\) é o número de observações, - \\(k\\) é o número de parâmetros do modelo.\nOu seja, o AIC penaliza o número de parâmetros no modelo (quanto maior o número de parâmetros, maior a penalização). O objetivo é minimizar o AIC, ou seja, escolher o modelo com o menor AIC.\nO BIC, também conhecido como Critério de Informação de Schwarz (SIC), é calculado como:\n\\[\nBIC = \\ln\\left(\\frac{SSR}{n}\\right) + \\frac{k \\ln(n)}{n}\n\\tag{25}\\]\nem que: - \\(SSR\\) é a soma dos quadrados dos resíduos do modelo, - \\(n\\) é o número de observações, - \\(k\\) é o número de parâmetros do modelo.\nOu seja, o BIC penaliza o número de parâmetros no modelo, mas a penalização é mais acentuada do que no AIC, sobretudo em grandes amostras. O objetivo é minimizar o BIC, ou seja, escolher o modelo com o menor BIC.\nPara obter o AIC e o BIC de um modelo no R vamos utilizar a função compare_performance() da biblioteca performance. Para um modelo utilizamos a função model_performance(). Vamos estimar 2 modelos para efeitos de comparação:\n\nlibrary(performance)\n\nmodelo1 &lt;- lm(price ~ area + rooms + age, \n              data = hprice3)\nmodelo2 &lt;- lm(price ~ area + inst + rooms + age + I(inst^2), \n              data = hprice3)\n\ncompare_performance(modelo1, modelo2)\n\n# Comparison of Model Performance Indices\n\nName    | Model |  AIC (weights) | AICc (weights) |  BIC (weights) |    R2\n--------------------------------------------------------------------------\nmodelo1 |    lm | 7537.4 (0.014) | 7537.6 (0.015) | 7556.3 (0.383) | 0.520\nmodelo2 |    lm | 7528.9 (0.986) | 7529.3 (0.985) | 7555.3 (0.617) | 0.538\n\nName    | R2 (adj.) |      RMSE |     Sigma\n-------------------------------------------\nmodelo1 |     0.515 | 29910.848 | 30098.969\nmodelo2 |     0.531 | 29334.076 | 29612.130\n\n\nObtemos bárias métricas:\n\nAIC: Critério de Informação de Akaike\nAICc: AIC corrigido para amostras pequenas\nBIC: Critério de Informação Bayesiano\nR2: Coeficiente de Determinação - mede a proporção da variabilidade na variável dependente que é explicada pelo modelo.\nR2 adj: Coeficiente de Determinação Ajustado - ajusta o R² para o número de variáveis independentes no modelo.\nRMSE: Raiz do Erro Quadrático Médio (\\(RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}\\)) - mede a média dos erros de previsão do modelo.\nSigma: Estimativa do desvio padrão dos resíduos - mede a variabilidade dos resíduos do modelo.\n\nO objetivo é minimizar o AIC, BIC, RMSE e o Sigma. Enquanto que o R² e o R² ajustado devem ser maximizados. Neste exemplo, o modelo 2 é melhor em todas as métricas. A escolha entre os modelos deve considerar o trade-off entre a complexidade do modelo e a sua capacidade de prever a variável dependente.\n\n\n\nA validação cruzada é uma técnica utilizada para avaliar a capacidade de previsão de um modelo. O objetivo é dividir os dados em subamostras de treino e de teste, onde o modelo é “treinado” no conjunto de treino e avaliado no conjunto de teste. Existem várias formas de realizar a validação cruzada, mas as mais comuns são a validação cruzada simples (hold-out) e a validação cruzada k-fold.\nA Validação Cruzada Simples (Hold-out) consiste em dividir os dados em duas partes: uma para treino e outra para teste. O modelo é “treinado”/estimado com a parte de treino e avaliado com a parte de teste. Esta abordagem é simples, mas muito sensível à forma de como os dados são divididos. Para dividir os dados recorresmos à biblioteca caTools com as funções sample.split() para definir a proporção, e subset() para criar os conjuntos de treino e teste. Para os dados wage3 vamos utilizar 70% dos dados para treino e 30% para teste.\n\n#dados\nlibrary(wooldridge)\ndata(\"wage2\")\n\n#separar dados treino e teste\nlibrary(caTools)\n\nsplit &lt;- sample.split(wage2$wage, SplitRatio = 0.7)\ndados_treino &lt;- subset(wage2, split == TRUE)\ndados_teste &lt;- subset(wage2, split == FALSE)\n\nEstimar um modelo para com dados de treino e fazemos uma previão com os dados de teste, mas recorremos ao modelo de treino. A métrica mais comum para avaliar a capacidade de previsão é o RMSE (Root Mean Squared Error).\n\n# Estimar modelo nos dados de treino\nmodelo_treino &lt;- lm(wage ~ educ + exper + tenure, data = dados_treino)\n\n# Prever valores nos dados de teste com o modelo de treino\nprevisoes &lt;- predict(modelo_treino, newdata = dados_teste)\n# Calcular RMSE\nrmse &lt;- sqrt(mean((dados_teste$wage - previsoes)^2))\nrmse\n\n[1] 365.1301\n\n\nObtivemos um RMSE de 384.199, o que significa que, em média, as previsões do modelo estão a 384.199 unidades do valor real da variável dependente wage. Este valor pode ser comparado com a média da variável dependente para ter uma ideia da precisão do modelo.\n\nmean_wage&lt;-mean(wage2$wage)\n\nrmse / mean_wage\n\n[1] 0.3811596\n\n\nComo a média de wage é 957.9455, o RMSE representa aproximadamente 39% da média, o que indica que o modelo tem uma capacidade de previsão moderada.\nTambém podemos comparar graficamente o modelo nas duas subamostras:\n\nlibrary(ggplot2)\n\n# Criar data frame combinando dados de treino e teste\ndados_plot &lt;- data.frame(\n  Valores_Reais = c(dados_treino$wage, dados_teste$wage),\n  Previsão = c(predict(modelo_treino, newdata = dados_treino), previsoes),\n  Amostras = c(rep(\"Treino\", nrow(dados_treino)), rep(\"Teste\", nrow(dados_teste)))\n)\n\nggplot(dados_plot, aes(x = Valores_Reais, y = Previsão, color = Amostras)) +\n  geom_point(alpha = 0.7, size = 5.5) +\n  scale_color_manual(values = c(\"Treino\" = \"steelblue\", \"Teste\" = \"coral\")) +\n  geom_abline(intercept = 0, slope = 1, colour = \"black\", linetype = \"dashed\", size = 2) +\n  labs(title = \"Comparação da Previsão: Treino vs Teste\",\n       x = \"Valores Reais (wage)\",\n       y = \"Previsões do Modelo\",\n       color = \"Amostras\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n        axis.title = element_text(size = 12),\n        panel.grid.minor = element_blank(),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nA validação cruzada k-fold é uma técnica mais robusta do que a validação cruzada simples. Consiste em dividir os dados em k subamostras (folds). O modelo é “treinado” em k-1 folds e testado no fold restante. Este processo é repetido k vezes, cada vez com um fold diferente como conjunto de teste. A métrica de avaliação (por exemplo, RMSE) é então calculada como a média das métricas obtidas em cada iteração.\nPara este exemplo vamos utilizar a biblioteca caret que tem uma função específica para realizar a validação cruzada k-fold. Vamos definir o número de folds (k) e utilizar a função trainControl() para configurar a validação cruzada. Depois, utilizamos a função train() para estimar o modelo com validação cruzada.\n\n#carregar bibliotecas\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n# Definir controle de treino com validação cruzada k-fold\ncontrole_treino &lt;- trainControl(method = \"cv\", number = 10) # 10-fold CV\n# Estimar modelo com validação cruzada k-fold\nmodelo_kfold &lt;- train(wage ~ educ + exper + tenure, \n                      data = wage2, \n                      method = \"lm\", \n                      trControl = controle_treino)\n# Resultados do modelo\nprint(modelo_kfold)\n\nLinear Regression \n\n935 samples\n  3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 842, 842, 841, 843, 843, 840, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  373.7798  0.1549275  281.1262\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n# RMSE médio\nmodelo_kfold$results$RMSE\n\n[1] 373.7798\n\n# R² médio\nmodelo_kfold$results$Rsquared\n\n[1] 0.1549275\n\n\nOs valores de RMSE e R² apresentados são a média dos valores obtidos em cada uma das 10 iterações da validação cruzada. O RMSE médio é 372.959, o que indica que, em média, as previsões do modelo estão a 372.959 unidades do valor real da variável dependente wage. O R² médio é 0.1487, o que significa que, em média, o modelo explica aproximadamente 14.87% da variabilidade na variável dependente wage. Provavelmente o modelo pode ser melhorado, seja com a introdução de novas variáveis, seja com a alteração da forma funcional. No próximo capítulo vamos abordar algumas extensões do modeli de regressão linear que podem ajudar a melhorar a capacidade de previsão dos modelos."
  },
  {
    "objectID": "referencias.html",
    "href": "referencias.html",
    "title": "Referências",
    "section": "",
    "text": "Esta secção contém as referências bibliográficas utilizadas ao longo do livro.\n\n\n\n\n\n\n\n\nWooldridge, J. M. (2020). Introductory Econometrics: A Modern Approach. 7ª edição. Cengage Learning.\nStock, J. H., & Watson, M. W. (2019). Introduction to Econometrics. 4ª edição. Pearson.\nGreene, W. H. (2018). Econometric Analysis. 8ª edição. Pearson.\nHill, R. C., Griffiths, W. E., & Lim, G. C. (2018). Principles of Econometrics. 5ª edição. Wiley.\nCameron, A. C., & Trivedi, P. K. (2005). Microeconometrics: Methods and Applications. Cambridge University Press.\n\n\n\n\n\nKleiber, C., & Zeileis, A. (2008). Applied Econometrics with R. Springer.\nFarnsworth, G. V. (2008). Econometrics in R. CRAN Task View.\nHeiss, F. (2016). Using R for Introductory Econometrics. CreateSpace Independent Publishing Platform.\n\n\n\n\n\nR Core Team (2024). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.\nWickham, H., & Grolemund, G. (2017). R for Data Science. O’Reilly Media.\n\n\n\n\n\n\n\n\nbase: R Core Team (2024). R: A Language and Environment for Statistical Computing.\nstats: R Core Team (2024). The R Stats Package.\nutils: R Core Team (2024). The R Utils Package.\n\n\n\n\n\nAER: Kleiber, C., & Zeileis, A. (2019). Applied Econometrics with R. R package version 1.2-9.\nplm: Croissant, Y., & Millo, G. (2008). Panel Data Econometrics in R: The plm Package. Journal of Statistical Software, 27(2), 1-43.\nlmtest: Zeileis, A., & Hothorn, T. (2002). Diagnostic Checking in Regression Relationships. R News, 2(3), 7-10.\nsandwich: Zeileis, A. (2004). Econometric Computing with HC and HAC Covariance Matrix Estimators. Journal of Statistical Software, 11(10), 1-17.\ncar: Fox, J., & Weisberg, S. (2019). An R Companion to Applied Regression. 3ª edição. Sage.\nstargazer: Hlavac, M. (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2.\n\n\n\n\n\nMASS: Venables, W. N., & Ripley, B. D. (2002). Modern Applied Statistics with S. 4ª edição. Springer.\nnnet: Venables, W. N., & Ripley, B. D. (2002). Modern Applied Statistics with S. 4ª edição. Springer.\nglmnet: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22.\nsampleSelection: Toomet, O., & Henningsen, A. (2008). Sample Selection Models in R: Package sampleSelection. Journal of Statistical Software, 27(7), 1-23.\npscl: Jackman, S. (2017). pscl: Classes and Methods for R Developed in the Political Science Computational Laboratory. R package version 1.5.2.\n\n\n\n\n\ndplyr: Wickham, H., François, R., Henry, L., & Müller, K. (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7.\nggplot2: Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York.\nreadxl: Wickham, H., & Bryan, J. (2019). readxl: Read Excel Files. R package version 1.3.1.\nmargins: Leeper, T. J. (2018). margins: An R Package to Interpret Regression Models. R package version 0.3.23.\n\n\n\n\n\n\n\n\nR Project: https://www.r-project.org/\nCRAN Task View - Econometrics: https://cran.r-project.org/web/views/Econometrics.html\nRStudio: https://www.rstudio.com/\n\n\n\n\n\nR for Data Science: https://r4ds.had.co.nz/\nIntroduction to Econometrics with R: https://www.econometrics-with-r.org/\nApplied Econometrics with R (livro online): https://eeecon.uibk.ac.at/~zeileis/teaching/AER/\n\n\n\n\n\nStack Overflow (tag R): https://stackoverflow.com/questions/tagged/r\nR-help mailing list: https://stat.ethz.ch/mailman/listinfo/r-help\nRStudio Community: https://community.rstudio.com/\n\n\n\n\n\n\n\n\nInstituto Nacional de Estatística (INE): https://www.ine.pt/\nBanco de Portugal: https://www.bportugal.pt/\nPORDATA: https://www.pordata.pt/\nEurostat: https://ec.europa.eu/eurostat/\nWorld Bank Open Data: https://data.worldbank.org/\nOECD Data: https://data.oecd.org/\n\n\n\n\nOs dados utilizados nos exemplos deste livro incluem:\n\nDados simulados para fins pedagógicos\nConjuntos de dados públicos disponíveis nos pacotes de R\nDados económicos de acesso livre de organizações internacionais\n\n\n\n\n\n\n\n\nHausman, J. A. (1978). Specification Tests in Econometrics. Econometrica, 46(6), 1251-1271.\nWhite, H. (1980). A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity. Econometrica, 48(4), 817-838.\nBreusch, T. S., & Pagan, A. R. (1979). A Simple Test for Heteroscedasticity and Random Coefficient Variation. Econometrica, 47(5), 1287-1294.\nDurbin, J., & Watson, G. S. (1971). Testing for Serial Correlation in Least Squares Regression. Biometrika, 58(1), 1-19.\nHeckman, J. J. (1979). Sample Selection Bias as a Specification Error. Econometrica, 47(1), 153-161.\n\n\n\n\n\nJarque, C. M., & Bera, A. K. (1980). Efficient Tests for Normality, Homoscedasticity and Serial Independence of Regression Residuals. Economics Letters, 6(3), 255-259.\nShapiro, S. S., & Wilk, M. B. (1965). An Analysis of Variance Test for Normality (Complete Samples). Biometrika, 52(3/4), 591-611.\nChow, G. C. (1960). Tests of Equality Between Sets of Coefficients in Two Linear Regressions. Econometrica, 28(3), 591-605.\n\n\n\n\n\nEste livro beneficiou do contributo de várias fontes e recursos:\n\nComunidade R pela disponibilização de ferramentas open-source\nAutores dos pacotes de R utilizados\nInvestigadores que desenvolveram as metodologias econométricas apresentadas\nEstudantes e colegas que forneceram feedback durante o desenvolvimento\n\n\nNota: Todas as URL foram verificadas em Julho de 2024. Algumas podem ter mudado desde então."
  },
  {
    "objectID": "referencias.html#bibliografia-principal",
    "href": "referencias.html#bibliografia-principal",
    "title": "Referências",
    "section": "",
    "text": "Wooldridge, J. M. (2020). Introductory Econometrics: A Modern Approach. 7ª edição. Cengage Learning.\nStock, J. H., & Watson, M. W. (2019). Introduction to Econometrics. 4ª edição. Pearson.\nGreene, W. H. (2018). Econometric Analysis. 8ª edição. Pearson.\nHill, R. C., Griffiths, W. E., & Lim, G. C. (2018). Principles of Econometrics. 5ª edição. Wiley.\nCameron, A. C., & Trivedi, P. K. (2005). Microeconometrics: Methods and Applications. Cambridge University Press.\n\n\n\n\n\nKleiber, C., & Zeileis, A. (2008). Applied Econometrics with R. Springer.\nFarnsworth, G. V. (2008). Econometrics in R. CRAN Task View.\nHeiss, F. (2016). Using R for Introductory Econometrics. CreateSpace Independent Publishing Platform.\n\n\n\n\n\nR Core Team (2024). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.\nWickham, H., & Grolemund, G. (2017). R for Data Science. O’Reilly Media."
  },
  {
    "objectID": "referencias.html#pacotes-de-r-utilizados",
    "href": "referencias.html#pacotes-de-r-utilizados",
    "title": "Referências",
    "section": "",
    "text": "base: R Core Team (2024). R: A Language and Environment for Statistical Computing.\nstats: R Core Team (2024). The R Stats Package.\nutils: R Core Team (2024). The R Utils Package.\n\n\n\n\n\nAER: Kleiber, C., & Zeileis, A. (2019). Applied Econometrics with R. R package version 1.2-9.\nplm: Croissant, Y., & Millo, G. (2008). Panel Data Econometrics in R: The plm Package. Journal of Statistical Software, 27(2), 1-43.\nlmtest: Zeileis, A., & Hothorn, T. (2002). Diagnostic Checking in Regression Relationships. R News, 2(3), 7-10.\nsandwich: Zeileis, A. (2004). Econometric Computing with HC and HAC Covariance Matrix Estimators. Journal of Statistical Software, 11(10), 1-17.\ncar: Fox, J., & Weisberg, S. (2019). An R Companion to Applied Regression. 3ª edição. Sage.\nstargazer: Hlavac, M. (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.2.\n\n\n\n\n\nMASS: Venables, W. N., & Ripley, B. D. (2002). Modern Applied Statistics with S. 4ª edição. Springer.\nnnet: Venables, W. N., & Ripley, B. D. (2002). Modern Applied Statistics with S. 4ª edição. Springer.\nglmnet: Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization Paths for Generalized Linear Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22.\nsampleSelection: Toomet, O., & Henningsen, A. (2008). Sample Selection Models in R: Package sampleSelection. Journal of Statistical Software, 27(7), 1-23.\npscl: Jackman, S. (2017). pscl: Classes and Methods for R Developed in the Political Science Computational Laboratory. R package version 1.5.2.\n\n\n\n\n\ndplyr: Wickham, H., François, R., Henry, L., & Müller, K. (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.7.\nggplot2: Wickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York.\nreadxl: Wickham, H., & Bryan, J. (2019). readxl: Read Excel Files. R package version 1.3.1.\nmargins: Leeper, T. J. (2018). margins: An R Package to Interpret Regression Models. R package version 0.3.23."
  },
  {
    "objectID": "referencias.html#recursos-online-recomendados",
    "href": "referencias.html#recursos-online-recomendados",
    "title": "Referências",
    "section": "",
    "text": "R Project: https://www.r-project.org/\nCRAN Task View - Econometrics: https://cran.r-project.org/web/views/Econometrics.html\nRStudio: https://www.rstudio.com/\n\n\n\n\n\nR for Data Science: https://r4ds.had.co.nz/\nIntroduction to Econometrics with R: https://www.econometrics-with-r.org/\nApplied Econometrics with R (livro online): https://eeecon.uibk.ac.at/~zeileis/teaching/AER/\n\n\n\n\n\nStack Overflow (tag R): https://stackoverflow.com/questions/tagged/r\nR-help mailing list: https://stat.ethz.ch/mailman/listinfo/r-help\nRStudio Community: https://community.rstudio.com/"
  },
  {
    "objectID": "referencias.html#dados-utilizados",
    "href": "referencias.html#dados-utilizados",
    "title": "Referências",
    "section": "",
    "text": "Instituto Nacional de Estatística (INE): https://www.ine.pt/\nBanco de Portugal: https://www.bportugal.pt/\nPORDATA: https://www.pordata.pt/\nEurostat: https://ec.europa.eu/eurostat/\nWorld Bank Open Data: https://data.worldbank.org/\nOECD Data: https://data.oecd.org/\n\n\n\n\nOs dados utilizados nos exemplos deste livro incluem:\n\nDados simulados para fins pedagógicos\nConjuntos de dados públicos disponíveis nos pacotes de R\nDados económicos de acesso livre de organizações internacionais"
  },
  {
    "objectID": "referencias.html#citações-e-referências-específicas",
    "href": "referencias.html#citações-e-referências-específicas",
    "title": "Referências",
    "section": "",
    "text": "Hausman, J. A. (1978). Specification Tests in Econometrics. Econometrica, 46(6), 1251-1271.\nWhite, H. (1980). A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity. Econometrica, 48(4), 817-838.\nBreusch, T. S., & Pagan, A. R. (1979). A Simple Test for Heteroscedasticity and Random Coefficient Variation. Econometrica, 47(5), 1287-1294.\nDurbin, J., & Watson, G. S. (1971). Testing for Serial Correlation in Least Squares Regression. Biometrika, 58(1), 1-19.\nHeckman, J. J. (1979). Sample Selection Bias as a Specification Error. Econometrica, 47(1), 153-161.\n\n\n\n\n\nJarque, C. M., & Bera, A. K. (1980). Efficient Tests for Normality, Homoscedasticity and Serial Independence of Regression Residuals. Economics Letters, 6(3), 255-259.\nShapiro, S. S., & Wilk, M. B. (1965). An Analysis of Variance Test for Normality (Complete Samples). Biometrika, 52(3/4), 591-611.\nChow, G. C. (1960). Tests of Equality Between Sets of Coefficients in Two Linear Regressions. Econometrica, 28(3), 591-605."
  },
  {
    "objectID": "referencias.html#agradecimentos",
    "href": "referencias.html#agradecimentos",
    "title": "Referências",
    "section": "",
    "text": "Este livro beneficiou do contributo de várias fontes e recursos:\n\nComunidade R pela disponibilização de ferramentas open-source\nAutores dos pacotes de R utilizados\nInvestigadores que desenvolveram as metodologias econométricas apresentadas\nEstudantes e colegas que forneceram feedback durante o desenvolvimento\n\n\nNota: Todas as URL foram verificadas em Julho de 2024. Algumas podem ter mudado desde então."
  },
  {
    "objectID": "capitulo-06-extensoes-regressao.html",
    "href": "capitulo-06-extensoes-regressao.html",
    "title": "Extensões dos Modelos de Regressão",
    "section": "",
    "text": "Este capítulo apresenta várias extensões dos modelos de regressão linear básicos que são frequentemente necessárias na estimação de modelos econométricos. Estas extensões permitem lidar com situações mais complexas e realistas que os modelos lineares simples não conseguem capturar adequadamente.\n\n\nUma variável dummy é uma variável que assume valores binários (0 ou 1) que indica apresença ou não de uma característica específica. Por exemplo, uma variável dummy pode ser usada para indicar a presença de uma determinada condição (sim ou não) ou para categorias. Por exemplo, uma variável dummy pode ser usada para indicar se uma empresa está localizada numa determinada região (1) ou não (0), ou em várias regiões (mais de duas categorias).\n\n\nAs variáveis dummy são variáveis binárias que assumem valores 0 ou 1.\nUma dummmy, é geralmente representada por:\n\\[\nD_i = \\begin{cases}\n1, & \\text{se a condição é satisfeita} \\\\\n0, & \\text{a condição não é satisfeita}\n\\end{cases}\n\\]\nNum modelo de regressão, uma variável dummy pode ser incluída como uma variável explicativa para capturar o efeito de uma característica qualitativa na variável dependente:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 D_i + u_i\n\\tag{1}\\]\nÉ necessário definir muito bem o que é o 0 ou o que e o 1., pois vai mudar a interpretação dos coeficientes no modelo. Para este exemplo vamos utilizar os dados wage2 da biblioteca wooldridge, que contém informações sobre o salário, bem como outras características como anos de experiência, anos na empresa e outras características. O conjunto de dados contém variáveis dummy em que uma delas (married), que indica se o funcionário é casado (1) ou não (0). No R uma variável não tem de ser necessariamente binária para ser tratada como dummy, o R cria automaticamente as dummies para variáveis categóricas que são fatores (para o R). Vamos estimar a regressão do salário em função dos anos de educação, anos de experiência e se é casado ou não:\n\\[\nwage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 married_i + u_i\n\\tag{2}\\]\nA interpretação do coeficientes da dummy (\\(\\beta_3\\)) é a diferença média no salário entre indivíduos casados e não casados (\\(married = 1\\) e \\(married = 0\\)), mantendo a educação e a experiência constantes. Ou seja, depende das unidades da variável dependente (neste caso, salário em dólares americanos).\n\nlibrary(tidyverse)\nlibrary(wooldridge)\ndata(\"wage2\")\n\n# Estimar modelo de regressão\nmodelo_dummy &lt;- lm(wage ~ educ + exper + married, data = wage2)\nsummary(modelo_dummy)\n\n\nCall:\nlm(formula = wage ~ educ + exper + married, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-864.16 -241.25  -39.12  194.22 2145.23 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -427.775    111.109  -3.850 0.000126 ***\neduc          76.550      6.227  12.293  &lt; 2e-16 ***\nexper         16.317      3.139   5.198 2.48e-07 ***\nmarried      185.907     39.604   4.694 3.08e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 372.1 on 931 degrees of freedom\nMultiple R-squared:  0.1558,    Adjusted R-squared:  0.1531 \nF-statistic: 57.29 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretação dos coeficientes:\n\n\\(\\beta_0\\) (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência e que não é casado é de -427.76 US$.\n\\(\\beta_1\\) (educ): Cada ano adicional de educação está associado a um aumento médio de 76.55 US$ no salário, ceteris paribus.\n\\(\\beta_2\\) (exper): Cada ano adicional de experiência está associado a um aumento médio de 16.32 US$ no salário, ceteris paribus.\n\\(\\beta_3\\) (married): um indivíduo casado ganha, em média, 185.91 US$ a mais do que um indivíduo não casado, ceteris paribus.\n\nPara um regressão simples, apenas com a variável dummy como independente, o valor da constante é o valor médio da variável dependente quando a dummy é 0. Considerando o modelo:\n\n# Estimar modelo de regressão com apenas a dummy\nmodelo_dummy_s &lt;- lm(wage ~ married, data = wage2)\nsummary(modelo_dummy_s)\n\n\nCall:\nlm(formula = wage ~ married, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-862.05 -284.05  -51.05  210.95 2100.95 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   798.44      40.08  19.922  &lt; 2e-16 ***\nmarried       178.61      42.41   4.211 2.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 400.8 on 933 degrees of freedom\nMultiple R-squared:  0.01865,   Adjusted R-squared:  0.0176 \nF-statistic: 17.74 on 1 and 933 DF,  p-value: 2.784e-05\n\n\nO salário médio para indivíduos não casados é de 798.44 US$. E qual o salário médio para indicíduos casados? Podemos calculamos o valor do salário para quando \\(married = 1\\) considerando os coeficientes do modelo:\n\n# Calcular salário médio para indivíduos casados\nsalario_casado &lt;- predict(modelo_dummy_s, newdata = data.frame(married = 1))\nsalario_casado\n\n       1 \n977.0479 \n\n\nO salário médio para indivíduos casados é em média de 977.05 US$. Apenas para confirmar, podemos calcular a média diretamente com uma subamostra para cada grupo:\n\n# Calcular salário médio para indivíduos casados\n\n# Para indivíduos não casados (married = 0)\nsalario_nao_casado &lt;- wage2 %&gt;% \n    filter(married == 0) %&gt;% \n    summarise(salario_medio = mean(wage))\nsalario_nao_casado\n\n  salario_medio\n1        798.44\n\n# Para indivíduos casados (married = 1)\nsalario_casado &lt;- wage2 %&gt;% \n    filter(married == 1) %&gt;% \n    summarise(salario_medio = mean(wage))\nsalario_casado\n\n  salario_medio\n1      977.0479\n\n\nAtravé do gráfico podemos visualizar a diferença nos salários entre indivíduos casados e não casados:\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n\n\n\nO ponto vermelho indica a média do salário para cada grupo.\nPodemos converter a variável married para non-married (0 = casado, 1 = não casado) e re-estimar o modelo:\n\n# Converter variável married para non-married\nwage2$non_married &lt;- ifelse(wage2$married == 1, 0, 1)\n\n# Estimar modelo de regressão com a nova variável\nmodelo_dummy_non_married &lt;- lm(wage ~ non_married,\n                                data = wage2)\n\n#Comparar modelos\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nstargazer(modelo_dummy_s,\n        modelo_dummy_non_married,\n        type = \"text\")\n\n\n===========================================================\n                                   Dependent variable:     \n                               ----------------------------\n                                           wage            \n                                    (1)            (2)     \n-----------------------------------------------------------\nmarried                          178.608***                \n                                  (42.411)                 \n                                                           \nnon_married                                    -178.608*** \n                                                (42.411)   \n                                                           \nConstant                         798.440***    977.048***  \n                                  (40.079)      (13.870)   \n                                                           \n-----------------------------------------------------------\nObservations                        935            935     \nR2                                 0.019          0.019    \nAdjusted R2                        0.018          0.018    \nResidual Std. Error (df = 933)    400.786        400.786   \nF Statistic (df = 1; 933)        17.736***      17.736***  \n===========================================================\nNote:                           *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nPodemos ver que o coeficiente da dummy mudou de sinal, mas a interpretação alterou de a diferença média no salário entre indivíduos casados e não casados (modelo_dummy_s) para a diferença média no salário entre indivíduos não casados e casados (modelo_dummy_non_married). A constante do segundo modelo é o salário médio para indivíduos casados (quando non_married = 0).\nQuando a variável dependete está em logaritmos, o coeficiente da dummy pode ser interpretado como uma diferença percentual aproximada. Por exemplo para o modelo:\n\\[\n\\log(wage_i) = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 married_i + u_i\n\\tag{3}\\]\nNo R:\n\n# Estimar modelo de regressão com log do salário\nmodelo_dummy_log &lt;- lm(log(wage) ~ educ + exper + married,\n                        data = wage2)\nsummary(modelo_dummy_log)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + married, data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.89479 -0.23544  0.02585  0.25920  1.27757 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.327960   0.115832  45.997  &lt; 2e-16 ***\neduc        0.078158   0.006492  12.039  &lt; 2e-16 ***\nexper       0.018290   0.003273   5.588 3.01e-08 ***\nmarried     0.209262   0.041288   5.068 4.84e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3879 on 931 degrees of freedom\nMultiple R-squared:  0.1542,    Adjusted R-squared:  0.1515 \nF-statistic: 56.58 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nPortanto a variação percentual aproximada é de 20.9% \\(100 \\times \\beta_{married}\\). Neste caso, indivíduos casados ganham, em média, cerca de 20.9% a mais do que indivíduos não casados, ceteris paribus. Para uma interpretação mais precisa, podemos usar a fórmula de @halvorsen_interpretation_1980:\n\\[\n\\text{Variação Percentual} = (e^{\\beta_3} - 1) \\times 100\n\\]\nno R é necessário extrair o coefiente do modelo e calcular o valor exponencial:\n\n# Extrair coeficiente da dummy\nbeta_married &lt;- coef(modelo_dummy_log)[\"married\"]\n# Calcular Variação percentual\nvariacao_percentual &lt;- (exp(beta_married) - 1) * 100\nvariacao_percentual\n\n married \n23.27677 \n\n\nEm média, indivíduos casados ganham cerca de 23.28% a mais do que indivíduos não casados, ceteris paribus.\nou então pelo método alternativo sugerido por @kennedy_estimation_1981:\n\\[\n\\text{Variação Percentual} \\approx 100 \\times \\left(\\frac{e^{\\beta_3} - 1}{1}\\right)\n\\]\nEste método é especialmente útil quando o coeficiente é grande (geralmente maior que 0.1 ou 10%) e para amostras pequenas, pois leva em consideração a variância do estimador:\n\\[\n\\Delta Y \\approx \\left(e^{\\beta - \\frac{1}{2} \\cdot \\text{Var}(\\beta)} - 1\\right) \\times 100\n\\]\nNo R, a variância do coeficiente pode ser obtida a partir da matriz de variância-covariância do modelo:\n\n# Obter variância do coeficiente\nvar_beta_married &lt;- vcov(modelo_dummy_log)[\"married\", \"married\"]\n# Calcular Variação percentual ajustada\ndelta_y &lt;- (exp(beta_married - 0.5 * var_beta_married) - 1) * 100\ndelta_y\n\n married \n23.17174 \n\n\nNeste caso, a variação percentual ajustada é de 23.17%, que é ligeiramente diferente da estimativa anterior.\n\n\n\nAs variáveis dummy também podem ser usadas para representar variáveis categóricas. Por exemplo, se tivermos uma variável multinominal que indica a cor dos carros vendidos (vermelho, azul, verde), podemos criar dummies para cada categoria:\n\\[\nD_{vermelho} = \\begin{cases}  \n1, & \\text{se o carro é vermelho} \\\\\n0, & \\text{não é vermelho}\n\\end{cases}\n\\tag{4}\\]\n\\[\nD_{azul} = \\begin{cases}\n1, & \\text{se o carro é azul} \\\\\n0, & \\text{não é azul}\n\\end{cases}\n\\tag{5}\\]\n\\[\nD_{verde} = \\begin{cases}\n1, & \\text{se o carro é verde} \\\\\n0, & \\text{não é verde}\n\\end{cases}\n\\tag{6}\\]\nExemplo no R:\n\n#introduzir dados\ncores &lt;- c(\"vermelho\", \"azul\", \"verde\", \"vermelho\",\n            \"verde\", \"azul\", \"vermelho\", \"verde\", \n            \"azul\", \"vermelho\")\npreço &lt;- c(50000, 52000, 48000, 51000, 49000, 53000, \n            50000, 52000, 48000, 51000)\n\ndados &lt;- data.frame(cores, preço)\n\n#para tabela resumo\nlibrary(gtsummary)\n\n# Criar dummies e taela\ndados |&gt;\n    mutate(\n        vermelho = ifelse(cores == \"vermelho\", 1, 0),\n        azul = ifelse(cores == \"azul\", 1, 0),\n        verde = ifelse(cores == \"verde\", 1, 0)\n    ) |&gt;\n    select(cores, vermelho, azul, verde) |&gt;\n    tbl_summary(\n        by = cores)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nazul\nN = 31\nverde\nN = 31\nvermelho\nN = 41\n\n\n\n\nvermelho\n0 (0%)\n0 (0%)\n4 (100%)\n\n\nazul\n3 (100%)\n0 (0%)\n0 (0%)\n\n\nverde\n0 (0%)\n3 (100%)\n0 (0%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\nA função ifelse() é usada para criar as variáveis dummy. A sintaxe é ifelse(condição, valor_se_condicao_verificada, valor_se_condicao_nao_verificada), ver ?ifelse para mais detalhes.\nPara um exemplo real vamos usar o conjunto de dados ceosal1 da biblioteca wooldridge, que contém informações sobre o salário dos CEOs de várias empresas, bem como outras características como anos de experiência, anos na empresa dummies que ondicam o setor de atividade da empresa:\n\nfinance - empresa do setor financeiro (1) ou não (0)\nindus - empresa do setor industrial (1) ou não (0)\nconsprod - empresa do setor de bens de consumo (1) ou não (0)\nutility - empresa do setor de serviços essenciais (1) ou não (0)\n\nNeste conjunto não é necessário calcular as dummies, pois já estão incluídas (o que nem sempre acontece). Vamos estimar um modelo de regressão do logaritmo do salário dos CEOs (lsalary) em função do logaritmo vendas (lsales), da rendibilidade dos capitais próprios (roe`) e o setor de atividade da empresa:\n\\[\n\\begin{split}\n\\log(salary_i) = \\beta_0 &+ \\beta_1 \\log(sales_i) + \\beta_2 roe_i + \\beta_3 D_{finance} + \\\\\n&\\beta_4 D_{indus} + \\beta_5 D_{consprod} + \\beta_6 D_{utility} + u_i\n\\end{split}\n\\tag{7}\\]\nNo R:\n\nlibrary(wooldridge)\ndata(\"ceosal1\")\n# Estimar modelo de regressão\nmodelo_dummy_mult &lt;- lm(lsalary ~ lsales + roe + finance + indus + consprod + utility,\n                        data = ceosal1)\nsummary(modelo_dummy_mult)\n\n\nCall:\nlm(formula = lsalary ~ lsales + roe + finance + indus + consprod + \n    utility, data = ceosal1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09465 -0.22173 -0.01973  0.17141  2.64394 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.30510    0.28156  15.290  &lt; 2e-16 ***\nlsales       0.25719    0.03204   8.029 7.85e-14 ***\nroe          0.01115    0.00430   2.594   0.0102 *  \nfinance      0.44096    0.10365   4.254 3.20e-05 ***\nindus        0.28300    0.09923   2.852   0.0048 ** \nconsprod     0.46389    0.10887   4.261 3.12e-05 ***\nutility           NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4598 on 203 degrees of freedom\nMultiple R-squared:  0.3569,    Adjusted R-squared:  0.341 \nF-statistic: 22.53 on 5 and 203 DF,  p-value: &lt; 2.2e-16\n\n\nNa regressão anterior o coeficiente da dummy utility aparece com valor NA, porque existe multicolinearidade perfeita entre as dummies (todas somadas é obtida uma coluna de 1’s, que é uma constante).\n\n# Verificar multicolinearidade perfeita\nceosal1 %&gt;%\n    mutate(soma_dummies = finance + indus + consprod + utility) %&gt;%\n    summarise(min_soma = min(soma_dummies),\n              max_soma = max(soma_dummies),\n              unique_somas = n_distinct(soma_dummies))\n\n  min_soma max_soma unique_somas\n1        1        1            1\n\n\nEste fenómeno é conhecido como a armadilha das dummies. Em alguns softwares econométrico pode aparecer uma mensagem de erro a indicar que existe multicolinearidade perfeita. Para evitar este problema, uma das categorias deve ser excluída do modelo, ou seja, o modelo deve incluir apenas \\(k-1\\) dummies (com \\(k\\) o número de categorias). A categoria excluída é a categoria de referência (base) e as outras dummies medem o efeito relativo em comparação com essa mesma categoria. De referir que nesta amostra cada empresa pertence a apenas um setor. Neste caso, podemos excluir a dummy utility e re-estimar o modelo:\n\\[\n\\begin{split}\n\\log(salary_i) = \\beta_0 &+ \\beta_1 \\log(sales_i) + \\beta_2 roe_i + \\beta_3 D_{finance} + \\\\\n& \\beta_4 D_{indus} + \\beta_5 D_{consprod} + u_i\n\\end{split}\n\\tag{8}\\]\nNo R:\n\n# Estimar modelo de regressão sem a dummy utility\nmodelo_dummy_mult2 &lt;- lm(lsalary ~ lsales + roe + finance + indus + consprod,\n                         data = ceosal1)\nsummary(modelo_dummy_mult2)\n\n\nCall:\nlm(formula = lsalary ~ lsales + roe + finance + indus + consprod, \n    data = ceosal1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09465 -0.22173 -0.01973  0.17141  2.64394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.30510    0.28156  15.290  &lt; 2e-16 ***\nlsales       0.25719    0.03204   8.029 7.85e-14 ***\nroe          0.01115    0.00430   2.594   0.0102 *  \nfinance      0.44096    0.10365   4.254 3.20e-05 ***\nindus        0.28300    0.09923   2.852   0.0048 ** \nconsprod     0.46389    0.10887   4.261 3.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4598 on 203 degrees of freedom\nMultiple R-squared:  0.3569,    Adjusted R-squared:  0.341 \nF-statistic: 22.53 on 5 and 203 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretação dos coeficientes :\n\n\\(\\beta_0\\) (constante): O salário médio para uma empresa do setor de serviços essenciais (categoria de referência) com vendas iguais a 1 (log(1) = 0) e rendibilidade dos capitais próprios igual a 0 é de 257 US$.\n\\(\\beta_1\\) (lsales): Um aumento de 1% nas vendas está associado a um aumento médio de 25.7% no salário do CEO, ceteris paribus.\n\\(\\beta_2\\) (roe): Um aumento de 1 ponto percentual na rendibilidade dos capitais próprios está associado a um aumento médio de aproximadamente 1.1% no salário do CEO, ceteris paribus (a variável roe está expressa em percentagem, 0-100).\n\\(\\beta_3\\) (finance): CEOs de empresas do setor financeiro ganham, aproximadamente em média, cerca de 44.1% a mais do que CEOs de empresas do setor de serviços essenciais (categoria de referência), ceteris paribus.\n\\(\\beta_4\\) (indus): CEOs de empresas do setor industrial ganham, aproximadamente em média, cerca de 28.3% a mais do que CEOs de empresas do setor de serviços essenciais, ceteris paribus.\n\\(\\beta_5\\) (consprod): CEOs de empresas do setor de bens de consumo ganham, aproximadamente em média, cerca de 3.3% a mais do que CEOs de empresas do setor de serviços essenciais, ceteris paribus.\n\npodemos considerar a dummy indus como a categoria de referência, excluindo-a do modelo:\n\n# Estimar modelo de regressão sem a dummy indus\nmodelo_dummy_mult3 &lt;- lm(lsalary ~ lsales + roe + finance + consprod + utility,\n                         data = ceosal1)\n\nlibrary(stargazer)\nstargazer(modelo_dummy_mult2,\n        modelo_dummy_mult3,\n        type = \"text\")\n\n\n===========================================================\n                                   Dependent variable:     \n                               ----------------------------\n                                         lsalary           \n                                    (1)            (2)     \n-----------------------------------------------------------\nlsales                            0.257***      0.257***   \n                                  (0.032)        (0.032)   \n                                                           \nroe                               0.011**        0.011**   \n                                  (0.004)        (0.004)   \n                                                           \nfinance                           0.441***       0.158*    \n                                  (0.104)        (0.089)   \n                                                           \nindus                             0.283***                 \n                                  (0.099)                  \n                                                           \nconsprod                          0.464***       0.181**   \n                                  (0.109)        (0.085)   \n                                                           \nutility                                         -0.283***  \n                                                 (0.099)   \n                                                           \nConstant                          4.305***      4.588***   \n                                  (0.282)        (0.295)   \n                                                           \n-----------------------------------------------------------\nObservations                        209            209     \nR2                                 0.357          0.357    \nAdjusted R2                        0.341          0.341    \nResidual Std. Error (df = 203)     0.460          0.460    \nF Statistic (df = 5; 203)        22.529***      22.529***  \n===========================================================\nNote:                           *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nPodemos observar que do modelo modelo_dummy_mult2 e modelo_dummy_mult3 que os coeficientes das dummies dos setores alteraram, pois a categoria de referência mudou. O coeficiente da dummy indus no primeiro modelo é o inverso do coeficiente da dummy utility no segundo modelo. Tal como na regressão Equation 2, também é possível calcular a variação de salário em percentagem para as dummies do modelo Equation 8, usando a fórmula de @halvorsen_interpretation_1980 ou o método alternativo sugerido por @kennedy_estimation_1981.\n\n# Extrair coeficientes das dummies\nbeta_finance &lt;- coef(modelo_dummy_mult2)[\"finance\"]\nbeta_indus &lt;- coef(modelo_dummy_mult2)[\"indus\"]\nbeta_consprod &lt;- coef(modelo_dummy_mult2)[\"consprod\"]\n# Calcular variação percentual usando a fórmula de Halvorsen e Palmquist (1980)\nvariacao_percentual_finance &lt;- (exp(beta_finance) - 1) * 100\nvariacao_percentual_indus &lt;- (exp(beta_indus) - 1) * 100\nvariacao_percentual_consprod &lt;- (exp(beta_consprod) - 1) * 100\n\n#calcular variação percentual ajustada usando o método de Kennedy (1981)\nvar_beta_finance &lt;- vcov(modelo_dummy_mult2)[\"finance\", \"finance\"]\nvar_beta_indus &lt;- vcov(modelo_dummy_mult2)[\"indus\", \"indus\"]\nvar_beta_consprod &lt;- vcov(modelo_dummy_mult2)[\"consprod\", \"consprod\"]\ndelta_y_finance &lt;- (exp(beta_finance - 0.5 * var_beta_finance) - 1) * 100\ndelta_y_indus &lt;- (exp(beta_indus - 0.5 * var_beta_indus) - 1) * 100\ndelta_y_consprod &lt;- (exp(beta_consprod - 0.5 * var_beta_consprod) - 1) * 100\n\n#organizar em tabela:\ntabela_variacao &lt;- data.frame(\n    Categoria = c(\"finance\", \"indus\", \"consprod\"),\n    Variacao_Percentual = c(variacao_percentual_finance,\n                            variacao_percentual_indus,\n                            variacao_percentual_consprod),\n    Variacao_Percentual_Ajustada = c(delta_y_finance,\n                                    delta_y_indus,\n                                    delta_y_consprod)\n)\ntabela_variacao\n\n         Categoria Variacao_Percentual Variacao_Percentual_Ajustada\nfinance    finance            55.41952                     54.58691\nindus        indus            32.71071                     32.05889\nconsprod  consprod            59.02531                     58.08560\n\n\nA interpretação é semelhantes ao exemplo anterior. Também é possível utilizar variáveis dummy como variável independente. Esta abordagem será discutida na ?@sec-variavel-dependente-limitada.\n\n\n\n\nUm termo de interação é utilizado para capturar o efeito conjunto de duas ou mais variáveis independentes na variável dependente. Portanto num exemplo simples, um termo de interação entre duas variáveis \\(X_1\\) e \\(X_2\\) é representado como \\(X_1 \\times X_2\\). O termo de interação permite que o efeito de uma variável independente dependa do valor de outra variável independente.\n\\[\ny_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 (X_{1i} \\times X_{2i}) + u_i\n\\tag{9}\\]\nPodemos criar termos de interaão entre variáveis contínuas, entre variáveis dummy e entre variáveis dummy e contínuas.\n\n\nPara esta apicação vamos utilizar o conjunto de dados wage2 da biblioteca wooldridge. Mais informação sobre o conjunto de dados pode ser obtida com ?wage2.\nVamos estimar a regressão do salário em função dos anos de educação, anos de experiência e a interacção entre educação e experiência:\n\\[\nwage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 (educ_i \\times exper_i) + u_i\n\\tag{10}\\]\nO termo de interação pode ser crido no data.frame (mutate(educ_exper = educ* exper)) ou diretamente na fórmula do modelo. No R:\n\nlibrary(tidyverse)\nlibrary(wooldridge)\n\n# carregar dados\ndata(\"wage2\")\n\n#Estimar modelo de regressão com interacção\nmodelo_interacoes &lt;- lm(wage ~ educ + exper + I(educ * exper), data = wage2)\nsummary(modelo_interacoes)\n\n\nCall:\nlm(formula = wage ~ educ + exper + I(educ * exper), data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-948.46 -254.53  -29.57  192.59 2150.77 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      271.934    230.227   1.181  0.23784   \neduc              35.106     16.626   2.112  0.03499 * \nexper            -32.663     19.099  -1.710  0.08757 . \nI(educ * exper)    3.904      1.462   2.670  0.00771 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 375.1 on 931 degrees of freedom\nMultiple R-squared:  0.1424,    Adjusted R-squared:  0.1397 \nF-statistic: 51.54 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nO I é necessário para indicar ao R que a operação deve ser interpretada literalmente, ou seja, como uma multiplicação entre as variáveis educ e exper. Sem o I, o R pode interpretar o símbolo * como um operador especial na fórmula do modelo, o que pode levar a resultados inesperados.\nNa interpretação dos coeficientes é necessário ter alguma atenção pois o coeficiente de cada variável que também faz parte do termo de interação depende do valor da outra variável. A interpretação dos coeficientes é a seguinte:\n\n\\(\\beta_0\\) (constante): O salário médio para alguém com 0 anos de educação e 0 anos de experiência é de 271.934 US$. Pode não ter uma interpretação prática, pois é improvável que alguém tenha 0 anos de educação e experiência.\n\\(\\beta_1\\) (educ): Cada ano adicional de educação está associado a um aumento médio de 35.106 US$ no salário, ceteris paribus, para alguém sem experiência (exper = 0).\n\\(\\beta_2\\) (exper): Cada ano adicional de experiência está associado a uma diminuição média de 32.663 US$ no salário, ceteris paribus, para alguém sem educação (educ = 0).\n\\(\\beta_3\\) (I(educ * exper)): O coeficiente de interação indica que o efeito combinado de educação e experiência no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito da experiência no salário aumenta em 3.904 US$, e vice-versa.\n\nNum caso em que o termo de interação é negativo, por exemplo de -3.904, indicaria que o efeito combinado de educação e experiência no salário é negativo, ou seja, para cada ano adicional de educação, o efeito da experiência no salário diminui em 3.904 US$, e vice-versa.\nPara calcular o efeito marginal da educação no salário para diferentes níveis de experiência. O efeito marginal da educação é dado por:\n\\[\n\\frac{\\partial wage}{\\partial educ} = \\beta_1 + \\beta_3 \\times exper\n\\] Portanto, o efeito marginal da educação varia com o nível de experiência. Podemos calcular o efeito marginal da educação para diferentes níveis de experiência (0, 5, 10, 20 anos):\n\n# Níveis de experiência para calcular o efeito marginal\nniv_experiencia &lt;- c(0, 5, 10, 20)\n# Coeficientes do modelo\ncoeficientes &lt;- coef(modelo_interacoes)\n# Calcular efeito marginal da educação para diferentes níveis de experiência\nefeito_marginal_educ &lt;- coeficientes[\"educ\"] +\n                coeficientes[\"I(educ * exper)\"] * niv_experiencia\n# Criar data frame com os resultados obtidos\ndata.frame(Nivel_Experiencia = niv_experiencia,\n            Efeito_Marginal_Educacao = efeito_marginal_educ)\n\n  Nivel_Experiencia Efeito_Marginal_Educacao\n1                 0                 35.10600\n2                 5                 54.62379\n3                10                 74.14158\n4                20                113.17716\n\n\nPara um modelo onde a variável dependente está em logaritmo natural, o coeficiente da variável contínua no termo de interação pode ser interpretado como uma variação percentual aproximada. Por exemplo, para o modelo:\n\nmodelo_interacoes_log &lt;- lm(log(wage) ~ educ + exper + I(educ * exper), data = wage2)\nsummary(modelo_interacoes_log)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + I(educ * exper), data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.88558 -0.24553  0.03558  0.26171  1.28836 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      5.949455   0.240826  24.704   &lt;2e-16 ***\neduc             0.044050   0.017391   2.533   0.0115 *  \nexper           -0.021496   0.019978  -1.076   0.2822    \nI(educ * exper)  0.003203   0.001529   2.095   0.0365 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3923 on 931 degrees of freedom\nMultiple R-squared:  0.1349,    Adjusted R-squared:  0.1321 \nF-statistic: 48.41 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\n\nconstante: O salário médio para alguém com 0 anos de educação e 0 anos de experiência é de 6.579 US$ (log(719.82)).\neduc: Cada ano adicional de educação está associado a um aumento médio de aproximadamente 4.4% no salário, ceteris paribus, para alguém sem experiência (exper = 0).\nexper: Cada ano adicional de experiência está associado a uma diminuição média de aproximadamente 2.15% no salário, ceteris paribus, para alguém sem educação (educ = 0).\nI(educ * exper): O coeficiente de interação indica que o efeito combinado de educação e experiência no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito da experiência no salário aumenta em aproximadamente 0.32%, e vice-versa.\n\nDe notar que o coeficiente da variável exper não é estatisticamente diferente de zero para nenhum nível de significância estatística.\n\n\n\nRecorrendo ao conjunto de dados anterior (wage2), onde a variável married indica se o indivíduo é casado (1) ou não (0) e a variável dummy urban indica se o indivíduo vive numa área urbana (1) ou não (0), vamos estimar o modelo de regressão com termos de interação entre variáveis dummy:\n\\[\n\\log(wage_i) = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 married_i + \\beta_4 urban_i + \\beta_5 (married_i \\times urban_i) + u_i\n\\tag{11}\\]\nno R:\n\n#estimar modelo de regressão com interacção entre dummies\nmodelo_interacoes_dummy &lt;- lm(log(wage) ~ educ + exper + married + \n                            urban + I(married * urban), data = wage2)\nsummary(modelo_interacoes_dummy)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + married + urban + I(married * \n    urban), data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.94789 -0.22230  0.02629  0.24607  1.22611 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.198499   0.134840  38.553  &lt; 2e-16 ***\neduc                0.075924   0.006374  11.911  &lt; 2e-16 ***\nexper               0.018587   0.003207   5.796 9.29e-09 ***\nmarried             0.240197   0.083079   2.891  0.00393 ** \nurban               0.204342   0.090328   2.262  0.02391 *  \nI(married * urban) -0.028581   0.094926  -0.301  0.76341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3799 on 929 degrees of freedom\nMultiple R-squared:  0.1905,    Adjusted R-squared:  0.1861 \nF-statistic: 43.71 on 5 and 929 DF,  p-value: &lt; 2.2e-16\n\n\ninterpretação dos coeficientes: - \\(\\beta_0\\) (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência, que não é casado e que não vive numa área urbana é de 180.91 US$ (exp(5.198)).\n\n\\(\\beta_1\\) (educ): Cada ano adicional de educação está associado a um aumento médio de aproximadamente 7.5 % no salário, ceteris paribus.\n\\(\\beta_2\\) (exper): Cada ano adicional de experiência está associado a um aumento médio de aproximadamente 1.8% no salário, ceteris paribus.\n\\(\\beta_3\\) (married): Ser casado está associado a um aumento médio de aproximadamente 24.20% no salário, ceteris paribus, para alguém sem experiência e sem educação (exper = 0, educ = 0).\n\\(\\beta_4\\) (urban): Viver numa área urbana está associado a um aumento médio de aproximadamente 20.4.% no salário, ceteris paribus, para alguém sem experiência e sem educação (exper = 0, educ = 0).\n\\(\\beta_5\\) (married * urban): O coeficiente de interação indica que o efeito combinado de ser casado e viver em uma área urbana no salário é negativo. Especificamente, para indivíduos casados e viver numa área urbana, o efeito combinado no salário é de aproximadamente -2.86% em relação ao efeito individual de ser casado e viver numa área urbana.\n\nNo termo de interação entre variáveis dummy, neste caso married e urban, o 1 (da multiplicaão) corresponde a indivíduos que são casados e vivem numa área urbana (11), o 0 (da multiplicação) corresponde a indivíduos que não são casados e não vivem numa área urbana (00), e os outros dois casos (10 e 01) correspondem a indivíduos que são casados mas não vivem numa área urbana ou que não são casados mas vivem numa área urbana. Portanto, o coeficiente do termo de interação mede o efeito adicional de ser casado e viver numa área urbana em comparação com os efeitos individuais de ser casado e viver numa área urbana. Se criarmos uma tabela com os diferentes grupos, podemos ver melhor a interpretação de cada caso: ou seja:\n\n\n\n\n\n\n\n\n\n\n\nmarried\nurban\nInteração (= married × urban)\nlog(wage) previsto\nwage previsto (e^{})\nInterpretação\n\n\n\n\n0\n0\n0\n\\(\\beta_0\\)\n\\(e^{\\beta_0}\\)\nNão casado, não vive numa área urbana\n\n\n1\n0\n0\n\\(\\beta_0 + \\beta_3\\)\n\\(e^{\\beta_0 + \\beta_3}\\)\nCasado, não vive numa área urbana\n\n\n0\n1\n0\n\\(\\beta_0 + \\beta_4\\)\n\\(e^{\\beta_0 + \\beta_4}\\)\nNão casado, vive numa área urbana\n\n\n1\n1\n1\n\\(\\beta_0 + \\beta_3 + \\beta_4 + \\beta_5\\)\n\\(e^{\\beta_0 + \\beta_3 + \\beta_4 + \\beta_5}\\)\nCasado, vive numa área urbana\n\n\n\nDe notar que no modelo Equation 11 o coeficiente do termo de interação não é estatisticamente diferente de zero para nenhum nível de significância estatística. Ou seja, não existe evidência estatística de que o efeito combinado de ser casado e viver numa área urbana no salário seja diferente do efeito individual de ser casado e viver numa área urbana, o que também é um resultado interessante.\n\n\n\nPara esta aplicação vamos considerar o mesmo conjunto de dados wage2 e vamos estimar a regressão com o termo de interação entre a variável dummy south e a variável contínua educ:\n\\[\n\\log(wage_i) = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 south_i + \\beta_4 (south_i \\times educ_i) + u_i\n\\tag{12}\\]\nno R:\n\n#carregar dados\ndata(\"wage2\")\n\n# Estimar modelo\nmodelo_interacoes_dummy_cont &lt;- lm(log(wage) ~ educ + exper + south +\n                                     I(south * educ), data = wage2)\n\nsummary(modelo_interacoes_dummy_cont)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + south + I(south * educ), \n    data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.93539 -0.23116  0.02579  0.25730  1.28557 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      5.699845   0.122511  46.525  &lt; 2e-16 ***\neduc             0.066953   0.007543   8.876  &lt; 2e-16 ***\nexper            0.019671   0.003256   6.041 2.21e-09 ***\nsouth           -0.464313   0.167538  -2.771  0.00569 ** \nI(south * educ)  0.024110   0.012422   1.941  0.05257 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3868 on 930 degrees of freedom\nMultiple R-squared:  0.1601,    Adjusted R-squared:  0.1564 \nF-statistic: 44.31 on 4 and 930 DF,  p-value: &lt; 2.2e-16\n\n\nEm que a interpretação dos coeficientes é a seguinte:\n\n\\(\\beta_0\\) (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência e que não vive no sul é de 298.87 US$ (exp(5.70)).\n\\(\\beta_1\\) (educ): Cada ano adicional de educação está associado a um aumento médio de aproximadamente 6.7% no salário, ceteris paribus, para alguém que não vive no sul (south = 0).\n\\(\\beta_2\\) (exper): Cada ano adicional de experiência está associado a um aumento médio de aproximadamente 2% no salário, ceteris paribus.\n\\(\\beta_3\\) (south): Viver no sul está associado a uma diminuição média de aproximadamente 46.4% no salário, ceteris paribus, para alguém sem anos de educação (educ = 0).\n\\(\\beta_4\\) (south * educ): O coeficiente de interação indica que o efeito combinado de viver no sul e anos de educação no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito de viver no sul no salário aumenta em aproximadamente 2.41%, ceteris paribus.\n\nO efeito marginal da educação no salário para diferentes níveis de experiência é dado por: \\[\n\\frac{\\partial \\log(wage)}{\\partial educ} = \\beta_1 + \\beta_4 \\times south\n\\tag{13}\\]\nPortanto, o efeito marginal da educação varia com o valor da variável dummy south. Podemos calcular o efeito marginal da educação para os dois grupos (south = 0 e south = 1):\n\n# Coeficientes do modelo\ncoeficientes &lt;- coef(modelo_interacoes_dummy_cont)\n# Calcular efeito marginal da educação para os dois grupos\nefeito_marginal_educacao &lt;- data.frame(\n  south = c(0, 1),\n  efeito_marginal = c(coeficientes[\"educ\"] + 0 * coeficientes[\"I(south * educ)\"],\n                      coeficientes[\"educ\"] + 1 * coeficientes[\"I(south * educ)\"])\n)\nefeito_marginal_educacao\n\n  south efeito_marginal\n1     0      0.06695309\n2     1      0.09106290\n\n\nPara indivíduos que não vivem no sul (south = 0), cada ano adicional de educação está associado a um aumento médio de aproximadamente 6.7% no salário, ceteris paribus. Para indivíduos que vivem no sul (south = 1), cada ano adicional de educação está associado a um aumento médio de aproximadamente 9.11% no salário, ceteris paribus.\n\n\n\n\nModelos não lineares são modelos em que a relação entre a variável dependente e uma ou mais variáveis independentes não é linear. A equação do modelo não linear para a variável dependente y e para as variáveis independentes X é dada por:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{1i}^2 + u_i\n\\tag{14}\\]\nem que \\(\\beta_0\\) é a constante, \\(\\beta_1\\) é o coeficiente da variável independente x1, \\(\\beta_2\\) é o coeficiente do termo quadrático da variável independente x2 e \\(u_i\\) é o termo de erro. O modelo é estimado pelo método dos mínimos quadrados ordinários. Quando devemos estimar um modelo não linear? Uma abordagem comum é a análise gráfica. Se a relação entre a variável dependente e a variável independente não for aproximadamente linear, pode ser apropriado considerar um modelo não linear. Outra abordagem é incluir termos polinomiais (como o termo quadrático) na regressão e verificar se esses termos são estatisticamente diferentes de 0. Existem alguma forma de lidar com a não linearidade sem recorrer a modelos não lineares, como por exemplo, transformar as variáveis (logaritmos).\nUm exemplo de um modelo que poderá ser não linear é a relação entre o preço das casas e a distância à autoestrada mais próxima. Vamos considerar o conjunto de dados hprice2 da biblioteca wooldridge, que contém informações sobre o preço das casas (price), a distância à autoestrada mais próxima (dist) e outras características das casas. Mais informação sobre o conjunto de dados pode ser obtida com ?hprice2.\nPara esta aplicação vamos estimar o modelo:\n\\[\n\\log(price_i) = \\beta_0 + \\beta_1 dist_i + \\beta_2 dist_i^2 + u_i\n\\tag{15}\\]\nno R:\n\nlibrary(wooldridge)\ndata(\"hprice3\")\n\n# Estimar modelo de regressão não linear\nmodelo_nao_linear &lt;- lm(price ~ inst + I(inst^2), data = hprice3)\nsummary(modelo_nao_linear)\n\n\nCall:\nlm(formula = price ~ inst + I(inst^2), data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-82012 -25286  -8394  20140 194917 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.443e+04  7.340e+03   4.691 4.04e-06 ***\ninst         8.618e+00  1.013e+00   8.510 6.95e-16 ***\nI(inst^2)   -2.276e-04  2.953e-05  -7.708 1.64e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38860 on 318 degrees of freedom\nMultiple R-squared:  0.1969,    Adjusted R-squared:  0.1918 \nF-statistic: 38.98 on 2 and 318 DF,  p-value: 7.228e-16\n\n\nA interpretação do termo de interação é a seguinte:\n\n\\(\\beta_2\\) (I(inst^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -0,0002276, o que indica que para cada unidade de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 0,0002276 dólares, ceteris paribus. O efeito é muito pequeno o que pode estar relacionado com a escala da variável inst (distância em pés), 1 pé são aproximadamente 0.3048 metros.\n\nVamos converter a variável inst de pés para quilómetros (1 pé = 0.0003048 km) e re-estimar o modelo:\n\n# Converter inst de pés para quilómetros\nhprice3 &lt;- hprice3 %&gt;%\n    mutate(inst_km = inst * 0.0003048)\n# Estimar modelo de regressão não linear com inst em km\nmodelo_nao_linear_km &lt;- lm(price ~ inst_km + I(inst_km^2), data = hprice3)\nsummary(modelo_nao_linear_km)\n\n\nCall:\nlm(formula = price ~ inst_km + I(inst_km^2), data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-82012 -25286  -8394  20140 194917 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   34434.5     7339.9   4.691 4.04e-06 ***\ninst_km       28275.8     3322.5   8.510 6.95e-16 ***\nI(inst_km^2)  -2449.7      317.8  -7.708 1.64e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38860 on 318 degrees of freedom\nMultiple R-squared:  0.1969,    Adjusted R-squared:  0.1918 \nF-statistic: 38.98 on 2 and 318 DF,  p-value: 7.228e-16\n\n\nA interpretação dos coeficientes é a seguinte:\n\n\\(\\beta_0\\) (constante): O preço médio de uma casa localizada exatamente na autoestrada (distância = 0 km) é de aproximadamente 34434.5 dólares.\n\\(\\beta_1\\) (inst_km): Cada quilómetro adicional de distância à autoestrada está associado a uma diminuição média de aproximadamente 28278.80 dólares no preço da casa, ceteris paribus.\n\\(\\beta_2\\) (I(inst_km^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -2449.70, o que indica que para cada quilómetro de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 2449.70 dólares, ceteris paribus.\n\nSe o coeficiente do termo quadrático fosse positivo, indicava que o efeito da distância no preço da casa aumentaria à medida que a distância aumenta, o que poderia sugerir que casas mais afastadas da autoestrada são mais valorizadas.\nTambém é possível calcular o efeito marginal da distância no preço da casa. O efeito marginal da distância é dado por:\n\\[\n\\frac{\\partial y}{\\partial x} = \\beta_1 + 2 \\beta_2 x\n\\tag{16}\\]\nPortanto, o efeito marginal da distância varia com o nível de distância. Podemos calcular o efeito marginal da distância para diferentes níveis de distância (0, 1, 2, 5, 10 unidades):\n\n# Níveis de distância para calcular o efeito marginal\nniv_distancia &lt;- c(0, 1, 2, 5, 10)\n\n# Coeficientes do modelo\ncoeficientes &lt;- coef(modelo_nao_linear_km)\n\n# Calcular efeito marginal para diferentes níveis de distância\nefeito_marginal &lt;- coeficientes[\"inst_km\"] + 2 * coeficientes[\"I(inst_km^2)\"] * niv_distancia\nefeito_marginal\n\n[1]  28275.79  23376.30  18476.81   3778.34 -20719.11\n\n# Criar data frame com os resultados obtidos\ndata.frame(Nivel_Distancia = niv_distancia,\n            Efeito_Marginal_Distancia = efeito_marginal)\n\n  Nivel_Distancia Efeito_Marginal_Distancia\n1               0                  28275.79\n2               1                  23376.30\n3               2                  18476.81\n4               5                   3778.34\n5              10                 -20719.11\n\n\nCom isto podemos ver que o efeito marginal da distância no preço da casa diminui à medida que a distância aumenta (relação de U invertido). Por exemplo, para uma casa localizada exatamente na autoestrada (inst_km = 0), o efeito marginal da distância é de aproximadamente 28275.79 dólares. Isto significa que se a casa estiver afastada 1 unidade da autoestrada, o preço da casa aumentará em aproximadamente 23376.30 doláres, ceteris paribus. No entanto, para uma casa localizada a 10 unidades da autoestrada (inst_km = 10), o efeito marginal da distância é de aproximadamente -20719.11 dólares. Portanto existe um ponto em que o efeito marginal da distância se torna negativo (turning point), ou seja, a partir desse ponto, aumentar a distância à autoestrada está associado a uma diminuição no preço da casa. Podemos calcular o ponto de inversão (máximo da função) do efeito marginal da distância resolvendo a equação:\n\\[\n\\frac{\\partial price}{\\partial inst} = 0  \\implies \\beta_1 + 2 \\beta_2 inst_i = 0 \\implies inst_i = -\\frac{\\beta_1}{2 \\beta_2}\n\\tag{17}\\]\nno R:\n\n# Calcular ponto de inversão\nponto_inversao &lt;- -coef(modelo_nao_linear_km)[\"inst_km\"] / (2 * coef(modelo_nao_linear_km)[\"I(inst_km^2)\"])\nponto_inversao\n\ninst_km \n5.77117 \n\n\nA partir da distância de aproximadamente 5.77 km, o efeito marginal da distância no preço da casa torna-se negativo, o que indica que aumentar a distância à autoestrada está associado a uma diminuição no preço da casa. Isto faz todo o sentido, pois se a casa está demasiado perto da autoestrada, o ruído, a poluição e o trãnsito podem diminuir o valor da casa. No entanto, se a casa está demasiado longe da autoestrada, a acessibilidade pode ser um fator negativo para o valor da casa.\nPodemos calcular o valor máximo do preço da casa substituindo o ponto de inversão na equação do modelo:\n\n# Calcular valor máximo do preço da casa\nvalor_maximo_preco &lt;- predict(modelo_nao_linear_km, newdata = data.frame(inst_km = ponto_inversao))\nvalor_maximo_preco\n\n inst_km \n116026.7 \n\n\nA mesma abordagem pode ser seguida para modelos com mais variáveis, por exemplo, incluindo a variável area (área da casa) e a variável rooms (número de quartos):\n\\[\n\\log(price_i) = \\beta_0 + \\beta_1 inst_i + \\beta_2 inst_i^2 + \\beta_3 area_i + \\beta_4 rooms_i + u_i\n\\tag{18}\\]\nno R:\n\n# Estimar modelo de regressão não linear com mais variáveis\nmodelo_nao_linear_2 &lt;- lm(log(price) ~ inst_km + I(inst_km^2)\n                            + area + rooms, data = hprice3)\nsummary(modelo_nao_linear_2)\n\n\nCall:\nlm(formula = log(price) ~ inst_km + I(inst_km^2) + area + rooms, \n    data = hprice3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.27199 -0.17547 -0.01315  0.22890  0.81846 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.947e+00  1.299e-01  76.583  &lt; 2e-16 ***\ninst_km       1.668e-01  2.890e-02   5.773 1.86e-08 ***\nI(inst_km^2) -1.366e-02  2.706e-03  -5.046 7.61e-07 ***\narea          2.989e-04  3.018e-05   9.906  &lt; 2e-16 ***\nrooms         6.256e-02  2.365e-02   2.645  0.00858 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3093 on 316 degrees of freedom\nMultiple R-squared:  0.508, Adjusted R-squared:  0.5017 \nF-statistic: 81.56 on 4 and 316 DF,  p-value: &lt; 2.2e-16\n\n# Calcular ponto de inversão\nponto_inversao_2 &lt;- -coef(modelo_nao_linear_2)[\"inst_km\"] / (2 * coef(modelo_nao_linear_2)[\"I(inst_km^2)\"])\n\nponto_inversao_2\n\n inst_km \n6.107991 \n\n\nA interpretação dos coeficientes é a seguinte:\n\n\\(\\beta_0\\) (constante): O preço médio de uma casa localizada exatamente na autoestrada (distância = 0 km), com área igual a 0 e 0 quartos é de aproximadamente 9947 dólares. Neste caso a constante não tem uma interpretação prática, pois é praticamente uma casa estar na autoestrada, com área igual a 0 e 0 quartos.\n\\(\\beta_1\\) (inst_km): Cada quilómetro adicional de distância à autoestrada está associado a um aumento médio de aproximadamente 0.17 dólares no preço da casa, ceteris paribus.\n\\(\\beta_2\\) (I(inst_km^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -0.01, o que indica que para cada quilómetro de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 0.01 dólares, ceteris paribus.\n\\(\\beta_3\\) (area): Cada unidade adicional de área da casa está associada a um aumento médio de aproximadamente 0.002 dólares no preço da casa, ceteris paribus.\n\\(\\beta_4\\) (rooms): Cada quarto adicional na casa está associado a um aumento médio de aproximadamente 0.06 dólares no preço da casa, ceteris paribus.\n\nO ceteris paribus é interpretado como “para uma casa com as mesmas características, com exceção da variável em questão”.\nO ponto de inversão (máximo da função) do efeito marginal da distância é de aproximadamente 6.11 km. É a partir desta distância que o efeito marginal da distância no preço da casa se torna negativo.\nPodemos ter também o termo ao cubo. Ao aumentar o grau do polinómio, o modelo torna-se mais flexível, podendo capturar relações mais complexas entre as variáveis. No entanto, também pode levar a problemas de overfitting, onde o modelo se ajusta demasiado aos dados e não pode ser generalizado para novos dados. Por isso, temos de mantar sempre um equilíbrio entre a complexidade e a interpretação do modelo.\n\n\n\nPara calcular o desvio padrão robusto para a heterocedasticidade no R, podemos utilizar a função vcovHC() do pacote sandwich. A função vcovHAC() do mesmo pacote permite calcular o desvio padrão robusto a autocorrelação e heterocedasticidade. A função coeftest() do pacote lmtest pode ser utilizada para apresentar os resultados da regressão com o desvio padrão robusto.\nOs argumentos da função vcovHC() permitem especificar o tipo de matriz de covariância robusta a ser utilizada. O argumento type pode assumir os seguintes valores:\n\n\"HC0\": Matriz de covariância robusta de White (1980).\n\"HC1\": Matriz de covariância robusta de MacKinnon e White (1985), que ajusta a matriz de covariância de White para amostras pequenas.\n\"HC2\": Matriz de covariância robusta de Long e Ervin (1983), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras.\n\"HC3\": Matriz de covariância robusta de Davidson e MacKinnon (1993), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras e o número de observações.\n\"HC4\": Matriz de covariância robusta de Cribari-Neto (2004), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras e o número de observações, com um ajuste adicional para observações influentes.\n\"HC4m\": Matriz de covariância robusta de Cribari-Neto (2004), que é uma versão modificada da HC4, com um ajuste adicional para observações influentes.\n\"HC5\": Matriz de covariância robusta de Pustejovsky e Tipton (2018), que é uma versão modificada da HC4, com um ajuste adicional para observações influentes, considerando o número de regressoras e o número de observações.\n\nA função vcovHAC() permite especificar o número de desfasamentos a considerar na matriz de covariância robusta para autocorrelação e heterocedasticidade, através do argumento lag.\n\n\n\n\n\n\nfunção gls() do pacote nlme permite estimar modelos de regressão linear com erros que podem ter diferentes estruturas de correlação e variância. A função gls() é particularmente útil quando os pressupostos clássicos da regressão linear (como homocedasticidade e independência dos erros) não são verificados."
  },
  {
    "objectID": "capitulo-06-extensoes-regressao.html#modelos-com-variáveis-dummy",
    "href": "capitulo-06-extensoes-regressao.html#modelos-com-variáveis-dummy",
    "title": "Extensões dos Modelos de Regressão",
    "section": "",
    "text": "Uma variável dummy é uma variável que assume valores binários (0 ou 1) que indica apresença ou não de uma característica específica. Por exemplo, uma variável dummy pode ser usada para indicar a presença de uma determinada condição (sim ou não) ou para categorias. Por exemplo, uma variável dummy pode ser usada para indicar se uma empresa está localizada numa determinada região (1) ou não (0), ou em várias regiões (mais de duas categorias).\n\n\nAs variáveis dummy são variáveis binárias que assumem valores 0 ou 1.\nUma dummmy, é geralmente representada por:\n\\[\nD_i = \\begin{cases}\n1, & \\text{se a condição é satisfeita} \\\\\n0, & \\text{a condição não é satisfeita}\n\\end{cases}\n\\]\nNum modelo de regressão, uma variável dummy pode ser incluída como uma variável explicativa para capturar o efeito de uma característica qualitativa na variável dependente:\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\beta_2 D_i + u_i\n\\tag{1}\\]\nÉ necessário definir muito bem o que é o 0 ou o que e o 1., pois vai mudar a interpretação dos coeficientes no modelo. Para este exemplo vamos utilizar os dados wage2 da biblioteca wooldridge, que contém informações sobre o salário, bem como outras características como anos de experiência, anos na empresa e outras características. O conjunto de dados contém variáveis dummy em que uma delas (married), que indica se o funcionário é casado (1) ou não (0). No R uma variável não tem de ser necessariamente binária para ser tratada como dummy, o R cria automaticamente as dummies para variáveis categóricas que são fatores (para o R). Vamos estimar a regressão do salário em função dos anos de educação, anos de experiência e se é casado ou não:\n\\[\nwage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 married_i + u_i\n\\tag{2}\\]\nA interpretação do coeficientes da dummy (\\(\\beta_3\\)) é a diferença média no salário entre indivíduos casados e não casados (\\(married = 1\\) e \\(married = 0\\)), mantendo a educação e a experiência constantes. Ou seja, depende das unidades da variável dependente (neste caso, salário em dólares americanos).\n\nlibrary(tidyverse)\nlibrary(wooldridge)\ndata(\"wage2\")\n\n# Estimar modelo de regressão\nmodelo_dummy &lt;- lm(wage ~ educ + exper + married, data = wage2)\nsummary(modelo_dummy)\n\n\nCall:\nlm(formula = wage ~ educ + exper + married, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-864.16 -241.25  -39.12  194.22 2145.23 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -427.775    111.109  -3.850 0.000126 ***\neduc          76.550      6.227  12.293  &lt; 2e-16 ***\nexper         16.317      3.139   5.198 2.48e-07 ***\nmarried      185.907     39.604   4.694 3.08e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 372.1 on 931 degrees of freedom\nMultiple R-squared:  0.1558,    Adjusted R-squared:  0.1531 \nF-statistic: 57.29 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretação dos coeficientes:\n\n\\(\\beta_0\\) (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência e que não é casado é de -427.76 US$.\n\\(\\beta_1\\) (educ): Cada ano adicional de educação está associado a um aumento médio de 76.55 US$ no salário, ceteris paribus.\n\\(\\beta_2\\) (exper): Cada ano adicional de experiência está associado a um aumento médio de 16.32 US$ no salário, ceteris paribus.\n\\(\\beta_3\\) (married): um indivíduo casado ganha, em média, 185.91 US$ a mais do que um indivíduo não casado, ceteris paribus.\n\nPara um regressão simples, apenas com a variável dummy como independente, o valor da constante é o valor médio da variável dependente quando a dummy é 0. Considerando o modelo:\n\n# Estimar modelo de regressão com apenas a dummy\nmodelo_dummy_s &lt;- lm(wage ~ married, data = wage2)\nsummary(modelo_dummy_s)\n\n\nCall:\nlm(formula = wage ~ married, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-862.05 -284.05  -51.05  210.95 2100.95 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   798.44      40.08  19.922  &lt; 2e-16 ***\nmarried       178.61      42.41   4.211 2.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 400.8 on 933 degrees of freedom\nMultiple R-squared:  0.01865,   Adjusted R-squared:  0.0176 \nF-statistic: 17.74 on 1 and 933 DF,  p-value: 2.784e-05\n\n\nO salário médio para indivíduos não casados é de 798.44 US$. E qual o salário médio para indicíduos casados? Podemos calculamos o valor do salário para quando \\(married = 1\\) considerando os coeficientes do modelo:\n\n# Calcular salário médio para indivíduos casados\nsalario_casado &lt;- predict(modelo_dummy_s, newdata = data.frame(married = 1))\nsalario_casado\n\n       1 \n977.0479 \n\n\nO salário médio para indivíduos casados é em média de 977.05 US$. Apenas para confirmar, podemos calcular a média diretamente com uma subamostra para cada grupo:\n\n# Calcular salário médio para indivíduos casados\n\n# Para indivíduos não casados (married = 0)\nsalario_nao_casado &lt;- wage2 %&gt;% \n    filter(married == 0) %&gt;% \n    summarise(salario_medio = mean(wage))\nsalario_nao_casado\n\n  salario_medio\n1        798.44\n\n# Para indivíduos casados (married = 1)\nsalario_casado &lt;- wage2 %&gt;% \n    filter(married == 1) %&gt;% \n    summarise(salario_medio = mean(wage))\nsalario_casado\n\n  salario_medio\n1      977.0479\n\n\nAtravé do gráfico podemos visualizar a diferença nos salários entre indivíduos casados e não casados:\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n\n\n\nO ponto vermelho indica a média do salário para cada grupo.\nPodemos converter a variável married para non-married (0 = casado, 1 = não casado) e re-estimar o modelo:\n\n# Converter variável married para non-married\nwage2$non_married &lt;- ifelse(wage2$married == 1, 0, 1)\n\n# Estimar modelo de regressão com a nova variável\nmodelo_dummy_non_married &lt;- lm(wage ~ non_married,\n                                data = wage2)\n\n#Comparar modelos\nlibrary(stargazer)\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\nstargazer(modelo_dummy_s,\n        modelo_dummy_non_married,\n        type = \"text\")\n\n\n===========================================================\n                                   Dependent variable:     \n                               ----------------------------\n                                           wage            \n                                    (1)            (2)     \n-----------------------------------------------------------\nmarried                          178.608***                \n                                  (42.411)                 \n                                                           \nnon_married                                    -178.608*** \n                                                (42.411)   \n                                                           \nConstant                         798.440***    977.048***  \n                                  (40.079)      (13.870)   \n                                                           \n-----------------------------------------------------------\nObservations                        935            935     \nR2                                 0.019          0.019    \nAdjusted R2                        0.018          0.018    \nResidual Std. Error (df = 933)    400.786        400.786   \nF Statistic (df = 1; 933)        17.736***      17.736***  \n===========================================================\nNote:                           *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nPodemos ver que o coeficiente da dummy mudou de sinal, mas a interpretação alterou de a diferença média no salário entre indivíduos casados e não casados (modelo_dummy_s) para a diferença média no salário entre indivíduos não casados e casados (modelo_dummy_non_married). A constante do segundo modelo é o salário médio para indivíduos casados (quando non_married = 0).\nQuando a variável dependete está em logaritmos, o coeficiente da dummy pode ser interpretado como uma diferença percentual aproximada. Por exemplo para o modelo:\n\\[\n\\log(wage_i) = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 married_i + u_i\n\\tag{3}\\]\nNo R:\n\n# Estimar modelo de regressão com log do salário\nmodelo_dummy_log &lt;- lm(log(wage) ~ educ + exper + married,\n                        data = wage2)\nsummary(modelo_dummy_log)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + married, data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.89479 -0.23544  0.02585  0.25920  1.27757 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.327960   0.115832  45.997  &lt; 2e-16 ***\neduc        0.078158   0.006492  12.039  &lt; 2e-16 ***\nexper       0.018290   0.003273   5.588 3.01e-08 ***\nmarried     0.209262   0.041288   5.068 4.84e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3879 on 931 degrees of freedom\nMultiple R-squared:  0.1542,    Adjusted R-squared:  0.1515 \nF-statistic: 56.58 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nPortanto a variação percentual aproximada é de 20.9% \\(100 \\times \\beta_{married}\\). Neste caso, indivíduos casados ganham, em média, cerca de 20.9% a mais do que indivíduos não casados, ceteris paribus. Para uma interpretação mais precisa, podemos usar a fórmula de @halvorsen_interpretation_1980:\n\\[\n\\text{Variação Percentual} = (e^{\\beta_3} - 1) \\times 100\n\\]\nno R é necessário extrair o coefiente do modelo e calcular o valor exponencial:\n\n# Extrair coeficiente da dummy\nbeta_married &lt;- coef(modelo_dummy_log)[\"married\"]\n# Calcular Variação percentual\nvariacao_percentual &lt;- (exp(beta_married) - 1) * 100\nvariacao_percentual\n\n married \n23.27677 \n\n\nEm média, indivíduos casados ganham cerca de 23.28% a mais do que indivíduos não casados, ceteris paribus.\nou então pelo método alternativo sugerido por @kennedy_estimation_1981:\n\\[\n\\text{Variação Percentual} \\approx 100 \\times \\left(\\frac{e^{\\beta_3} - 1}{1}\\right)\n\\]\nEste método é especialmente útil quando o coeficiente é grande (geralmente maior que 0.1 ou 10%) e para amostras pequenas, pois leva em consideração a variância do estimador:\n\\[\n\\Delta Y \\approx \\left(e^{\\beta - \\frac{1}{2} \\cdot \\text{Var}(\\beta)} - 1\\right) \\times 100\n\\]\nNo R, a variância do coeficiente pode ser obtida a partir da matriz de variância-covariância do modelo:\n\n# Obter variância do coeficiente\nvar_beta_married &lt;- vcov(modelo_dummy_log)[\"married\", \"married\"]\n# Calcular Variação percentual ajustada\ndelta_y &lt;- (exp(beta_married - 0.5 * var_beta_married) - 1) * 100\ndelta_y\n\n married \n23.17174 \n\n\nNeste caso, a variação percentual ajustada é de 23.17%, que é ligeiramente diferente da estimativa anterior.\n\n\n\nAs variáveis dummy também podem ser usadas para representar variáveis categóricas. Por exemplo, se tivermos uma variável multinominal que indica a cor dos carros vendidos (vermelho, azul, verde), podemos criar dummies para cada categoria:\n\\[\nD_{vermelho} = \\begin{cases}  \n1, & \\text{se o carro é vermelho} \\\\\n0, & \\text{não é vermelho}\n\\end{cases}\n\\tag{4}\\]\n\\[\nD_{azul} = \\begin{cases}\n1, & \\text{se o carro é azul} \\\\\n0, & \\text{não é azul}\n\\end{cases}\n\\tag{5}\\]\n\\[\nD_{verde} = \\begin{cases}\n1, & \\text{se o carro é verde} \\\\\n0, & \\text{não é verde}\n\\end{cases}\n\\tag{6}\\]\nExemplo no R:\n\n#introduzir dados\ncores &lt;- c(\"vermelho\", \"azul\", \"verde\", \"vermelho\",\n            \"verde\", \"azul\", \"vermelho\", \"verde\", \n            \"azul\", \"vermelho\")\npreço &lt;- c(50000, 52000, 48000, 51000, 49000, 53000, \n            50000, 52000, 48000, 51000)\n\ndados &lt;- data.frame(cores, preço)\n\n#para tabela resumo\nlibrary(gtsummary)\n\n# Criar dummies e taela\ndados |&gt;\n    mutate(\n        vermelho = ifelse(cores == \"vermelho\", 1, 0),\n        azul = ifelse(cores == \"azul\", 1, 0),\n        verde = ifelse(cores == \"verde\", 1, 0)\n    ) |&gt;\n    select(cores, vermelho, azul, verde) |&gt;\n    tbl_summary(\n        by = cores)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nazul\nN = 31\nverde\nN = 31\nvermelho\nN = 41\n\n\n\n\nvermelho\n0 (0%)\n0 (0%)\n4 (100%)\n\n\nazul\n3 (100%)\n0 (0%)\n0 (0%)\n\n\nverde\n0 (0%)\n3 (100%)\n0 (0%)\n\n\n\n1 n (%)\n\n\n\n\n\n\n\n\nA função ifelse() é usada para criar as variáveis dummy. A sintaxe é ifelse(condição, valor_se_condicao_verificada, valor_se_condicao_nao_verificada), ver ?ifelse para mais detalhes.\nPara um exemplo real vamos usar o conjunto de dados ceosal1 da biblioteca wooldridge, que contém informações sobre o salário dos CEOs de várias empresas, bem como outras características como anos de experiência, anos na empresa dummies que ondicam o setor de atividade da empresa:\n\nfinance - empresa do setor financeiro (1) ou não (0)\nindus - empresa do setor industrial (1) ou não (0)\nconsprod - empresa do setor de bens de consumo (1) ou não (0)\nutility - empresa do setor de serviços essenciais (1) ou não (0)\n\nNeste conjunto não é necessário calcular as dummies, pois já estão incluídas (o que nem sempre acontece). Vamos estimar um modelo de regressão do logaritmo do salário dos CEOs (lsalary) em função do logaritmo vendas (lsales), da rendibilidade dos capitais próprios (roe`) e o setor de atividade da empresa:\n\\[\n\\begin{split}\n\\log(salary_i) = \\beta_0 &+ \\beta_1 \\log(sales_i) + \\beta_2 roe_i + \\beta_3 D_{finance} + \\\\\n&\\beta_4 D_{indus} + \\beta_5 D_{consprod} + \\beta_6 D_{utility} + u_i\n\\end{split}\n\\tag{7}\\]\nNo R:\n\nlibrary(wooldridge)\ndata(\"ceosal1\")\n# Estimar modelo de regressão\nmodelo_dummy_mult &lt;- lm(lsalary ~ lsales + roe + finance + indus + consprod + utility,\n                        data = ceosal1)\nsummary(modelo_dummy_mult)\n\n\nCall:\nlm(formula = lsalary ~ lsales + roe + finance + indus + consprod + \n    utility, data = ceosal1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09465 -0.22173 -0.01973  0.17141  2.64394 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.30510    0.28156  15.290  &lt; 2e-16 ***\nlsales       0.25719    0.03204   8.029 7.85e-14 ***\nroe          0.01115    0.00430   2.594   0.0102 *  \nfinance      0.44096    0.10365   4.254 3.20e-05 ***\nindus        0.28300    0.09923   2.852   0.0048 ** \nconsprod     0.46389    0.10887   4.261 3.12e-05 ***\nutility           NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4598 on 203 degrees of freedom\nMultiple R-squared:  0.3569,    Adjusted R-squared:  0.341 \nF-statistic: 22.53 on 5 and 203 DF,  p-value: &lt; 2.2e-16\n\n\nNa regressão anterior o coeficiente da dummy utility aparece com valor NA, porque existe multicolinearidade perfeita entre as dummies (todas somadas é obtida uma coluna de 1’s, que é uma constante).\n\n# Verificar multicolinearidade perfeita\nceosal1 %&gt;%\n    mutate(soma_dummies = finance + indus + consprod + utility) %&gt;%\n    summarise(min_soma = min(soma_dummies),\n              max_soma = max(soma_dummies),\n              unique_somas = n_distinct(soma_dummies))\n\n  min_soma max_soma unique_somas\n1        1        1            1\n\n\nEste fenómeno é conhecido como a armadilha das dummies. Em alguns softwares econométrico pode aparecer uma mensagem de erro a indicar que existe multicolinearidade perfeita. Para evitar este problema, uma das categorias deve ser excluída do modelo, ou seja, o modelo deve incluir apenas \\(k-1\\) dummies (com \\(k\\) o número de categorias). A categoria excluída é a categoria de referência (base) e as outras dummies medem o efeito relativo em comparação com essa mesma categoria. De referir que nesta amostra cada empresa pertence a apenas um setor. Neste caso, podemos excluir a dummy utility e re-estimar o modelo:\n\\[\n\\begin{split}\n\\log(salary_i) = \\beta_0 &+ \\beta_1 \\log(sales_i) + \\beta_2 roe_i + \\beta_3 D_{finance} + \\\\\n& \\beta_4 D_{indus} + \\beta_5 D_{consprod} + u_i\n\\end{split}\n\\tag{8}\\]\nNo R:\n\n# Estimar modelo de regressão sem a dummy utility\nmodelo_dummy_mult2 &lt;- lm(lsalary ~ lsales + roe + finance + indus + consprod,\n                         data = ceosal1)\nsummary(modelo_dummy_mult2)\n\n\nCall:\nlm(formula = lsalary ~ lsales + roe + finance + indus + consprod, \n    data = ceosal1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09465 -0.22173 -0.01973  0.17141  2.64394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.30510    0.28156  15.290  &lt; 2e-16 ***\nlsales       0.25719    0.03204   8.029 7.85e-14 ***\nroe          0.01115    0.00430   2.594   0.0102 *  \nfinance      0.44096    0.10365   4.254 3.20e-05 ***\nindus        0.28300    0.09923   2.852   0.0048 ** \nconsprod     0.46389    0.10887   4.261 3.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4598 on 203 degrees of freedom\nMultiple R-squared:  0.3569,    Adjusted R-squared:  0.341 \nF-statistic: 22.53 on 5 and 203 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretação dos coeficientes :\n\n\\(\\beta_0\\) (constante): O salário médio para uma empresa do setor de serviços essenciais (categoria de referência) com vendas iguais a 1 (log(1) = 0) e rendibilidade dos capitais próprios igual a 0 é de 257 US$.\n\\(\\beta_1\\) (lsales): Um aumento de 1% nas vendas está associado a um aumento médio de 25.7% no salário do CEO, ceteris paribus.\n\\(\\beta_2\\) (roe): Um aumento de 1 ponto percentual na rendibilidade dos capitais próprios está associado a um aumento médio de aproximadamente 1.1% no salário do CEO, ceteris paribus (a variável roe está expressa em percentagem, 0-100).\n\\(\\beta_3\\) (finance): CEOs de empresas do setor financeiro ganham, aproximadamente em média, cerca de 44.1% a mais do que CEOs de empresas do setor de serviços essenciais (categoria de referência), ceteris paribus.\n\\(\\beta_4\\) (indus): CEOs de empresas do setor industrial ganham, aproximadamente em média, cerca de 28.3% a mais do que CEOs de empresas do setor de serviços essenciais, ceteris paribus.\n\\(\\beta_5\\) (consprod): CEOs de empresas do setor de bens de consumo ganham, aproximadamente em média, cerca de 3.3% a mais do que CEOs de empresas do setor de serviços essenciais, ceteris paribus.\n\npodemos considerar a dummy indus como a categoria de referência, excluindo-a do modelo:\n\n# Estimar modelo de regressão sem a dummy indus\nmodelo_dummy_mult3 &lt;- lm(lsalary ~ lsales + roe + finance + consprod + utility,\n                         data = ceosal1)\n\nlibrary(stargazer)\nstargazer(modelo_dummy_mult2,\n        modelo_dummy_mult3,\n        type = \"text\")\n\n\n===========================================================\n                                   Dependent variable:     \n                               ----------------------------\n                                         lsalary           \n                                    (1)            (2)     \n-----------------------------------------------------------\nlsales                            0.257***      0.257***   \n                                  (0.032)        (0.032)   \n                                                           \nroe                               0.011**        0.011**   \n                                  (0.004)        (0.004)   \n                                                           \nfinance                           0.441***       0.158*    \n                                  (0.104)        (0.089)   \n                                                           \nindus                             0.283***                 \n                                  (0.099)                  \n                                                           \nconsprod                          0.464***       0.181**   \n                                  (0.109)        (0.085)   \n                                                           \nutility                                         -0.283***  \n                                                 (0.099)   \n                                                           \nConstant                          4.305***      4.588***   \n                                  (0.282)        (0.295)   \n                                                           \n-----------------------------------------------------------\nObservations                        209            209     \nR2                                 0.357          0.357    \nAdjusted R2                        0.341          0.341    \nResidual Std. Error (df = 203)     0.460          0.460    \nF Statistic (df = 5; 203)        22.529***      22.529***  \n===========================================================\nNote:                           *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nPodemos observar que do modelo modelo_dummy_mult2 e modelo_dummy_mult3 que os coeficientes das dummies dos setores alteraram, pois a categoria de referência mudou. O coeficiente da dummy indus no primeiro modelo é o inverso do coeficiente da dummy utility no segundo modelo. Tal como na regressão Equation 2, também é possível calcular a variação de salário em percentagem para as dummies do modelo Equation 8, usando a fórmula de @halvorsen_interpretation_1980 ou o método alternativo sugerido por @kennedy_estimation_1981.\n\n# Extrair coeficientes das dummies\nbeta_finance &lt;- coef(modelo_dummy_mult2)[\"finance\"]\nbeta_indus &lt;- coef(modelo_dummy_mult2)[\"indus\"]\nbeta_consprod &lt;- coef(modelo_dummy_mult2)[\"consprod\"]\n# Calcular variação percentual usando a fórmula de Halvorsen e Palmquist (1980)\nvariacao_percentual_finance &lt;- (exp(beta_finance) - 1) * 100\nvariacao_percentual_indus &lt;- (exp(beta_indus) - 1) * 100\nvariacao_percentual_consprod &lt;- (exp(beta_consprod) - 1) * 100\n\n#calcular variação percentual ajustada usando o método de Kennedy (1981)\nvar_beta_finance &lt;- vcov(modelo_dummy_mult2)[\"finance\", \"finance\"]\nvar_beta_indus &lt;- vcov(modelo_dummy_mult2)[\"indus\", \"indus\"]\nvar_beta_consprod &lt;- vcov(modelo_dummy_mult2)[\"consprod\", \"consprod\"]\ndelta_y_finance &lt;- (exp(beta_finance - 0.5 * var_beta_finance) - 1) * 100\ndelta_y_indus &lt;- (exp(beta_indus - 0.5 * var_beta_indus) - 1) * 100\ndelta_y_consprod &lt;- (exp(beta_consprod - 0.5 * var_beta_consprod) - 1) * 100\n\n#organizar em tabela:\ntabela_variacao &lt;- data.frame(\n    Categoria = c(\"finance\", \"indus\", \"consprod\"),\n    Variacao_Percentual = c(variacao_percentual_finance,\n                            variacao_percentual_indus,\n                            variacao_percentual_consprod),\n    Variacao_Percentual_Ajustada = c(delta_y_finance,\n                                    delta_y_indus,\n                                    delta_y_consprod)\n)\ntabela_variacao\n\n         Categoria Variacao_Percentual Variacao_Percentual_Ajustada\nfinance    finance            55.41952                     54.58691\nindus        indus            32.71071                     32.05889\nconsprod  consprod            59.02531                     58.08560\n\n\nA interpretação é semelhantes ao exemplo anterior. Também é possível utilizar variáveis dummy como variável independente. Esta abordagem será discutida na ?@sec-variavel-dependente-limitada."
  },
  {
    "objectID": "capitulo-06-extensoes-regressao.html#modelos-com-termos-de-interação",
    "href": "capitulo-06-extensoes-regressao.html#modelos-com-termos-de-interação",
    "title": "Extensões dos Modelos de Regressão",
    "section": "",
    "text": "Um termo de interação é utilizado para capturar o efeito conjunto de duas ou mais variáveis independentes na variável dependente. Portanto num exemplo simples, um termo de interação entre duas variáveis \\(X_1\\) e \\(X_2\\) é representado como \\(X_1 \\times X_2\\). O termo de interação permite que o efeito de uma variável independente dependa do valor de outra variável independente.\n\\[\ny_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 (X_{1i} \\times X_{2i}) + u_i\n\\tag{9}\\]\nPodemos criar termos de interaão entre variáveis contínuas, entre variáveis dummy e entre variáveis dummy e contínuas.\n\n\nPara esta apicação vamos utilizar o conjunto de dados wage2 da biblioteca wooldridge. Mais informação sobre o conjunto de dados pode ser obtida com ?wage2.\nVamos estimar a regressão do salário em função dos anos de educação, anos de experiência e a interacção entre educação e experiência:\n\\[\nwage_i = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 (educ_i \\times exper_i) + u_i\n\\tag{10}\\]\nO termo de interação pode ser crido no data.frame (mutate(educ_exper = educ* exper)) ou diretamente na fórmula do modelo. No R:\n\nlibrary(tidyverse)\nlibrary(wooldridge)\n\n# carregar dados\ndata(\"wage2\")\n\n#Estimar modelo de regressão com interacção\nmodelo_interacoes &lt;- lm(wage ~ educ + exper + I(educ * exper), data = wage2)\nsummary(modelo_interacoes)\n\n\nCall:\nlm(formula = wage ~ educ + exper + I(educ * exper), data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-948.46 -254.53  -29.57  192.59 2150.77 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)      271.934    230.227   1.181  0.23784   \neduc              35.106     16.626   2.112  0.03499 * \nexper            -32.663     19.099  -1.710  0.08757 . \nI(educ * exper)    3.904      1.462   2.670  0.00771 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 375.1 on 931 degrees of freedom\nMultiple R-squared:  0.1424,    Adjusted R-squared:  0.1397 \nF-statistic: 51.54 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nO I é necessário para indicar ao R que a operação deve ser interpretada literalmente, ou seja, como uma multiplicação entre as variáveis educ e exper. Sem o I, o R pode interpretar o símbolo * como um operador especial na fórmula do modelo, o que pode levar a resultados inesperados.\nNa interpretação dos coeficientes é necessário ter alguma atenção pois o coeficiente de cada variável que também faz parte do termo de interação depende do valor da outra variável. A interpretação dos coeficientes é a seguinte:\n\n\\(\\beta_0\\) (constante): O salário médio para alguém com 0 anos de educação e 0 anos de experiência é de 271.934 US$. Pode não ter uma interpretação prática, pois é improvável que alguém tenha 0 anos de educação e experiência.\n\\(\\beta_1\\) (educ): Cada ano adicional de educação está associado a um aumento médio de 35.106 US$ no salário, ceteris paribus, para alguém sem experiência (exper = 0).\n\\(\\beta_2\\) (exper): Cada ano adicional de experiência está associado a uma diminuição média de 32.663 US$ no salário, ceteris paribus, para alguém sem educação (educ = 0).\n\\(\\beta_3\\) (I(educ * exper)): O coeficiente de interação indica que o efeito combinado de educação e experiência no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito da experiência no salário aumenta em 3.904 US$, e vice-versa.\n\nNum caso em que o termo de interação é negativo, por exemplo de -3.904, indicaria que o efeito combinado de educação e experiência no salário é negativo, ou seja, para cada ano adicional de educação, o efeito da experiência no salário diminui em 3.904 US$, e vice-versa.\nPara calcular o efeito marginal da educação no salário para diferentes níveis de experiência. O efeito marginal da educação é dado por:\n\\[\n\\frac{\\partial wage}{\\partial educ} = \\beta_1 + \\beta_3 \\times exper\n\\] Portanto, o efeito marginal da educação varia com o nível de experiência. Podemos calcular o efeito marginal da educação para diferentes níveis de experiência (0, 5, 10, 20 anos):\n\n# Níveis de experiência para calcular o efeito marginal\nniv_experiencia &lt;- c(0, 5, 10, 20)\n# Coeficientes do modelo\ncoeficientes &lt;- coef(modelo_interacoes)\n# Calcular efeito marginal da educação para diferentes níveis de experiência\nefeito_marginal_educ &lt;- coeficientes[\"educ\"] +\n                coeficientes[\"I(educ * exper)\"] * niv_experiencia\n# Criar data frame com os resultados obtidos\ndata.frame(Nivel_Experiencia = niv_experiencia,\n            Efeito_Marginal_Educacao = efeito_marginal_educ)\n\n  Nivel_Experiencia Efeito_Marginal_Educacao\n1                 0                 35.10600\n2                 5                 54.62379\n3                10                 74.14158\n4                20                113.17716\n\n\nPara um modelo onde a variável dependente está em logaritmo natural, o coeficiente da variável contínua no termo de interação pode ser interpretado como uma variação percentual aproximada. Por exemplo, para o modelo:\n\nmodelo_interacoes_log &lt;- lm(log(wage) ~ educ + exper + I(educ * exper), data = wage2)\nsummary(modelo_interacoes_log)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + I(educ * exper), data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.88558 -0.24553  0.03558  0.26171  1.28836 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      5.949455   0.240826  24.704   &lt;2e-16 ***\neduc             0.044050   0.017391   2.533   0.0115 *  \nexper           -0.021496   0.019978  -1.076   0.2822    \nI(educ * exper)  0.003203   0.001529   2.095   0.0365 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3923 on 931 degrees of freedom\nMultiple R-squared:  0.1349,    Adjusted R-squared:  0.1321 \nF-statistic: 48.41 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\n\nconstante: O salário médio para alguém com 0 anos de educação e 0 anos de experiência é de 6.579 US$ (log(719.82)).\neduc: Cada ano adicional de educação está associado a um aumento médio de aproximadamente 4.4% no salário, ceteris paribus, para alguém sem experiência (exper = 0).\nexper: Cada ano adicional de experiência está associado a uma diminuição média de aproximadamente 2.15% no salário, ceteris paribus, para alguém sem educação (educ = 0).\nI(educ * exper): O coeficiente de interação indica que o efeito combinado de educação e experiência no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito da experiência no salário aumenta em aproximadamente 0.32%, e vice-versa.\n\nDe notar que o coeficiente da variável exper não é estatisticamente diferente de zero para nenhum nível de significância estatística.\n\n\n\nRecorrendo ao conjunto de dados anterior (wage2), onde a variável married indica se o indivíduo é casado (1) ou não (0) e a variável dummy urban indica se o indivíduo vive numa área urbana (1) ou não (0), vamos estimar o modelo de regressão com termos de interação entre variáveis dummy:\n\\[\n\\log(wage_i) = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 married_i + \\beta_4 urban_i + \\beta_5 (married_i \\times urban_i) + u_i\n\\tag{11}\\]\nno R:\n\n#estimar modelo de regressão com interacção entre dummies\nmodelo_interacoes_dummy &lt;- lm(log(wage) ~ educ + exper + married + \n                            urban + I(married * urban), data = wage2)\nsummary(modelo_interacoes_dummy)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + married + urban + I(married * \n    urban), data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.94789 -0.22230  0.02629  0.24607  1.22611 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.198499   0.134840  38.553  &lt; 2e-16 ***\neduc                0.075924   0.006374  11.911  &lt; 2e-16 ***\nexper               0.018587   0.003207   5.796 9.29e-09 ***\nmarried             0.240197   0.083079   2.891  0.00393 ** \nurban               0.204342   0.090328   2.262  0.02391 *  \nI(married * urban) -0.028581   0.094926  -0.301  0.76341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3799 on 929 degrees of freedom\nMultiple R-squared:  0.1905,    Adjusted R-squared:  0.1861 \nF-statistic: 43.71 on 5 and 929 DF,  p-value: &lt; 2.2e-16\n\n\ninterpretação dos coeficientes: - \\(\\beta_0\\) (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência, que não é casado e que não vive numa área urbana é de 180.91 US$ (exp(5.198)).\n\n\\(\\beta_1\\) (educ): Cada ano adicional de educação está associado a um aumento médio de aproximadamente 7.5 % no salário, ceteris paribus.\n\\(\\beta_2\\) (exper): Cada ano adicional de experiência está associado a um aumento médio de aproximadamente 1.8% no salário, ceteris paribus.\n\\(\\beta_3\\) (married): Ser casado está associado a um aumento médio de aproximadamente 24.20% no salário, ceteris paribus, para alguém sem experiência e sem educação (exper = 0, educ = 0).\n\\(\\beta_4\\) (urban): Viver numa área urbana está associado a um aumento médio de aproximadamente 20.4.% no salário, ceteris paribus, para alguém sem experiência e sem educação (exper = 0, educ = 0).\n\\(\\beta_5\\) (married * urban): O coeficiente de interação indica que o efeito combinado de ser casado e viver em uma área urbana no salário é negativo. Especificamente, para indivíduos casados e viver numa área urbana, o efeito combinado no salário é de aproximadamente -2.86% em relação ao efeito individual de ser casado e viver numa área urbana.\n\nNo termo de interação entre variáveis dummy, neste caso married e urban, o 1 (da multiplicaão) corresponde a indivíduos que são casados e vivem numa área urbana (11), o 0 (da multiplicação) corresponde a indivíduos que não são casados e não vivem numa área urbana (00), e os outros dois casos (10 e 01) correspondem a indivíduos que são casados mas não vivem numa área urbana ou que não são casados mas vivem numa área urbana. Portanto, o coeficiente do termo de interação mede o efeito adicional de ser casado e viver numa área urbana em comparação com os efeitos individuais de ser casado e viver numa área urbana. Se criarmos uma tabela com os diferentes grupos, podemos ver melhor a interpretação de cada caso: ou seja:\n\n\n\n\n\n\n\n\n\n\n\nmarried\nurban\nInteração (= married × urban)\nlog(wage) previsto\nwage previsto (e^{})\nInterpretação\n\n\n\n\n0\n0\n0\n\\(\\beta_0\\)\n\\(e^{\\beta_0}\\)\nNão casado, não vive numa área urbana\n\n\n1\n0\n0\n\\(\\beta_0 + \\beta_3\\)\n\\(e^{\\beta_0 + \\beta_3}\\)\nCasado, não vive numa área urbana\n\n\n0\n1\n0\n\\(\\beta_0 + \\beta_4\\)\n\\(e^{\\beta_0 + \\beta_4}\\)\nNão casado, vive numa área urbana\n\n\n1\n1\n1\n\\(\\beta_0 + \\beta_3 + \\beta_4 + \\beta_5\\)\n\\(e^{\\beta_0 + \\beta_3 + \\beta_4 + \\beta_5}\\)\nCasado, vive numa área urbana\n\n\n\nDe notar que no modelo Equation 11 o coeficiente do termo de interação não é estatisticamente diferente de zero para nenhum nível de significância estatística. Ou seja, não existe evidência estatística de que o efeito combinado de ser casado e viver numa área urbana no salário seja diferente do efeito individual de ser casado e viver numa área urbana, o que também é um resultado interessante.\n\n\n\nPara esta aplicação vamos considerar o mesmo conjunto de dados wage2 e vamos estimar a regressão com o termo de interação entre a variável dummy south e a variável contínua educ:\n\\[\n\\log(wage_i) = \\beta_0 + \\beta_1 educ_i + \\beta_2 exper_i + \\beta_3 south_i + \\beta_4 (south_i \\times educ_i) + u_i\n\\tag{12}\\]\nno R:\n\n#carregar dados\ndata(\"wage2\")\n\n# Estimar modelo\nmodelo_interacoes_dummy_cont &lt;- lm(log(wage) ~ educ + exper + south +\n                                     I(south * educ), data = wage2)\n\nsummary(modelo_interacoes_dummy_cont)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + south + I(south * educ), \n    data = wage2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.93539 -0.23116  0.02579  0.25730  1.28557 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      5.699845   0.122511  46.525  &lt; 2e-16 ***\neduc             0.066953   0.007543   8.876  &lt; 2e-16 ***\nexper            0.019671   0.003256   6.041 2.21e-09 ***\nsouth           -0.464313   0.167538  -2.771  0.00569 ** \nI(south * educ)  0.024110   0.012422   1.941  0.05257 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3868 on 930 degrees of freedom\nMultiple R-squared:  0.1601,    Adjusted R-squared:  0.1564 \nF-statistic: 44.31 on 4 and 930 DF,  p-value: &lt; 2.2e-16\n\n\nEm que a interpretação dos coeficientes é a seguinte:\n\n\\(\\beta_0\\) (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência e que não vive no sul é de 298.87 US$ (exp(5.70)).\n\\(\\beta_1\\) (educ): Cada ano adicional de educação está associado a um aumento médio de aproximadamente 6.7% no salário, ceteris paribus, para alguém que não vive no sul (south = 0).\n\\(\\beta_2\\) (exper): Cada ano adicional de experiência está associado a um aumento médio de aproximadamente 2% no salário, ceteris paribus.\n\\(\\beta_3\\) (south): Viver no sul está associado a uma diminuição média de aproximadamente 46.4% no salário, ceteris paribus, para alguém sem anos de educação (educ = 0).\n\\(\\beta_4\\) (south * educ): O coeficiente de interação indica que o efeito combinado de viver no sul e anos de educação no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito de viver no sul no salário aumenta em aproximadamente 2.41%, ceteris paribus.\n\nO efeito marginal da educação no salário para diferentes níveis de experiência é dado por: \\[\n\\frac{\\partial \\log(wage)}{\\partial educ} = \\beta_1 + \\beta_4 \\times south\n\\tag{13}\\]\nPortanto, o efeito marginal da educação varia com o valor da variável dummy south. Podemos calcular o efeito marginal da educação para os dois grupos (south = 0 e south = 1):\n\n# Coeficientes do modelo\ncoeficientes &lt;- coef(modelo_interacoes_dummy_cont)\n# Calcular efeito marginal da educação para os dois grupos\nefeito_marginal_educacao &lt;- data.frame(\n  south = c(0, 1),\n  efeito_marginal = c(coeficientes[\"educ\"] + 0 * coeficientes[\"I(south * educ)\"],\n                      coeficientes[\"educ\"] + 1 * coeficientes[\"I(south * educ)\"])\n)\nefeito_marginal_educacao\n\n  south efeito_marginal\n1     0      0.06695309\n2     1      0.09106290\n\n\nPara indivíduos que não vivem no sul (south = 0), cada ano adicional de educação está associado a um aumento médio de aproximadamente 6.7% no salário, ceteris paribus. Para indivíduos que vivem no sul (south = 1), cada ano adicional de educação está associado a um aumento médio de aproximadamente 9.11% no salário, ceteris paribus."
  },
  {
    "objectID": "capitulo-06-extensoes-regressao.html#modelos-não-lineares",
    "href": "capitulo-06-extensoes-regressao.html#modelos-não-lineares",
    "title": "Extensões dos Modelos de Regressão",
    "section": "",
    "text": "Modelos não lineares são modelos em que a relação entre a variável dependente e uma ou mais variáveis independentes não é linear. A equação do modelo não linear para a variável dependente y e para as variáveis independentes X é dada por:\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{1i}^2 + u_i\n\\tag{14}\\]\nem que \\(\\beta_0\\) é a constante, \\(\\beta_1\\) é o coeficiente da variável independente x1, \\(\\beta_2\\) é o coeficiente do termo quadrático da variável independente x2 e \\(u_i\\) é o termo de erro. O modelo é estimado pelo método dos mínimos quadrados ordinários. Quando devemos estimar um modelo não linear? Uma abordagem comum é a análise gráfica. Se a relação entre a variável dependente e a variável independente não for aproximadamente linear, pode ser apropriado considerar um modelo não linear. Outra abordagem é incluir termos polinomiais (como o termo quadrático) na regressão e verificar se esses termos são estatisticamente diferentes de 0. Existem alguma forma de lidar com a não linearidade sem recorrer a modelos não lineares, como por exemplo, transformar as variáveis (logaritmos).\nUm exemplo de um modelo que poderá ser não linear é a relação entre o preço das casas e a distância à autoestrada mais próxima. Vamos considerar o conjunto de dados hprice2 da biblioteca wooldridge, que contém informações sobre o preço das casas (price), a distância à autoestrada mais próxima (dist) e outras características das casas. Mais informação sobre o conjunto de dados pode ser obtida com ?hprice2.\nPara esta aplicação vamos estimar o modelo:\n\\[\n\\log(price_i) = \\beta_0 + \\beta_1 dist_i + \\beta_2 dist_i^2 + u_i\n\\tag{15}\\]\nno R:\n\nlibrary(wooldridge)\ndata(\"hprice3\")\n\n# Estimar modelo de regressão não linear\nmodelo_nao_linear &lt;- lm(price ~ inst + I(inst^2), data = hprice3)\nsummary(modelo_nao_linear)\n\n\nCall:\nlm(formula = price ~ inst + I(inst^2), data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-82012 -25286  -8394  20140 194917 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.443e+04  7.340e+03   4.691 4.04e-06 ***\ninst         8.618e+00  1.013e+00   8.510 6.95e-16 ***\nI(inst^2)   -2.276e-04  2.953e-05  -7.708 1.64e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38860 on 318 degrees of freedom\nMultiple R-squared:  0.1969,    Adjusted R-squared:  0.1918 \nF-statistic: 38.98 on 2 and 318 DF,  p-value: 7.228e-16\n\n\nA interpretação do termo de interação é a seguinte:\n\n\\(\\beta_2\\) (I(inst^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -0,0002276, o que indica que para cada unidade de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 0,0002276 dólares, ceteris paribus. O efeito é muito pequeno o que pode estar relacionado com a escala da variável inst (distância em pés), 1 pé são aproximadamente 0.3048 metros.\n\nVamos converter a variável inst de pés para quilómetros (1 pé = 0.0003048 km) e re-estimar o modelo:\n\n# Converter inst de pés para quilómetros\nhprice3 &lt;- hprice3 %&gt;%\n    mutate(inst_km = inst * 0.0003048)\n# Estimar modelo de regressão não linear com inst em km\nmodelo_nao_linear_km &lt;- lm(price ~ inst_km + I(inst_km^2), data = hprice3)\nsummary(modelo_nao_linear_km)\n\n\nCall:\nlm(formula = price ~ inst_km + I(inst_km^2), data = hprice3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-82012 -25286  -8394  20140 194917 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   34434.5     7339.9   4.691 4.04e-06 ***\ninst_km       28275.8     3322.5   8.510 6.95e-16 ***\nI(inst_km^2)  -2449.7      317.8  -7.708 1.64e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38860 on 318 degrees of freedom\nMultiple R-squared:  0.1969,    Adjusted R-squared:  0.1918 \nF-statistic: 38.98 on 2 and 318 DF,  p-value: 7.228e-16\n\n\nA interpretação dos coeficientes é a seguinte:\n\n\\(\\beta_0\\) (constante): O preço médio de uma casa localizada exatamente na autoestrada (distância = 0 km) é de aproximadamente 34434.5 dólares.\n\\(\\beta_1\\) (inst_km): Cada quilómetro adicional de distância à autoestrada está associado a uma diminuição média de aproximadamente 28278.80 dólares no preço da casa, ceteris paribus.\n\\(\\beta_2\\) (I(inst_km^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -2449.70, o que indica que para cada quilómetro de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 2449.70 dólares, ceteris paribus.\n\nSe o coeficiente do termo quadrático fosse positivo, indicava que o efeito da distância no preço da casa aumentaria à medida que a distância aumenta, o que poderia sugerir que casas mais afastadas da autoestrada são mais valorizadas.\nTambém é possível calcular o efeito marginal da distância no preço da casa. O efeito marginal da distância é dado por:\n\\[\n\\frac{\\partial y}{\\partial x} = \\beta_1 + 2 \\beta_2 x\n\\tag{16}\\]\nPortanto, o efeito marginal da distância varia com o nível de distância. Podemos calcular o efeito marginal da distância para diferentes níveis de distância (0, 1, 2, 5, 10 unidades):\n\n# Níveis de distância para calcular o efeito marginal\nniv_distancia &lt;- c(0, 1, 2, 5, 10)\n\n# Coeficientes do modelo\ncoeficientes &lt;- coef(modelo_nao_linear_km)\n\n# Calcular efeito marginal para diferentes níveis de distância\nefeito_marginal &lt;- coeficientes[\"inst_km\"] + 2 * coeficientes[\"I(inst_km^2)\"] * niv_distancia\nefeito_marginal\n\n[1]  28275.79  23376.30  18476.81   3778.34 -20719.11\n\n# Criar data frame com os resultados obtidos\ndata.frame(Nivel_Distancia = niv_distancia,\n            Efeito_Marginal_Distancia = efeito_marginal)\n\n  Nivel_Distancia Efeito_Marginal_Distancia\n1               0                  28275.79\n2               1                  23376.30\n3               2                  18476.81\n4               5                   3778.34\n5              10                 -20719.11\n\n\nCom isto podemos ver que o efeito marginal da distância no preço da casa diminui à medida que a distância aumenta (relação de U invertido). Por exemplo, para uma casa localizada exatamente na autoestrada (inst_km = 0), o efeito marginal da distância é de aproximadamente 28275.79 dólares. Isto significa que se a casa estiver afastada 1 unidade da autoestrada, o preço da casa aumentará em aproximadamente 23376.30 doláres, ceteris paribus. No entanto, para uma casa localizada a 10 unidades da autoestrada (inst_km = 10), o efeito marginal da distância é de aproximadamente -20719.11 dólares. Portanto existe um ponto em que o efeito marginal da distância se torna negativo (turning point), ou seja, a partir desse ponto, aumentar a distância à autoestrada está associado a uma diminuição no preço da casa. Podemos calcular o ponto de inversão (máximo da função) do efeito marginal da distância resolvendo a equação:\n\\[\n\\frac{\\partial price}{\\partial inst} = 0  \\implies \\beta_1 + 2 \\beta_2 inst_i = 0 \\implies inst_i = -\\frac{\\beta_1}{2 \\beta_2}\n\\tag{17}\\]\nno R:\n\n# Calcular ponto de inversão\nponto_inversao &lt;- -coef(modelo_nao_linear_km)[\"inst_km\"] / (2 * coef(modelo_nao_linear_km)[\"I(inst_km^2)\"])\nponto_inversao\n\ninst_km \n5.77117 \n\n\nA partir da distância de aproximadamente 5.77 km, o efeito marginal da distância no preço da casa torna-se negativo, o que indica que aumentar a distância à autoestrada está associado a uma diminuição no preço da casa. Isto faz todo o sentido, pois se a casa está demasiado perto da autoestrada, o ruído, a poluição e o trãnsito podem diminuir o valor da casa. No entanto, se a casa está demasiado longe da autoestrada, a acessibilidade pode ser um fator negativo para o valor da casa.\nPodemos calcular o valor máximo do preço da casa substituindo o ponto de inversão na equação do modelo:\n\n# Calcular valor máximo do preço da casa\nvalor_maximo_preco &lt;- predict(modelo_nao_linear_km, newdata = data.frame(inst_km = ponto_inversao))\nvalor_maximo_preco\n\n inst_km \n116026.7 \n\n\nA mesma abordagem pode ser seguida para modelos com mais variáveis, por exemplo, incluindo a variável area (área da casa) e a variável rooms (número de quartos):\n\\[\n\\log(price_i) = \\beta_0 + \\beta_1 inst_i + \\beta_2 inst_i^2 + \\beta_3 area_i + \\beta_4 rooms_i + u_i\n\\tag{18}\\]\nno R:\n\n# Estimar modelo de regressão não linear com mais variáveis\nmodelo_nao_linear_2 &lt;- lm(log(price) ~ inst_km + I(inst_km^2)\n                            + area + rooms, data = hprice3)\nsummary(modelo_nao_linear_2)\n\n\nCall:\nlm(formula = log(price) ~ inst_km + I(inst_km^2) + area + rooms, \n    data = hprice3)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.27199 -0.17547 -0.01315  0.22890  0.81846 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.947e+00  1.299e-01  76.583  &lt; 2e-16 ***\ninst_km       1.668e-01  2.890e-02   5.773 1.86e-08 ***\nI(inst_km^2) -1.366e-02  2.706e-03  -5.046 7.61e-07 ***\narea          2.989e-04  3.018e-05   9.906  &lt; 2e-16 ***\nrooms         6.256e-02  2.365e-02   2.645  0.00858 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3093 on 316 degrees of freedom\nMultiple R-squared:  0.508, Adjusted R-squared:  0.5017 \nF-statistic: 81.56 on 4 and 316 DF,  p-value: &lt; 2.2e-16\n\n# Calcular ponto de inversão\nponto_inversao_2 &lt;- -coef(modelo_nao_linear_2)[\"inst_km\"] / (2 * coef(modelo_nao_linear_2)[\"I(inst_km^2)\"])\n\nponto_inversao_2\n\n inst_km \n6.107991 \n\n\nA interpretação dos coeficientes é a seguinte:\n\n\\(\\beta_0\\) (constante): O preço médio de uma casa localizada exatamente na autoestrada (distância = 0 km), com área igual a 0 e 0 quartos é de aproximadamente 9947 dólares. Neste caso a constante não tem uma interpretação prática, pois é praticamente uma casa estar na autoestrada, com área igual a 0 e 0 quartos.\n\\(\\beta_1\\) (inst_km): Cada quilómetro adicional de distância à autoestrada está associado a um aumento médio de aproximadamente 0.17 dólares no preço da casa, ceteris paribus.\n\\(\\beta_2\\) (I(inst_km^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -0.01, o que indica que para cada quilómetro de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 0.01 dólares, ceteris paribus.\n\\(\\beta_3\\) (area): Cada unidade adicional de área da casa está associada a um aumento médio de aproximadamente 0.002 dólares no preço da casa, ceteris paribus.\n\\(\\beta_4\\) (rooms): Cada quarto adicional na casa está associado a um aumento médio de aproximadamente 0.06 dólares no preço da casa, ceteris paribus.\n\nO ceteris paribus é interpretado como “para uma casa com as mesmas características, com exceção da variável em questão”.\nO ponto de inversão (máximo da função) do efeito marginal da distância é de aproximadamente 6.11 km. É a partir desta distância que o efeito marginal da distância no preço da casa se torna negativo.\nPodemos ter também o termo ao cubo. Ao aumentar o grau do polinómio, o modelo torna-se mais flexível, podendo capturar relações mais complexas entre as variáveis. No entanto, também pode levar a problemas de overfitting, onde o modelo se ajusta demasiado aos dados e não pode ser generalizado para novos dados. Por isso, temos de mantar sempre um equilíbrio entre a complexidade e a interpretação do modelo."
  },
  {
    "objectID": "capitulo-06-extensoes-regressao.html#modelo-com-o-desvio-padrão-robusto",
    "href": "capitulo-06-extensoes-regressao.html#modelo-com-o-desvio-padrão-robusto",
    "title": "Extensões dos Modelos de Regressão",
    "section": "",
    "text": "Para calcular o desvio padrão robusto para a heterocedasticidade no R, podemos utilizar a função vcovHC() do pacote sandwich. A função vcovHAC() do mesmo pacote permite calcular o desvio padrão robusto a autocorrelação e heterocedasticidade. A função coeftest() do pacote lmtest pode ser utilizada para apresentar os resultados da regressão com o desvio padrão robusto.\nOs argumentos da função vcovHC() permitem especificar o tipo de matriz de covariância robusta a ser utilizada. O argumento type pode assumir os seguintes valores:\n\n\"HC0\": Matriz de covariância robusta de White (1980).\n\"HC1\": Matriz de covariância robusta de MacKinnon e White (1985), que ajusta a matriz de covariância de White para amostras pequenas.\n\"HC2\": Matriz de covariância robusta de Long e Ervin (1983), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras.\n\"HC3\": Matriz de covariância robusta de Davidson e MacKinnon (1993), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras e o número de observações.\n\"HC4\": Matriz de covariância robusta de Cribari-Neto (2004), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras e o número de observações, com um ajuste adicional para observações influentes.\n\"HC4m\": Matriz de covariância robusta de Cribari-Neto (2004), que é uma versão modificada da HC4, com um ajuste adicional para observações influentes.\n\"HC5\": Matriz de covariância robusta de Pustejovsky e Tipton (2018), que é uma versão modificada da HC4, com um ajuste adicional para observações influentes, considerando o número de regressoras e o número de observações.\n\nA função vcovHAC() permite especificar o número de desfasamentos a considerar na matriz de covariância robusta para autocorrelação e heterocedasticidade, através do argumento lag."
  },
  {
    "objectID": "capitulo-06-extensoes-regressao.html#modelo-generalizado-de-mínimos-quadrados-gls",
    "href": "capitulo-06-extensoes-regressao.html#modelo-generalizado-de-mínimos-quadrados-gls",
    "title": "Extensões dos Modelos de Regressão",
    "section": "",
    "text": "função gls() do pacote nlme permite estimar modelos de regressão linear com erros que podem ter diferentes estruturas de correlação e variância. A função gls() é particularmente útil quando os pressupostos clássicos da regressão linear (como homocedasticidade e independência dos erros) não são verificados."
  },
  {
    "objectID": "capitulo-07-variavel-dependente-limitada.html",
    "href": "capitulo-07-variavel-dependente-limitada.html",
    "title": "Modelos de Variável Dependente Limitada",
    "section": "",
    "text": "Muitas situações económicas envolvem variáveis dependentes que não são contínuas e ilimitadas. Este capítulo apresenta modelos econométricos especializados para lidar com diferentes tipos de limitações na variável dependente: variáveis binárias, variáveis categóricas, dados censurados e dados de contagem.\n\n\n\n\n\nO modelo mais simples para variáveis binárias é o Modelo de Probabilidade Linear:\n\\[\nP(Y_i = 1|X_i) = \\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_k X_{ki}\n\\tag{1}\\]\n\n\n\nO modelo logit usa a função logística:\n\\[\nP(Y_i = 1|X_i) = \\frac{e^{\\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_k X_{ki}}}{1 + e^{\\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_k X_{ki}}}\n\\tag{2}\\]\n\n\n\nO modelo probit usa a função de distribuição normal:\n\\[\nP(Y_i = 1|X_i) = \\Phi(\\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_k X_{ki})\n\\tag{3}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare modelos LPM, Logit e Probit para um problema de escolha binária.\nEstime um modelo logit ordenado e teste a hipótese de linhas paralelas.\nAplique o modelo Tobit a dados com censuramento e compare com OLS.\nImplemente um modelo de Heckman para corrigir viés de seleção.\nCompare modelos Poisson, Binomial Negativo e Zero-Inflated para dados de contagem.\nAvalie a qualidade preditiva dos modelos usando várias métricas.\n\n\n\n\nNeste capítulo aprendemos:\n\nModelos para variáveis binárias (LPM, Logit, Probit)\nModelos para variáveis categóricas ordenadas e não-ordenadas\nModelos para dados censurados (Tobit)\nCorreção de viés de seleção (Heckman)\nModelos para dados de contagem (Poisson, Binomial Negativo, Zero-Inflated)\nMétodos de avaliação específicos para variáveis limitadas\n\nEstes modelos são essenciais para lidar com a natureza específica de muitos dados económicos e fornecem ferramentas robustas para análise econométrica aplicada."
  },
  {
    "objectID": "capitulo-07-variavel-dependente-limitada.html#introdução",
    "href": "capitulo-07-variavel-dependente-limitada.html#introdução",
    "title": "Modelos de Variável Dependente Limitada",
    "section": "",
    "text": "Muitas situações económicas envolvem variáveis dependentes que não são contínuas e ilimitadas. Este capítulo apresenta modelos econométricos especializados para lidar com diferentes tipos de limitações na variável dependente: variáveis binárias, variáveis categóricas, dados censurados e dados de contagem."
  },
  {
    "objectID": "capitulo-07-variavel-dependente-limitada.html#modelos-para-variáveis-binárias",
    "href": "capitulo-07-variavel-dependente-limitada.html#modelos-para-variáveis-binárias",
    "title": "Modelos de Variável Dependente Limitada",
    "section": "",
    "text": "O modelo mais simples para variáveis binárias é o Modelo de Probabilidade Linear:\n\\[\nP(Y_i = 1|X_i) = \\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_k X_{ki}\n\\tag{1}\\]\n\n\n\nO modelo logit usa a função logística:\n\\[\nP(Y_i = 1|X_i) = \\frac{e^{\\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_k X_{ki}}}{1 + e^{\\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_k X_{ki}}}\n\\tag{2}\\]\n\n\n\nO modelo probit usa a função de distribuição normal:\n\\[\nP(Y_i = 1|X_i) = \\Phi(\\beta_0 + \\beta_1 X_{1i} + \\cdots + \\beta_k X_{ki})\n\\tag{3}\\]"
  },
  {
    "objectID": "capitulo-07-variavel-dependente-limitada.html#exercícios",
    "href": "capitulo-07-variavel-dependente-limitada.html#exercícios",
    "title": "Modelos de Variável Dependente Limitada",
    "section": "",
    "text": "Compare modelos LPM, Logit e Probit para um problema de escolha binária.\nEstime um modelo logit ordenado e teste a hipótese de linhas paralelas.\nAplique o modelo Tobit a dados com censuramento e compare com OLS.\nImplemente um modelo de Heckman para corrigir viés de seleção.\nCompare modelos Poisson, Binomial Negativo e Zero-Inflated para dados de contagem.\nAvalie a qualidade preditiva dos modelos usando várias métricas."
  },
  {
    "objectID": "capitulo-07-variavel-dependente-limitada.html#resumo",
    "href": "capitulo-07-variavel-dependente-limitada.html#resumo",
    "title": "Modelos de Variável Dependente Limitada",
    "section": "",
    "text": "Neste capítulo aprendemos:\n\nModelos para variáveis binárias (LPM, Logit, Probit)\nModelos para variáveis categóricas ordenadas e não-ordenadas\nModelos para dados censurados (Tobit)\nCorreção de viés de seleção (Heckman)\nModelos para dados de contagem (Poisson, Binomial Negativo, Zero-Inflated)\nMétodos de avaliação específicos para variáveis limitadas\n\nEstes modelos são essenciais para lidar com a natureza específica de muitos dados económicos e fornecem ferramentas robustas para análise econométrica aplicada."
  }
]