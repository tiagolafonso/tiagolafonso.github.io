# Extensões dos Modelos de Regressão {#sec-extensoes-regressao}

Este capítulo apresenta várias extensões dos modelos de regressão linear básicos que são frequentemente necessárias na estimação de modelos econométricos. Estas extensões permitem lidar com situações mais complexas e realistas que os modelos lineares simples não conseguem capturar adequadamente.

## Modelos com Variáveis Dummy

Uma variável dummy é uma variável que assume valores binários (0 ou 1) que indica apresença ou não de uma característica específica. Por exemplo, uma variável dummy pode ser usada para indicar a presença de uma determinada condição (sim ou não) ou para categorias. Por exemplo, uma variável dummy pode ser usada para indicar se uma empresa está localizada numa determinada região (1) ou não (0), ou em várias regiões (mais de duas categorias).

### com variáveis Dummy Simples

As variáveis dummy são variáveis binárias que assumem valores 0 ou 1.

Uma dummmy, é geralmente representada por:

$$
D_i = \begin{cases}
1, & \text{se a condição é satisfeita} \\
0, & \text{a condição não é satisfeita}
\end{cases}
$$

Num modelo de regressão, uma variável dummy pode ser incluída como uma variável explicativa para capturar o efeito de uma característica qualitativa na variável dependente:

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 D_i + u_i
$$ {#eq-reg-dummy}

É necessário definir muito bem o que é o 0 ou o que e o 1., pois vai mudar a interpretação dos coeficientes no modelo. Para este exemplo vamos utilizar os dados `wage2` da biblioteca `wooldridge`, que contém informações sobre o salário, bem como outras características como anos de experiência, anos na empresa e outras características. O conjunto de dados contém variáveis dummy em que uma delas (`married`), que indica se o funcionário é casado (1) ou não (0). No `R` uma variável não tem de ser necessariamente binária para ser tratada como dummy, o `R` cria automaticamente as dummies para variáveis categóricas que são fatores (para o `R`). Vamos estimar a regressão do salário em função dos anos de educação, anos de experiência e se é casado ou não:

$$
wage_i = \beta_0 + \beta_1 educ_i + \beta_2 exper_i + \beta_3 married_i + u_i
$$ {#eq-reg-dummy-simples}

A interpretação do coeficientes da dummy ($\beta_3$) é a diferença média no salário entre indivíduos casados e não casados ($married = 1$ e $married = 0$), mantendo a educação e a experiência constantes. Ou seja, depende das unidades da variável dependente (neste caso, salário em dólares americanos).


```{r}
#| label: reg-dummy-simples
#| message: false
#| warning: false

library(tidyverse)
library(wooldridge)
data("wage2")

# Estimar modelo de regressão
modelo_dummy <- lm(wage ~ educ + exper + married, data = wage2)
summary(modelo_dummy)
```

Interpretação dos coeficientes:

-   $\beta_0$ (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência e que não é casado é de -427.76 US\$.
-   $\beta_1$ (`educ`): Cada ano adicional de educação está associado a um aumento médio de 76.55 US\$ no salário, *ceteris paribus*.
-   $\beta_2$ (`exper`): Cada ano adicional de experiência está associado a um aumento médio de 16.32 US\$ no salário, *ceteris paribus*.
-   $\beta_3$ (`married`): um indivíduo casado ganha, em média, 185.91 US\$ a mais do que um indivíduo não casado, *ceteris paribus*.

Para um regressão simples, apenas com a variável dummy como independente, o valor da constante é o valor médio da variável dependente quando a dummy é 0. Considerando o modelo:

```{r}
#| label: reg-dummy-simples-s-dummy
# Estimar modelo de regressão com apenas a dummy
modelo_dummy_s <- lm(wage ~ married, data = wage2)
summary(modelo_dummy_s)
```

O salário médio para indivíduos não casados é de 798.44 US\$. E qual o salário médio para indicíduos casados? Podemos calculamos o valor do salário para quando $married = 1$ considerando os coeficientes do modelo:

```{r}
#| label: calc-salario-casado

# Calcular salário médio para indivíduos casados
salario_casado <- predict(modelo_dummy_s, newdata = data.frame(married = 1))
salario_casado
```

O salário médio para indivíduos casados é em média de 977.05 US\$. Apenas para confirmar, podemos calcular a média diretamente com uma subamostra para cada grupo:

```{r}
#| label: calc-salario-casado-media

# Calcular salário médio para indivíduos casados

# Para indivíduos não casados (married = 0)
salario_nao_casado <- wage2 %>% 
    filter(married == 0) %>% 
    summarise(salario_medio = mean(wage))
salario_nao_casado

# Para indivíduos casados (married = 1)
salario_casado <- wage2 %>% 
    filter(married == 1) %>% 
    summarise(salario_medio = mean(wage))
salario_casado
```

Atravé do gráfico podemos visualizar a diferença nos salários entre indivíduos casados e não casados:

```{r}
#| label: grafico-boxplot-dummy
#| echo: false
library(plotly)

# Gráfico interativo com plotly
wage2 %>%
    mutate(married_factor = factor(married, 
                    labels = c("Não Casado", "Casado"))) %>%
    plot_ly(x = ~married_factor, y = ~wage, type = "box", 
                    color = ~married_factor, 
                    colors = c("steelblue", "orange")) %>%
    add_markers(x = ~married_factor, y = ~wage, 
                            marker = list(size = 3, opacity = 0.3),
                            showlegend = FALSE) %>%
    add_markers(data = wage2 %>% 
                    mutate(married_factor = factor(married,
                    labels = c("Não Casado", "Casado"))) %>%
                    group_by(married_factor) %>%
                    summarise(media = mean(wage), .groups = 'drop'),
                x = ~married_factor, y = ~media,
                marker = list(size = 10, color = "red", symbol = "circle"),
                name = "Média", showlegend = FALSE) %>%
    layout(title = "Distribuição do Salário por Estado Civil",
                 xaxis = list(title = "Estado Civil"),
                 yaxis = list(title = "Salário (US$)"),
                 showlegend = TRUE)

```

O ponto vermelho indica a média do salário para cada grupo.

Podemos converter a variável `married` para `non-married` (0 = casado, 1 = não casado) e re-estimar o modelo:

```{r}
#| label: reg-dummy-non-married
# Converter variável married para non-married
wage2$non_married <- ifelse(wage2$married == 1, 0, 1)

# Estimar modelo de regressão com a nova variável
modelo_dummy_non_married <- lm(wage ~ non_married,
                                data = wage2)

#Comparar modelos
library(stargazer)
stargazer(modelo_dummy_s,
        modelo_dummy_non_married,
        type = "text")
```

Podemos ver que o coeficiente da dummy mudou de sinal, mas a interpretação alterou de a diferença média no salário entre indivíduos casados e não casados (`modelo_dummy_s`) para a diferença média no salário entre indivíduos não casados e casados (`modelo_dummy_non_married`). A constante do segundo modelo é o salário médio para indivíduos casados (quando `non_married = 0`).

Quando a variável dependete está em logaritmos, o coeficiente da dummy pode ser interpretado como uma diferença percentual aproximada. Por exemplo para o modelo:

$$
\log(wage_i) = \beta_0 + \beta_1 educ_i + \beta_2 exper_i + \beta_3 married_i + u_i
$$ {#eq-reg-dummy-log}

No `R`:

```{r}
#| label: reg-dummy-log

# Estimar modelo de regressão com log do salário
modelo_dummy_log <- lm(log(wage) ~ educ + exper + married,
                        data = wage2)
summary(modelo_dummy_log)
```

Portanto a variação percentual aproximada é de 20.9% $100 \times \beta_{married}$. Neste caso, indivíduos casados ganham, em média, cerca de 20.9% a mais do que indivíduos não casados, *ceteris paribus*. Para uma interpretação mais precisa, podemos usar a fórmula de @halvorsen_interpretation_1980:

$$
\text{Variação Percentual} = (e^{\beta_3} - 1) \times 100
$$

no `R` é necessário extrair o coefiente do modelo e calcular o valor exponencial:

```{r}
#| label: calc-variacao-percentual

# Extrair coeficiente da dummy
beta_married <- coef(modelo_dummy_log)["married"]
# Calcular Variação percentual
variacao_percentual <- (exp(beta_married) - 1) * 100
variacao_percentual
```

Em média, indivíduos casados ganham cerca de 23.28% a mais do que indivíduos não casados, *ceteris paribus*.

ou então pelo método alternativo sugerido por @kennedy_estimation_1981:

$$
\text{Variação Percentual} \approx 100 \times \left(\frac{e^{\beta_3} - 1}{1}\right)
$$

Este método é especialmente útil quando o coeficiente é grande (geralmente maior que 0.1 ou 10%) e para amostras pequenas, pois leva em consideração a variância do estimador:

$$
\Delta Y \approx \left(e^{\beta - \frac{1}{2} \cdot \text{Var}(\beta)} - 1\right) \times 100
$$

No `R`, a variância do coeficiente pode ser obtida a partir da matriz de variância-covariância do modelo:

```{r}
#| label: calc-variancia-coeficiente

# Obter variância do coeficiente
var_beta_married <- vcov(modelo_dummy_log)["married", "married"]
# Calcular Variação percentual ajustada
delta_y <- (exp(beta_married - 0.5 * var_beta_married) - 1) * 100
delta_y
```

Neste caso, a variação percentual ajustada é de 23.17%, que é ligeiramente diferente da estimativa anterior.

### Múltiplas Categorias

As variáveis dummy também podem ser usadas para representar variáveis categóricas. Por exemplo, se tivermos uma variável multinominal que indica a cor dos carros vendidos (vermelho, azul, verde), podemos criar dummies para cada categoria:

$$
D_{vermelho} = \begin{cases}  
1, & \text{se o carro é vermelho} \\
0, & \text{não é vermelho}
\end{cases}
$$ {#eq-dummy-vermelho}

$$
D_{azul} = \begin{cases}
1, & \text{se o carro é azul} \\
0, & \text{não é azul}
\end{cases}
$$ {#eq-dummy-azul}

$$
D_{verde} = \begin{cases}
1, & \text{se o carro é verde} \\
0, & \text{não é verde}
\end{cases}
$$ {#eq-dummy-verde}

Exemplo no `R`:

```{r}
#| label: var-multinomial

#introduzir dados
cores <- c("vermelho", "azul", "verde", "vermelho",
            "verde", "azul", "vermelho", "verde", 
            "azul", "vermelho")
preço <- c(50000, 52000, 48000, 51000, 49000, 53000, 
            50000, 52000, 48000, 51000)

dados <- data.frame(cores, preço)

#para tabela resumo
library(gtsummary)

# Criar dummies e taela
dados |>
    mutate(
        vermelho = ifelse(cores == "vermelho", 1, 0),
        azul = ifelse(cores == "azul", 1, 0),
        verde = ifelse(cores == "verde", 1, 0)
    ) |>
    select(cores, vermelho, azul, verde) |>
    tbl_summary(
        by = cores)
```

A função `ifelse()` é usada para criar as variáveis dummy. A sintaxe é `ifelse(condição, valor_se_condicao_verificada, valor_se_condicao_nao_verificada)`, ver `?ifelse` para mais detalhes.

Para um exemplo real vamos usar o conjunto de dados `ceosal1` da biblioteca `wooldridge`, que contém informações sobre o salário dos CEOs de várias empresas, bem como outras características como anos de experiência, anos na empresa dummies que ondicam o setor de atividade da empresa:

-   `finance` - empresa do setor financeiro (1) ou não (0)
-   `indus` - empresa do setor industrial (1) ou não (0)
-   `consprod` - empresa do setor de bens de consumo (1) ou não (0)
-   `utility` - empresa do setor de serviços essenciais (1) ou não (0)

Neste conjunto não é necessário calcular as dummies, pois já estão incluídas (o que nem sempre acontece). Vamos estimar um modelo de regressão do logaritmo do salário dos CEOs (`lsalary`) em função do logaritmo vendas (`lsales), da rendibilidade dos capitais próprios (`roe\`) e o setor de atividade da empresa:

$$
\begin{split}
\log(salary_i) = \beta_0 &+ \beta_1 \log(sales_i) + \beta_2 roe_i + \beta_3 D_{finance} + \\
&\beta_4 D_{indus} + \beta_5 D_{consprod} + \beta_6 D_{utility} + u_i
\end{split}
$$ {#eq-reg-dummy-multiplas-categorias}

No `R`:

```{r}
#| label: reg-dummy-multiplas-categorias

library(wooldridge)
data("ceosal1")
# Estimar modelo de regressão
modelo_dummy_mult <- lm(lsalary ~ lsales + roe + finance + indus + consprod + utility,
                        data = ceosal1)
summary(modelo_dummy_mult)
```

Na regressão anterior o coeficiente da dummy `utility` aparece com valor NA, porque existe multicolinearidade perfeita entre as dummies (todas somadas é obtida uma coluna de 1's, que é uma constante).

```{r}
#| label: multicolinearidade-dummies

# Verificar multicolinearidade perfeita
ceosal1 %>%
    mutate(soma_dummies = finance + indus + consprod + utility) %>%
    summarise(min_soma = min(soma_dummies),
              max_soma = max(soma_dummies),
              unique_somas = n_distinct(soma_dummies))
```

Este fenómeno é conhecido como a armadilha das dummies. Em alguns softwares econométrico pode aparecer uma mensagem de erro a indicar que existe multicolinearidade perfeita. Para evitar este problema, uma das categorias deve ser excluída do modelo, ou seja, o modelo deve incluir apenas $k-1$ dummies (com $k$ o número de categorias). A categoria excluída é a categoria de referência (base) e as outras dummies medem o efeito relativo em comparação com essa mesma categoria. De referir que nesta amostra cada empresa pertence a apenas um setor. Neste caso, podemos excluir a dummy `utility` e re-estimar o modelo:

$$
\begin{split}
\log(salary_i) = \beta_0 &+ \beta_1 \log(sales_i) + \beta_2 roe_i + \beta_3 D_{finance} + \\
& \beta_4 D_{indus} + \beta_5 D_{consprod} + u_i
\end{split}
$$ {#eq-reg-dummy-multiplas-categorias-2}

No `R`:

```{r}
#| label: reg-dummy-multiplas-categorias-2

# Estimar modelo de regressão sem a dummy utility
modelo_dummy_mult2 <- lm(lsalary ~ lsales + roe + finance + indus + consprod,
                         data = ceosal1)
summary(modelo_dummy_mult2)
```

Interpretação dos coeficientes :

-   $\beta_0$ (constante): O salário médio para uma empresa do setor de serviços essenciais (categoria de referência) com vendas iguais a 1 (log(1) = 0) e rendibilidade dos capitais próprios igual a 0 é de 257 US\$.
-   $\beta_1$ (lsales): Um aumento de 1% nas vendas está associado a um aumento médio de 25.7% no salário do CEO, *ceteris paribus*.
-   $\beta_2$ (roe): Um aumento de 1 ponto percentual na rendibilidade dos capitais próprios está associado a um aumento médio de aproximadamente 1.1% no salário do CEO, *ceteris paribus* (a variável `roe` está expressa em percentagem, 0-100).
-   $\beta_3$ (finance): CEOs de empresas do setor financeiro ganham, aproximadamente em média, cerca de 44.1% a mais do que CEOs de empresas do setor de serviços essenciais (categoria de referência), *ceteris paribus*.
-   $\beta_4$ (indus): CEOs de empresas do setor industrial ganham, aproximadamente em média, cerca de 28.3% a mais do que CEOs de empresas do setor de serviços essenciais, *ceteris paribus*.
-   $\beta_5$ (consprod): CEOs de empresas do setor de bens de consumo ganham, aproximadamente em média, cerca de 3.3% a mais do que CEOs de empresas do setor de serviços essenciais, *ceteris paribus*.

podemos considerar a dummy `indus` como a categoria de referência, excluindo-a do modelo:

```{r}
#| label: reg-dummy-multiplas-categorias-3

# Estimar modelo de regressão sem a dummy indus
modelo_dummy_mult3 <- lm(lsalary ~ lsales + roe + finance + consprod + utility,
                         data = ceosal1)

library(stargazer)
stargazer(modelo_dummy_mult2,
        modelo_dummy_mult3,
        type = "text")
```

Podemos observar que do modelo `modelo_dummy_mult2` e `modelo_dummy_mult3` que os coeficientes das dummies dos setores alteraram, pois a categoria de referência mudou. O coeficiente da dummy `indus` no primeiro modelo é o inverso do coeficiente da dummy `utility` no segundo modelo. Tal como na regressão @eq-reg-dummy-simples, também é possível calcular a variação de salário em percentagem para as dummies do modelo @eq-reg-dummy-multiplas-categorias-2, usando a fórmula de @halvorsen_interpretation_1980 ou o método alternativo sugerido por @kennedy_estimation_1981.

```{r}
#| label: calc-variacao-percentual-multiplas-categorias

# Extrair coeficientes das dummies
beta_finance <- coef(modelo_dummy_mult2)["finance"]
beta_indus <- coef(modelo_dummy_mult2)["indus"]
beta_consprod <- coef(modelo_dummy_mult2)["consprod"]
# Calcular variação percentual usando a fórmula de Halvorsen e Palmquist (1980)
variacao_percentual_finance <- (exp(beta_finance) - 1) * 100
variacao_percentual_indus <- (exp(beta_indus) - 1) * 100
variacao_percentual_consprod <- (exp(beta_consprod) - 1) * 100

#calcular variação percentual ajustada usando o método de Kennedy (1981)
var_beta_finance <- vcov(modelo_dummy_mult2)["finance", "finance"]
var_beta_indus <- vcov(modelo_dummy_mult2)["indus", "indus"]
var_beta_consprod <- vcov(modelo_dummy_mult2)["consprod", "consprod"]
delta_y_finance <- (exp(beta_finance - 0.5 * var_beta_finance) - 1) * 100
delta_y_indus <- (exp(beta_indus - 0.5 * var_beta_indus) - 1) * 100
delta_y_consprod <- (exp(beta_consprod - 0.5 * var_beta_consprod) - 1) * 100

#organizar em tabela:
tabela_variacao <- data.frame(
    Categoria = c("finance", "indus", "consprod"),
    Variacao_Percentual = c(variacao_percentual_finance,
                            variacao_percentual_indus,
                            variacao_percentual_consprod),
    Variacao_Percentual_Ajustada = c(delta_y_finance,
                                    delta_y_indus,
                                    delta_y_consprod)
)
tabela_variacao
```

A interpretação é semelhantes ao exemplo anterior. Também é possível utilizar variáveis dummy como variável independente. Esta abordagem será discutida na @sec-variavel-dependente-limitada.

## Modelos com Termos de Interação

Um termo de interação é utilizado para capturar o efeito conjunto de duas ou mais variáveis independentes na variável dependente. Portanto num exemplo simples, um termo de interação entre duas variáveis $X_1$ e $X_2$ é representado como $X_1 \times X_2$. O termo de interação permite que o efeito de uma variável independente dependa do valor de outra variável independente.

$$
y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 (X_{1i} \times X_{2i}) + u_i
$$ {#eq-reg-interacao}

Podemos criar termos de interaão entre variáveis contínuas, entre variáveis dummy e entre variáveis dummy e contínuas.

### entre variáveis contínuas

Para esta apicação vamos utilizar o conjunto de dados `wage2` da biblioteca `wooldridge`. Mais informação sobre o conjunto de dados pode ser obtida com `?wage2`. 

Vamos estimar a regressão do salário em função dos anos de educação, anos de experiência e a interacção entre educação e experiência:

$$
wage_i = \beta_0 + \beta_1 educ_i + \beta_2 exper_i + \beta_3 (educ_i \times exper_i) + u_i
$$ {#eq-reg-interacao-continua}

O termo de interação pode ser crido no `data.frame` (`mutate(educ_exper = educ* exper)`) ou diretamente na fórmula do modelo. No `R`:

```{r}
#| label: reg-interacao-continua
#| message: false
#| warning: false
library(tidyverse)
library(wooldridge)

# carregar dados
data("wage2")

#Estimar modelo de regressão com interacção
modelo_interacoes <- lm(wage ~ educ + exper + I(educ * exper), data = wage2)
summary(modelo_interacoes)
```

O `I` é necessário para indicar ao `R` que a operação deve ser interpretada literalmente, ou seja, como uma multiplicação entre as variáveis `educ` e `exper`. Sem o `I`, o `R` pode interpretar o símbolo `*` como um operador especial na fórmula do modelo, o que pode levar a resultados inesperados.

Na interpretação dos coeficientes é necessário ter alguma atenção pois o coeficiente de cada variável que também faz parte do termo de interação depende do valor da outra variável. A interpretação dos coeficientes é a seguinte:

-   $\beta_0$ (constante): O salário médio para alguém com 0 anos de educação e 0 anos de experiência é de 271.934 US$. Pode não ter uma interpretação prática, pois é improvável que alguém tenha 0 anos de educação e experiência.
-   $\beta_1$ (`educ`): Cada ano adicional de educação está associado a um aumento médio de 35.106 US$ no salário, *ceteris paribus*, **para alguém sem experiência** (exper = 0).
-   $\beta_2$ (`exper`): Cada ano adicional de experiência está associado a uma diminuição média de 32.663 US$ no salário, *ceteris paribus*, **para alguém sem educação** (educ = 0).
-   $\beta_3$ (`I(educ * exper)`): O coeficiente de interação indica que o efeito combinado de educação e experiência no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito da experiência no salário aumenta em 3.904 US$, e vice-versa.

Num caso em que o termo de interação é negativo, por exemplo de -3.904, indicaria que o efeito combinado de educação e experiência no salário é negativo, ou seja, para cada ano adicional de educação, o efeito da experiência no salário diminui em 3.904 US$, e vice-versa.

Para calcular o efeito marginal da educação no salário para diferentes níveis de experiência. O efeito marginal da educação é dado por:

$$
\frac{\partial wage}{\partial educ} = \beta_1 + \beta_3 \times exper
$$
Portanto, o efeito marginal da educação varia com o nível de experiência. Podemos calcular o efeito marginal da educação para diferentes níveis de experiência (0, 5, 10, 20 anos):

```{r}
#| label: calc-efeito-marginal-educacao

# Níveis de experiência para calcular o efeito marginal
niv_experiencia <- c(0, 5, 10, 20)
# Coeficientes do modelo
coeficientes <- coef(modelo_interacoes)
# Calcular efeito marginal da educação para diferentes níveis de experiência
efeito_marginal_educ <- coeficientes["educ"] +
                coeficientes["I(educ * exper)"] * niv_experiencia
# Criar data frame com os resultados obtidos
data.frame(Nivel_Experiencia = niv_experiencia,
            Efeito_Marginal_Educacao = efeito_marginal_educ)
```


Para um modelo onde a variável dependente está em logaritmo natural, o coeficiente da variável contínua no termo de interação pode ser interpretado como uma variação percentual aproximada. Por exemplo, para o modelo:

```{r}
#| label: calc-efeito-marginal-educacao2

modelo_interacoes_log <- lm(log(wage) ~ educ + exper + I(educ * exper), data = wage2)
summary(modelo_interacoes_log)
```

-  `constante`: O salário médio para alguém com 0 anos de educação e 0 anos de experiência é de 6.579 US$ (log(719.82)).
-  `educ`: Cada ano adicional de educação está associado a um aumento médio de aproximadamente 4.4% no salário, *ceteris paribus*, **para alguém sem experiência** (exper = 0).
-  `exper`: Cada ano adicional de experiência está associado a uma diminuição média de aproximadamente 2.15% no salário, *ceteris paribus*, **para alguém sem educação** (educ = 0).
-  `I(educ * exper)`: O coeficiente de interação indica que o efeito combinado de educação e experiência no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito da experiência no salário aumenta em aproximadamente 0.32%, e vice-versa.

De notar que o coeficiente da variável `exper` não é estatisticamente diferente de zero para nenhum nível de significância estatística.

### entre variáveis dummy

Recorrendo ao conjunto de dados anterior (`wage2`), onde a variável `married` indica se o indivíduo é casado (1) ou não (0) e a variável dummy `urban` indica se o indivíduo vive numa área urbana (1) ou não (0), vamos estimar o modelo de regressão com termos de interação entre variáveis dummy:

$$
\log(wage_i) = \beta_0 + \beta_1 educ_i + \beta_2 exper_i + \beta_3 married_i + \beta_4 urban_i + \beta_5 (married_i \times urban_i) + u_i
$$ {#eq-reg-interacao-dummy}

no `R`:

```{r}
#| label: reg-interacao-dummy

#estimar modelo de regressão com interacção entre dummies
modelo_interacoes_dummy <- lm(log(wage) ~ educ + exper + married + 
                            urban + I(married * urban), data = wage2)
summary(modelo_interacoes_dummy)
```

interpretação dos coeficientes:
-   $\beta_0$ (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência, que não é casado e que não vive numa área urbana é de 180.91 US$ (`exp(5.198)`).

-   $\beta_1$ (educ): Cada ano adicional de educação está associado a um aumento médio de aproximadamente 7.5 % no salário, *ceteris paribus*.

-   $\beta_2$ (exper): Cada ano adicional de experiência está associado a um aumento médio de aproximadamente 1.8% no salário, *ceteris paribus*.

-   $\beta_3$ (married): Ser casado está associado a um aumento médio de aproximadamente 24.20% no salário, *ceteris paribus*, **para alguém sem experiência e sem educação** (exper = 0, educ = 0).

-   $\beta_4$ (urban): Viver numa área urbana está associado a um aumento médio de aproximadamente 20.4.% no salário, *ceteris paribus*, **para alguém sem experiência e sem educação** (exper = 0, educ = 0).

-   $\beta_5$ (married * urban): O coeficiente de interação indica que o efeito combinado de ser casado e viver em uma área urbana no salário é negativo. Especificamente, para indivíduos casados e viver numa área urbana, o efeito combinado no salário é de aproximadamente -2.86% em relação ao efeito individual de ser casado e viver numa área urbana.

No termo de interação entre variáveis dummy, neste caso `married` e `urban`, o 1 (da multiplicaão) corresponde a indivíduos que são casados e vivem numa área urbana (1*1), o 0 (da multiplicação) corresponde a indivíduos que não são casados e não vivem numa área urbana (0*0), e os outros dois casos (1*0 e 0*1) correspondem a indivíduos que são casados mas não vivem numa área urbana ou que não são casados mas vivem numa área urbana. Portanto, o coeficiente do termo de interação mede o efeito adicional de ser casado e viver numa área urbana em comparação com os efeitos individuais de ser casado e viver numa área urbana. Se criarmos uma tabela com os diferentes grupos, podemos ver melhor a interpretação de cada caso: ou seja:

| married | urban | Interação (= married × urban)   | log(wage) previsto | wage previsto (e^{\text{log(wage)}}) | Interpretação                                    |
|---------|-------|----------------------------------|--------------------|---------------------------------------|--------------------------------------------------|
| 0       | 0     | 0                                | $\beta_0$                 | $e^{\beta_0}$                               | Não casado, não vive numa área urbana               |
| 1       | 0     | 0                                | $\beta_0 + \beta_3$           | $e^{\beta_0 + \beta_3}$                         | Casado, não vive numa área urbana          |
| 0       | 1     | 0                                | $\beta_0 + \beta_4$           | $e^{\beta_0 + \beta_4}$                         | Não casado, vive numa área urbana          |
| 1       | 1     | 1                                | $\beta_0 + \beta_3 + \beta_4 + \beta_5$ | $e^{\beta_0 + \beta_3 + \beta_4 + \beta_5}$               | Casado, vive numa área urbana              |


De notar que no modelo @eq-reg-interacao-dummy o coeficiente do termo de interação não é estatisticamente diferente de zero para nenhum nível de significância estatística. Ou seja, não existe evidência estatística de que o efeito combinado de ser casado e viver numa área urbana no salário seja diferente do efeito individual de ser casado e viver numa área urbana, o que também é um resultado interessante.


### entre variáveis dummy e contínuas

Para esta aplicação vamos considerar o mesmo conjunto de dados `wage2` e vamos estimar a regressão com o termo de interação entre a variável dummy `south` e a variável contínua `educ`:

$$
\log(wage_i) = \beta_0 + \beta_1 educ_i + \beta_2 exper_i + \beta_3 south_i + \beta_4 (south_i \times educ_i) + u_i
$$ {#eq-reg-interacao-dummy-cont}

no `R`:

```{r}
#| label: reg-interacao-dummy-cont

#carregar dados
data("wage2")

# Estimar modelo
modelo_interacoes_dummy_cont <- lm(log(wage) ~ educ + exper + south +
                                     I(south * educ), data = wage2)

summary(modelo_interacoes_dummy_cont)
```

Em que a interpretação dos coeficientes é a seguinte:

-   $\beta_0$ (constante): O salário médio para alguém com 0 anos de educação, 0 anos de experiência e que não vive no sul é de 298.87 US$ (`exp(5.70)`).

-   $\beta_1$ (educ): Cada ano adicional de educação está associado a um aumento médio de aproximadamente 6.7% no salário, *ceteris paribus*, **para alguém que não vive no sul** (south = 0).

-   $\beta_2$ (exper): Cada ano adicional de experiência está associado a um aumento médio de aproximadamente 2% no salário, *ceteris paribus*.

-   $\beta_3$ (south): Viver no sul está associado a uma diminuição média de aproximadamente 46.4% no salário, *ceteris paribus*, **para alguém sem anos de educação** (educ = 0).

-   $\beta_4$ (south * educ): O coeficiente de interação indica que o efeito combinado de viver no sul e anos de educação no salário é positivo. Especificamente, para cada ano adicional de educação, o efeito de viver no sul no salário aumenta em aproximadamente 2.41%, *ceteris paribus*.

O efeito marginal da educação no salário para diferentes níveis de experiência é dado por:
$$
\frac{\partial \log(wage)}{\partial educ} = \beta_1 + \beta_4 \times south
$$  {#eq-efeito-marginal-educacao-dummy}

Portanto, o efeito marginal da educação varia com o valor da variável dummy `south`. Podemos calcular o efeito marginal da educação para os dois grupos (south = 0 e south = 1):

```{r}
#| label: calc-efeito-marginal-educacao-dummy
# Coeficientes do modelo
coeficientes <- coef(modelo_interacoes_dummy_cont)
# Calcular efeito marginal da educação para os dois grupos
efeito_marginal_educacao <- data.frame(
  south = c(0, 1),
  efeito_marginal = c(coeficientes["educ"] + 0 * coeficientes["I(south * educ)"],
                      coeficientes["educ"] + 1 * coeficientes["I(south * educ)"])
)
efeito_marginal_educacao
```

Para indivíduos que não vivem no sul (south = 0), cada ano adicional de educação está associado a um aumento médio de aproximadamente 6.7% no salário, *ceteris paribus*. Para indivíduos que vivem no sul (south = 1), cada ano adicional de educação está associado a um aumento médio de aproximadamente 9.11% no salário, *ceteris paribus*.

## Modelos Não Lineares

Modelos não lineares são modelos em que a relação entre a variável dependente e uma ou mais variáveis independentes não é linear. A equação do modelo não linear para a variável dependente `y` e para as variáveis independentes `X` é dada por:

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{1i}^2 + u_i
$$ {#eq-reg-nao-linear}

em que $\beta_0$ é a constante, $\beta_1$ é o coeficiente da variável independente `x1`, $\beta_2$ é o coeficiente do termo quadrático da variável independente `x2` e $u_i$ é o termo de erro. O modelo é estimado pelo método dos mínimos quadrados ordinários. Quando devemos estimar um modelo não linear? Uma abordagem comum é a análise gráfica. Se a relação entre a variável dependente e a variável independente não for aproximadamente linear, pode ser apropriado considerar um modelo não linear. Outra abordagem é incluir termos polinomiais (como o termo quadrático) na regressão e verificar se esses termos são estatisticamente diferentes de 0. Existem alguma forma de lidar com a não linearidade sem recorrer a modelos não lineares, como por exemplo, transformar as variáveis (logaritmos). 

Um exemplo de um modelo que poderá ser não linear é a relação entre o preço das casas e a distância à autoestrada mais próxima. Vamos considerar o conjunto de dados `hprice2` da biblioteca `wooldridge`, que contém informações sobre o preço das casas (`price`), a distância à autoestrada mais próxima (`dist`) e outras características das casas. Mais informação sobre o conjunto de dados pode ser obtida com `?hprice2`.

Para esta aplicação vamos estimar o modelo:

$$
\log(price_i) = \beta_0 + \beta_1 dist_i + \beta_2 dist_i^2 + u_i
$$ {#eq-reg-nao-linear-hprice}

no `R`:

```{r}
#| label: reg-nao-linear

library(wooldridge)
data("hprice3")

# Estimar modelo de regressão não linear
modelo_nao_linear <- lm(price ~ inst + I(inst^2), data = hprice3)
summary(modelo_nao_linear)
```

A interpretação do termo de interação é a seguinte:

-   $\beta_2$ (I(inst^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -0,0002276, o que indica que para cada unidade de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 0,0002276 dólares, *ceteris paribus*. O efeito é muito pequeno o que pode estar relacionado com a escala da variável `inst` (distância em pés), 1 pé são aproximadamente 0.3048 metros.

Vamos converter a variável `inst` de pés para quilómetros (1 pé = 0.0003048 km) e re-estimar o modelo:

```{r}
#| label: reg-nao-linear-km

# Converter inst de pés para quilómetros
hprice3 <- hprice3 %>%
    mutate(inst_km = inst * 0.0003048)
# Estimar modelo de regressão não linear com inst em km
modelo_nao_linear_km <- lm(price ~ inst_km + I(inst_km^2), data = hprice3)
summary(modelo_nao_linear_km)
```

A interpretação dos coeficientes é a seguinte:

-   $\beta_0$ (constante): O preço médio de uma casa localizada exatamente na autoestrada (distância = 0 km) é de aproximadamente 34434.5 dólares.

-   $\beta_1$ (inst_km): Cada quilómetro adicional de distância à autoestrada está associado a uma diminuição média de aproximadamente 28278.80 dólares no preço da casa, *ceteris paribus*.

-   $\beta_2$ (I(inst_km^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -2449.70, o que indica que para cada quilómetro de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 2449.70 dólares, *ceteris paribus*.

Se o coeficiente do termo quadrático fosse positivo, indicava que o efeito da distância no preço da casa aumentaria à medida que a distância aumenta, o que poderia sugerir que casas mais afastadas da autoestrada são mais valorizadas.

Também é possível calcular o efeito marginal da distância no preço da casa. O efeito marginal da distância é dado por:

$$
\frac{\partial y}{\partial x} = \beta_1 + 2 \beta_2 x
$$  {#eq-efeito-marginal-distancia}

Portanto, o efeito marginal da distância varia com o nível de distância. Podemos calcular o efeito marginal da distância para diferentes níveis de distância (0, 1, 2, 5, 10 unidades):

```{r}
#| label: calc-efeito-marginal-distancia

# Níveis de distância para calcular o efeito marginal
niv_distancia <- c(0, 1, 2, 5, 10)

# Coeficientes do modelo
coeficientes <- coef(modelo_nao_linear_km)

# Calcular efeito marginal para diferentes níveis de distância
efeito_marginal <- coeficientes["inst_km"] + 2 * coeficientes["I(inst_km^2)"] * niv_distancia
efeito_marginal

# Criar data frame com os resultados obtidos
data.frame(Nivel_Distancia = niv_distancia,
            Efeito_Marginal_Distancia = efeito_marginal)
```

Com isto podemos ver que o efeito marginal da distância no preço da casa diminui à medida que a distância aumenta (relação de U invertido). Por exemplo, para uma casa localizada exatamente na autoestrada (`inst_km = 0`), o efeito marginal da distância é de aproximadamente 28275.79 dólares. Isto significa que se a casa estiver afastada 1 unidade da autoestrada, o preço da casa aumentará em aproximadamente 23376.30 doláres, *ceteris paribus*. No entanto, para uma casa localizada a 10 unidades da autoestrada (`inst_km = 10`), o efeito marginal da distância é de aproximadamente -20719.11 dólares. Portanto existe um ponto em que o efeito marginal da distância se torna negativo (*turning point*), ou seja, a partir desse ponto, aumentar a distância à autoestrada está associado a uma diminuição no preço da casa. Podemos calcular o ponto de inversão (máximo da função) do efeito marginal da distância resolvendo a equação:

$$
\frac{\partial price}{\partial inst} = 0  \implies \beta_1 + 2 \beta_2 inst_i = 0 \implies inst_i = -\frac{\beta_1}{2 \beta_2}
$$  {#eq-ponto-inversao}

no `R`:

```{r}
#| label: calc-ponto-inversao
# Calcular ponto de inversão
ponto_inversao <- -coef(modelo_nao_linear_km)["inst_km"] / (2 * coef(modelo_nao_linear_km)["I(inst_km^2)"])
ponto_inversao
```

A partir da distância de aproximadamente 5.77 km, o efeito marginal da distância no preço da casa torna-se negativo, o que indica que aumentar a distância à autoestrada está associado a uma diminuição no preço da casa. Isto faz todo o sentido, pois se a casa está demasiado perto da autoestrada, o ruído, a poluição e o trãnsito podem diminuir o valor da casa. No entanto, se a casa está demasiado longe da autoestrada, a acessibilidade pode ser um fator negativo para o valor da casa.

Podemos calcular o valor máximo do preço da casa substituindo o ponto de inversão na equação do modelo:

```{r}
#| label: calc-valor-maximo-preco

# Calcular valor máximo do preço da casa
valor_maximo_preco <- predict(modelo_nao_linear_km, newdata = data.frame(inst_km = ponto_inversao))
valor_maximo_preco
```

A mesma abordagem pode ser seguida para modelos com mais variáveis, por exemplo, incluindo a variável `area` (área da casa) e a variável `rooms` (número de quartos):

$$
\log(price_i) = \beta_0 + \beta_1 inst_i + \beta_2 inst_i^2 + \beta_3 area_i + \beta_4 rooms_i + u_i
$$ {#eq-reg-nao-linear-hprice-2}

no `R`:

```{r}
#| label: reg-nao-linear-2

# Estimar modelo de regressão não linear com mais variáveis
modelo_nao_linear_2 <- lm(log(price) ~ inst_km + I(inst_km^2)
                            + area + rooms, data = hprice3)
summary(modelo_nao_linear_2)

# Calcular ponto de inversão
ponto_inversao_2 <- -coef(modelo_nao_linear_2)["inst_km"] / (2 * coef(modelo_nao_linear_2)["I(inst_km^2)"])

ponto_inversao_2
```

A interpretação dos coeficientes é a seguinte:

-   $\beta_0$ (constante): O preço médio de uma casa localizada exatamente na autoestrada (distância = 0 km), com área igual a 0 e 0 quartos é de aproximadamente 9947 dólares. Neste caso a constante não tem uma interpretação prática, pois é praticamente uma casa estar na autoestrada, com área igual a 0 e 0 quartos.

-   $\beta_1$ (inst_km): Cada quilómetro adicional de distância à autoestrada está associado a um aumento médio de aproximadamente 0.17 dólares no preço da casa, *ceteris paribus*.

-   $\beta_2$ (I(inst_km^2)): O coeficiente do termo quadrático indica que o efeito da distância no preço da casa não é constante. Especificamente, o efeito da distância no preço da casa diminui à medida que a distância aumenta. Isto sugere que o impacto inicial de uma casa estar afastada da autoestrada é maior do que o impacto adicional de uma casa ainda mais afastada. O coeficiente do termo quadrático é -0.01, o que indica que para cada quilómetro de aumento na distância, o efeito da distância no preço da casa diminui em aproximadamente 0.01 dólares, *ceteris paribus*.

-   $\beta_3$ (area): Cada unidade adicional de área da casa está associada a um aumento médio de aproximadamente 0.002 dólares no preço da casa, *ceteris paribus*.

-   $\beta_4$ (rooms): Cada quarto adicional na casa está associado a um aumento médio de aproximadamente 0.06 dólares no preço da casa, *ceteris paribus*.

O *ceteris paribus* é interpretado como "para uma casa com as mesmas características, com exceção da variável em questão".

O ponto de inversão (máximo da função) do efeito marginal da distância é de aproximadamente 6.11 km. É a partir desta distância que o efeito marginal da distância no preço da casa se torna negativo.

Podemos ter também o termo ao cubo. Ao aumentar o grau do polinómio, o modelo torna-se mais flexível, podendo capturar relações mais complexas entre as variáveis. No entanto, também pode levar a problemas de *overfitting*, onde o modelo se ajusta demasiado aos dados e não pode ser generalizado para novos dados. Por isso, temos de mantar sempre um equilíbrio entre a complexidade e a interpretação do modelo.

## Modelo com o desvio padrão robusto


Para calcular o desvio padrão robusto para a heterocedasticidade no `R`, podemos utilizar a função `vcovHC()` do pacote `sandwich`. A função `vcovHAC()` do mesmo pacote permite calcular o desvio padrão robusto a autocorrelação e heterocedasticidade. A função `coeftest()` do pacote `lmtest` pode ser utilizada para apresentar os resultados da regressão com o desvio padrão robusto.

Os argumentos da função `vcovHC()` permitem especificar o tipo de matriz de covariância robusta a ser utilizada. O argumento `type` pode assumir os seguintes valores:

-   `"HC0"`: Matriz de covariância robusta de White (1980).
-   `"HC1"`: Matriz de covariância robusta de MacKinnon e White (1985), que ajusta a matriz de covariância de White para amostras pequenas.
-   `"HC2"`: Matriz de covariância robusta de Long e Ervin (1983), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras.
-   `"HC3"`: Matriz de covariância robusta de Davidson e MacKinnon (1993), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras e o número de observações.
-   `"HC4"`: Matriz de covariância robusta de Cribari-Neto (2004), que ajusta a matriz de covariância de White para amostras pequenas, considerando o número de regressoras e o número de observações, com um ajuste adicional para observações influentes.
-   `"HC4m"`: Matriz de covariância robusta de Cribari-Neto (2004), que é uma versão modificada da HC4, com um ajuste adicional para observações influentes.
-   `"HC5"`: Matriz de covariância robusta de Pustejovsky e Tipton (2018), que é uma versão modificada da HC4, com um ajuste adicional para observações influentes, considerando o número de regressoras e o número de observações.

A função `vcovHAC()` permite especificar o número de desfasamentos a considerar na matriz de covariância robusta para autocorrelação e heterocedasticidade, através do argumento `lag`.


## Modelo dos Mínimos Quadrados Ponderados (WLS)

## Modelo Generalizado de Mínimos Quadrados (GLS)



função `gls()` do pacote `nlme` permite estimar modelos de regressão linear com erros que podem ter diferentes estruturas de correlação e variância. A função `gls()` é particularmente útil quando os pressupostos clássicos da regressão linear (como homocedasticidade e independência dos erros) não são verificados.